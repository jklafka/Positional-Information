---
title: Positional entropy, conditional entropy and mutual information in English CHILDES corpora
author: Josef Klafka and Dan Yurovsky 
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: false
    number_sections: false
    theme: lumen
    toc_float: false
    code_folding: show 
---

```{r setup, include = FALSE}
# load packages
library(knitr)
library(tidyverse)
library(directlabels)
library(childesr) #data
library(tidytext)
library(entropy)
library(tidyboot)
library(dplyr)
library(tokenizers)
library(gtools)

opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
               error = FALSE, cache = FALSE, tidy = FALSE)

theme_set(theme_classic(base_size = 16))
```


Get utterances
```{r get_utterances}
prov_utterances <- get_utterances(corpus = "Providence")

brown_utterances <- get_utterances(corpus = "Brown")
```

bootstrap entropy slopes
```{r slope bootstrapping}
lengths <- brown_utterances %>%
  filter(speaker_role  != "Target_Child") %>%
  mutate(length = str_count(stem, " ") + 1)

tokens <- lengths %>% 
  filter(length >= 6) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stem) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    summarise(n = n()) 

#extract tokens to be used for slope bootstrapping
extract_tokens <- function(role, min_length, utterances) {
if (role == "child") {
lengths <- utterances %>%
  filter(speaker_role == "Target_Child") %>%
  mutate(length = str_count(stem, " ") + 1)

} else {
  lengths <- utterances %>%
  filter(speaker_role != "Target_Child") %>%
  mutate(length = str_count(stem, " ") + 1)
}
  
  tokens <- lengths %>% 
    filter(length >= min_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stem) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    summarise(n = n()) 
}
 

start_slope <- function(df) {
 
 df %>%
    filter(word_order %in% c(1,2)) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `2` - `1`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

second_slope <- function(df) {
 
 df %>%
    ungroup() %>%
    mutate(word_order %in% c(2, 3)) %>%
    filter(word_order > 0) %>%
    group_by(length, word_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `3` - `2`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

mid_slope <- function(df) {
 
 df %>%
    ungroup() %>%
    mutate(word_order = if_else(word_order == length-2, 2,
                                if_else(word_order == 3, 1,
                                        0))) %>%
    filter(word_order > 0) %>%
    group_by(length, word_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `2` - `1`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

penult_slope <- function(df) {
 
 df %>%
    ungroup() %>%
    mutate(word_order = if_else(word_order == length-1, 2,
                                if_else(word_order == length -2, 1,
                                        0))) %>%
    filter(word_order > 0) %>%
    group_by(length, word_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `2` - `1`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}



end_slope <- function(df) {
 
 df %>%
    ungroup() %>%
    mutate(word_order = if_else(word_order == length, 2,
                                if_else(word_order == length -1 , 1,
                                        0))) %>%
    filter(word_order > 0) %>%
    group_by(length, word_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `2` - `1`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}


start_time <- Sys.time()
samples <- replicate(1000, mid_slope(tokens))  %>%
  as_data_frame()
end_time <- Sys.time()

end_time - start_time

samples %>%
  summarise(mean = mean(value), ci_upper = quantile(value, .975), 
            ci_lower = quantile(value, .025))


```
Compute Providence Corpus Estimates
```{r calc_pos_entropy}
get_entropies <- function(role, sen_length, utterances) {
  
  if(role == "Target_Child") {
    sub_utterances <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1) # uses stems of words, not original words
  } else {
     sub_utterances <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1)
  }
  
  # 
  # 
  # ggplot(adult_utterances, aes(x = length)) +
  #   geom_histogram(fill = "white", color = "black")
  
  tokens <- sub_utterances %>%
    filter(length == sen_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stem) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(word_order, word)
   
  tokens %>%
    summarise(n = n()) %>%
    tidyboot(summary_function = function(x) x %>% 
               summarise(entropy = entropy(n, unit = "log2")),
             statistics_functions = function(x) x %>%
             summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
    mutate(role = role, length = sen_length)
}

entropies <- map(2:10, ~get_entropies("not child", .x, brown_utterances)) %>%
  bind_rows()

ggplot(entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F)



entropies_child <- map(2:10, ~ get_entropies("Target_Child", .x, prov_utterances)) %>%
  bind_rows()

ggplot(entropies_child, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F)

entropies_all <- bind_rows(entropies, entropies_child)

ggplot(entropies_all, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper, color = role)) +
  facet_wrap(~ length) + 
  geom_pointrange(position = position_dodge(.25)) +
  geom_smooth(se = F) +
  ggtitle("Child and Adult Positional Entropy vs. Word Position (Providence corpus)")


new_utterances <- prov_utterances %>% filter(speaker_role=="Target_Child") %>% mutate(len = str_count(stem, " ") + 1)
#toks <- new_utterances %>% 

ggplot(new_utterances, aes(x=len)) +
  geom_histogram(binwidth = 1)
```

compute estimates
```{r calc_pos_entropy}
#get counts of each word in each position of utterances of length sen_length
#from the collection of CHILDES utterances in utterances with speaker_role in role
get_counts <- function(role, sen_length, utterances) { 
  
  if(role == "Target_Child") {
    sub_utterances <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1) # uses stems of words, not original words
  } else {
     sub_utterances <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1)
  }
 
  sub_utterances %>%
    filter(length == sen_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stem) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n())
} 

#compute the unigram entropy measure from the counts of each word in each position
unigram_entropy <- function(counts) {
     
  counts %>%
    group_by(word_order, word) %>%
    summarise(n = n()) %>%
    tidyboot(summary_function = function(x) x %>% 
               summarise(entropy = entropy(n, unit = "log2")),
             statistics_functions = function(x) x %>%
             summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
    mutate(role = role, length = sen_length)
}

#compute the pairwise Mutual Information measure between all pairs of word positions
all_pairwise_mi <- function(df) {
  max_len <- max(df$word_order)
  
  combs <- combn(1:max_len, 2, simplify = F) 
  
  map(combs, ~position_mi(df, .x[1], .x[2])) %>%
    bind_rows()
  
}

#perform the actual Mutual Information computation
position_mi <- function(df, pos1, pos2) {
  
  mi <- df %>%
    filter(word_order %in% c(pos1, pos2)) %>%
    mutate(word_order = factor(word_order, labels = c("first", "second"))) %>%
    spread(word_order, word) %>%
    group_by(first, second) %>%
    summarise(n = n()) %>%
    spread(second, n, fill = 0) %>% 
    ungroup() %>%
    select(-first) %>%
    mi.empirical(., unit = "log2")
  
  data_frame(length = max(df$word_order), pos1 = pos1, pos2 = pos2, mi = mi)
}

counts <- map(2:6, ~get_counts("not child", .x, prov_utterances))

mi <- map(counts, all_pairwise_mi)

ggplot(mi, aes(x = pos2, y = mi, color = as.factor(pos1))) + 
  facet_wrap(~length) + 
  geom_point() + 
  geom_line()


ggplot(mi[[4]], aes(x = pos1, y = mi, color = as.factor(pos2))) + 
  facet_wrap(~length) + 
  geom_point() + 
  geom_line()


entropies <- map(2, ~ get_counts("not child", .x, prov_utterances) %>%
                   bind_rows %>%
                   unigram_entropy) %>%
  bind_rows()

ggplot(entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F)


entropies_child <- map(2:10, ~ get_entropies("Target_Child", .x, prov_utterances)) %>%
  bind_rows()

ggplot(entropies_child, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F)

entropies_all <- bind_rows(entropies, entropies_child)

ggplot(entropies_all, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper, color = role)) +
  facet_wrap(~ length) + 
  geom_pointrange(position = position_dodge(.25)) +
  geom_smooth(se = F) +
  ggtitle("Child and Adult Positional Entropy vs. Word Position (Providence corpus)")


new_utterances <- prov_utterances %>% filter(speaker_role=="Target_Child") %>% mutate(len = str_count(stem, " ") + 1)
#toks <- new_utterances %>% 

ggplot(new_utterances, aes(x=len)) +
  geom_histogram(binwidth = 1)
```

English Conditional Entropy
```{r conditional_entropy}
#compute all Conditional Entropy from all one-way pairs of word positions
all_conditional_entropy <- function(df) {
  max_len <- max(df$word_order)
  #combs <- combn(1:max_len, 2, simplify = F)
  perms <- permutations(max_len, 2, 1:max_len, repeats.allowed = F) 
  combs <-  setNames(split(perms, seq(nrow(perms))), rownames(perms))
  
  map(combs, ~conditional_entropy(df, .x[1], .x[2])) %>%
    bind_rows()
  
}

#perform the actual Conditional Entropy calculation
conditional_entropy <- function(df, X, Y) { #Y is conditioning variable
  
  d <- df %>%
    filter(word_order %in% c(X, Y)) %>%
    mutate(word_order = factor(word_order, labels = c("first", "second"))) %>%
    spread(word_order, word) 
  
  ent <- d$first %>% table() %>% as.vector() %>% entropy.empirical(unit="log2")

  mi <- d %>% 
    group_by(first, second) %>%
    summarise(n = n()) %>%
    spread(second, n, fill = 0) %>% 
    ungroup() %>%
    select(-first) %>%
    mi.empirical(., unit = "log2")

  data_frame(length = max(df$word_order), X = X, Y = Y, mi = mi, ent = ent, ce = ent - mi)
}

counts <- map(2:8, ~get_counts("not child", .x, prov_utterances))

ce <- map(counts, all_conditional_entropy)

ggplot(ce, aes(x = X, y = ce, color = as.factor(X))) + 
  facet_wrap(~ length) + 
  geom_point() + 
  geom_line()


ggplot(ce_br, aes(x = X, y = ce, color = as.factor(Y))) + 
  facet_wrap(~ length) + 
  geom_point() + 
  geom_line() + 
  ggtitle("Conditional entropy vs. conditioned random variable")

```


N-Gram frequencies (saved for later)
```{r ngram_frequencies}
m_prov <- prov_utterances %>%
          filter(speaker_role == "Target_Child") %>%
          mutate(length = str_count(stem, " ") + 1) %>% 
          filter(length >= 2)


b_prov <- m_prov %>% mutate(bigrams = paste(unlist(textcnt(stem, n = 2, split = " ", method = "string")), collapse = ",")) #get all the counts of bigrams from all of the dataframe
counts <- b_prov$bigrams[1] %>% strsplit(split=',') %>% unlist() %>% as.numeric()
fileConn <- file("counts.txt")
writeLines(b_prov$bigrams[1], fileConn)
close(fileConn)

b_prov <- m_prov %>% mutate(bigrams = paste(unlist(names(textcnt(stem, n = 2, split = " ", method = "string"))), collapse = ",")) #get all of the bigrams from the entire dataframe
bigrams <- b_prov$bigrams[1] %>% strsplit(split=',') %>% unlist()
fileConn <- file("bigram_words.txt")
writeLines(b_prov$bigrams[1], fileConn)
close(fileConn)

```

