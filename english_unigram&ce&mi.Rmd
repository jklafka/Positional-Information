---
title: Positional entropy, conditional entropy and mutual information in English CHILDES corpora
author: Josef Klafka and Dan Yurovsky 
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: false
    number_sections: false
    theme: lumen
    toc_float: false
    code_folding: show 
---

```{r setup, include = FALSE}
# load packages
library(knitr)
library(tidyverse)
library(directlabels)
library(childesr) #data
library(tidytext)
library(entropy)
library(tidyboot)
library(dplyr)
library(tokenizers)
library(gtools)

opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
               error = FALSE, cache = FALSE, tidy = FALSE)

theme_set(theme_classic(base_size = 16))
```


Get utterances
```{r get_utterances}
prov_utterances <- get_utterances(corpus = "Providence") #stem or gloss

brown_utterances <- get_utterances(corpus = "Brown") #stem or gloss

shiro_utterances <- get_utterances(corpus = "Shiro") #use gloss

wagner_utterances <- get_utterances(corpus = "Wagner") #use gloss

okayama_utterances <- get_utterances(corpus = "Okayama") #use gloss

zhou_utterances <- get_utterances(corpus = "ZhouDinner") #use stem, NOT gloss
```

bootstrap entropy slopes
```{r slope bootstrapping}

#extract tokens to be used for slope bootstrapping
extract_stem_tokens <- function(role, target_length, utterances) {
  
  if (role == "child") {
    lengths <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1)

} else {
    lengths <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1)
}
  
  tokens <- lengths %>% 
    filter(length == target_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stem) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    summarise(n = n()) 
}

extract_gloss_tokens <- function(role, target_length, utterances) {
  
  if (role == "child") {
    lengths <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1)

} else {
    lengths <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1)
}
  
  tokens <- lengths %>% 
    filter(length == target_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, gloss) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    filter(word_order <= length) %>% 
    summarise(n = n()) 
}
 

start_slope <- function(df) {
 
 df %>%
    filter(word_order %in% c(1,2)) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `2` - `1`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

second_slope <- function(df) {
 
 df %>%
    ungroup() %>%
    mutate(word_order %in% c(2, 3)) %>%
    filter(word_order > 0) %>%
    group_by(length, word_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `3` - `2`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

#note this measurement only makes sense if utterance length is at least 6
mid_slope <- function(df) {
 
 df %>%
    ungroup() %>%
    mutate(word_order = if_else(word_order == length-2, 2,
                                if_else(word_order == 3, 1,
                                        0))) %>%
    filter(word_order > 0) %>%
    group_by(length, word_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `2` - `1`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

penult_slope <- function(df) {
 
 df %>%
    ungroup() %>%
    mutate(word_order = if_else(word_order == length-1, 2,
                                if_else(word_order == length -2, 1,
                                        0))) %>%
    filter(word_order > 0) %>%
    group_by(length, word_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `2` - `1`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}



end_slope <- function(df) {
 
 df %>%
    ungroup() %>%
    mutate(word_order = if_else(word_order == length, 2,
                                if_else(word_order == length -1 , 1,
                                        0))) %>%
    filter(word_order > 0) %>%
    group_by(length, word_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `2` - `1`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}


get_slopes <- function(role, target_length, utterances, stemgloss) {
  
  if (stemgloss == "stem") {
    tokens <- extract_stem_tokens(role, target_length, utterances)  
  } else {
    tokens <- extract_tokens(role, target_length, target_length, utterances)
  }
  
  start_slopes <- replicate(1000, start_slope(tokens)) %>% 
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975), 
              ci_lower = quantile(value, .025))
  
  second_slopes <- replicate(1000, second_slope(tokens)) %>% 
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975), 
              ci_lower = quantile(value, .025))
    
  mid_slopes <- replicate(1000, mid_slope(tokens)) %>% 
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975), 
              ci_lower = quantile(value, .025))
    
  penult_slopes <- replicate(1000, penult_slope(tokens)) %>% 
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975), 
              ci_lower = quantile(value, .025))
    
  end_slopes <- replicate(1000, end_slope(tokens)) %>% 
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975), 
              ci_lower = quantile(value, .025))
  
  rbind(start_slopes, second_slopes, mid_slopes, penult_slopes, end_slopes)
}


```

```{r log_lengths}
tokens <- extract_tokens("not child", 6, 30, prov_utterances)


tokens %>%
    filter(word_order %in% c(1,2)) %>%
    dplyr::group_by(length, word_order) %>%
    dplyr::summarise(entropy = entropy(n, unit = "log2"), n = n()) %>%
    spread(word_order, entropy) %>%
    group_by(length) %>%
    mutate(diff = `2` - `1`) %>%
  ggplot(aes(x = length, y = diff)) +
  geom_point(aes(size = n)) +
  geom_smooth(se = F) +
  scale_x_log10() #log length stats

extract_tokens <- function(role, min_length, max_length, utterances) {
  
  if (role == "child") {
    lengths <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(stem, " +") +1) 
    
} else {
    lengths <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
     mutate(length = str_count(stem, " +") +1)
}
  
  tokens <- lengths %>% 
    filter(length >= min_length) %>%
    filter(length <= max_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stem, token = stringr::str_split, pattern = " +") %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    filter(word_order <= length) %>% 
    summarise(n = n()) 
}
  
shiro_slopes <- map(6:10, ~get_slopes("child", .x, shiro_utterances, "gloss")) 

shiro_slopes <- get_slopes("child", 6, shiro_utterances, "gloss") #what we would expect
shiro_slopes <- get_slopes("not child", 6, shiro_utterances, "gloss")
wagner_slopes <- get_slopes("child", 6, wagner_utterances, "gloss")
okayama_slopes <- get_slopes("child", 6, okayama_utterances, "gloss")
zhou_slopes <- get_slopes("child", 6, zhou_utterances, "stem")


start_time <- Sys.time()
samples <- replicate(1000, start_slope(tokens))  %>%
  as_data_frame()
end_time <- Sys.time()

end_time - start_time

start_slopes <- samples %>%
  summarise(mean = mean(value), ci_upper = quantile(value, .975), 
            ci_lower = quantile(value, .025))

```

```{r slope_analysis}

new_get_tokens <- function(role, min_length, max_length, utterances) {
  
  if (role == "child") {
        lengths <- utterances %>%
            filter(speaker_role != "Target_Child") %>%
            mutate(length = str_count(gloss, " ") + 1)
    
  } else {
    
        lengths <- utterances %>%
            filter(speaker_role != "Target_Child") %>%
            mutate(length = str_count(gloss, " ") + 1)
  }

  sub_tokens <- lengths %>% 
     mutate(gloss = str_trim(gloss)) %>%
     mutate(length = str_count(gloss, " +") +1) %>%
     filter(length >= min_length) %>%
     filter(length <= max_length) %>%
     mutate(utterance_id = 1:n()) %>%
     unnest_tokens(word, gloss, token = stringr::str_split, pattern = " +") %>%
     group_by(utterance_id) %>%
     mutate(word_order = 1:n()) %>%
     group_by(length, word_order, word) %>%
     filter(word_order <= length) %>% 
     summarise(n = n()) 
}

tkns <- new_get_tokens("child", 6, 20, prov_utterances)

new_tkns <- tkns %>% 
    filter(word_order %in% c(1,2)) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2"), n = n()) %>%
    spread(word_order, entropy) %>%
    group_by(length) %>%
    mutate(diff = `2` - `1`) %>%
  ggplot(aes(x = length, y = diff)) +
  geom_point(aes(size = n)) +
  geom_smooth(se = F) +
  scale_x_log10() 

min_length <- 6
max_length <- 20

lengths <- prov_utterances %>%
       filter(speaker_role != "Target_Child") %>%
       mutate(length = str_count(gloss, " ") + 1)

sub_tokens <- lengths %>% 
     mutate(gloss = str_trim(gloss)) %>%
     mutate(length = str_count(gloss, " +") +1) %>%
     filter(length >= min_length) %>%
     filter(length <= max_length) %>%
     mutate(utterance_id = 1:n())


sub_tokens %>%
     unnest_tokens(word, gloss, token = stringr::str_split, pattern = " +") %>%
     group_by(utterance_id) %>%
     mutate(word_order = 1:n()) %>%
     group_by(utterance_id) %>%
     summarise(n = n()) %>%
     # filter(n != 6) %>%
     left_join(sub_tokens) %>%
     select(gloss, n, length) %>%
     View()

```

Compute Providence Corpus Estimates
```{r calc_pos_entropy}
get_entropies <- function(role, sen_length, utterances) {
  
  if(role == "Target_Child") {
    sub_utterances <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1) # uses stems of words, not original words
  } else {
     sub_utterances <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1)
  }
  
  # 
  # 
  # ggplot(adult_utterances, aes(x = length)) +
  #   geom_histogram(fill = "white", color = "black")
  
  tokens <- sub_utterances %>%
    filter(length == sen_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stem) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(word_order, word)
   
  tokens %>%
    summarise(n = n()) %>%
    tidyboot(summary_function = function(x) x %>% 
               summarise(entropy = entropy(n, unit = "log2")),
             statistics_functions = function(x) x %>%
             summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
    mutate(role = role, length = sen_length)
}

entropies <- map(2:10, ~get_entropies("not child", .x, brown_utterances)) %>%
  bind_rows()

ggplot(entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F)



entropies_child <- map(2:10, ~ get_entropies("Target_Child", .x, prov_utterances)) %>%
  bind_rows()

ggplot(entropies_child, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F)

entropies_all <- bind_rows(entropies, entropies_child)

ggplot(entropies_all, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper, color = role)) +
  facet_wrap(~ length) + 
  geom_pointrange(position = position_dodge(.25)) +
  geom_smooth(se = F) +
  ggtitle("Child and Adult Positional Entropy vs. Word Position (Providence corpus)")


new_utterances <- prov_utterances %>% filter(speaker_role=="Target_Child") %>% mutate(len = str_count(stem, " ") + 1)
#toks <- new_utterances %>% 

ggplot(new_utterances, aes(x=len)) +
  geom_histogram(binwidth = 1)
```

compute estimates
```{r calc_pos_entropy}
#get counts of each word in each position of utterances of length sen_length
#from the collection of CHILDES utterances in utterances with speaker_role in role
get_counts <- function(role, sen_length, utterances) { 
  
  if(role == "Target_Child") {
    sub_utterances <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1) # uses stems of words, not original words
  } else {
     sub_utterances <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1)
  }
 
  sub_utterances %>%
    filter(length == sen_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stem) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n())
} 

#compute the unigram entropy measure from the counts of each word in each position
unigram_entropy <- function(counts) {
     
  counts %>%
    group_by(word_order, word) %>%
    summarise(n = n()) %>%
    tidyboot(summary_function = function(x) x %>% 
               summarise(entropy = entropy(n, unit = "log2")),
             statistics_functions = function(x) x %>%
             summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
    mutate(role = role, length = sen_length)
}

#compute the pairwise Mutual Information measure between all pairs of word positions
all_pairwise_mi <- function(df) {
  max_len <- max(df$word_order)
  
  combs <- combn(1:max_len, 2, simplify = F) 
  
  map(combs, ~position_mi(df, .x[1], .x[2])) %>%
    bind_rows()
  
}

#perform the actual Mutual Information computation
position_mi <- function(df, pos1, pos2) {
  
  mi <- df %>%
    filter(word_order %in% c(pos1, pos2)) %>%
    mutate(word_order = factor(word_order, labels = c("first", "second"))) %>%
    spread(word_order, word) %>%
    group_by(first, second) %>%
    summarise(n = n()) %>%
    spread(second, n, fill = 0) %>% 
    ungroup() %>%
    select(-first) %>%
    mi.empirical(., unit = "log2")
  
  data_frame(length = max(df$word_order), pos1 = pos1, pos2 = pos2, mi = mi)
}

counts <- map(2:6, ~get_counts("not child", .x, prov_utterances))

mi <- map(counts, all_pairwise_mi) %>% bind_rows()

ggplot(mi, aes(x = pos2, y = mi, color = as.factor(pos1))) + 
  facet_wrap(~length) + 
  geom_point() + 
  geom_line()


ggplot(mi, aes(x = pos1, y = mi, color = as.factor(pos2))) + 
  facet_wrap(~length) + 
  geom_point() + 
  geom_line()


entropies <- map(2, ~ get_counts("not child", .x, prov_utterances) %>%
                   bind_rows %>%
                   unigram_entropy) %>%
  bind_rows()

ggplot(entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F)


entropies_child <- map(2:10, ~ get_entropies("Target_Child", .x, prov_utterances)) %>%
  bind_rows()

ggplot(entropies_child, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F)

entropies_all <- bind_rows(entropies, entropies_child)

ggplot(entropies_all, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper, color = role)) +
  facet_wrap(~ length) + 
  geom_pointrange(position = position_dodge(.25)) +
  geom_smooth(se = F) +
  ggtitle("Child and Adult Positional Entropy vs. Word Position (Providence corpus)")


new_utterances <- prov_utterances %>% filter(speaker_role=="Target_Child") %>% mutate(len = str_count(stem, " ") + 1)
#toks <- new_utterances %>% 

ggplot(new_utterances, aes(x=len)) +
  geom_histogram(binwidth = 1)
```

English Conditional Entropy
```{r conditional_entropy}
#compute all Conditional Entropy from all one-way pairs of word positions
all_conditional_entropy <- function(df) {
  max_len <- max(df$word_order)
  #combs <- combn(1:max_len, 2, simplify = F)
  perms <- permutations(max_len, 2, 1:max_len, repeats.allowed = F) 
  combs <-  setNames(split(perms, seq(nrow(perms))), rownames(perms))
  
  map(combs, ~conditional_entropy(df, .x[1], .x[2])) %>%
    bind_rows()
  
}

#perform the actual Conditional Entropy calculation
conditional_entropy <- function(df, X, Y) { #Y is conditioning variable
  
  d <- df %>%
    filter(word_order %in% c(X, Y)) %>%
    mutate(word_order = factor(word_order, labels = c("first", "second"))) %>%
    spread(word_order, word) 
  
  ent <- d$first %>% table() %>% as.vector() %>% entropy.empirical(unit="log2")

  mi <- d %>% 
    group_by(first, second) %>%
    summarise(n = n()) %>%
    spread(second, n, fill = 0) %>% 
    ungroup() %>%
    select(-first) %>%
    mi.empirical(., unit = "log2")

  data_frame(length = max(df$word_order), X = X, Y = Y, mi = mi, ent = ent, ce = ent - mi)
}

counts <- map(2:8, ~get_counts("not child", .x, prov_utterances))

ce <- map(counts, all_conditional_entropy)

ggplot(ce, aes(x = X, y = ce, color = as.factor(X))) + 
  facet_wrap(~ length) + 
  geom_point() + 
  geom_line()


ggplot(ce_br, aes(x = X, y = ce, color = as.factor(Y))) + 
  facet_wrap(~ length) + 
  geom_point() + 
  geom_line() + 
  ggtitle("Conditional entropy vs. conditioned random variable")

```


N-Gram frequencies (saved for later)
```{r ngram_frequencies}
m_prov <- prov_utterances %>%
          filter(speaker_role == "Target_Child") %>%
          mutate(length = str_count(stem, " ") + 1) %>% 
          filter(length >= 2)


b_prov <- m_prov %>% mutate(bigrams = paste(unlist(textcnt(stem, n = 2, split = " ", method = "string")), collapse = ",")) #get all the counts of bigrams from all of the dataframe
counts <- b_prov$bigrams[1] %>% strsplit(split=',') %>% unlist() %>% as.numeric()
fileConn <- file("counts.txt")
writeLines(b_prov$bigrams[1], fileConn)
close(fileConn)

b_prov <- m_prov %>% mutate(bigrams = paste(unlist(names(textcnt(stem, n = 2, split = " ", method = "string"))), collapse = ",")) #get all of the bigrams from the entire dataframe
bigrams <- b_prov$bigrams[1] %>% strsplit(split=',') %>% unlist()
fileConn <- file("bigram_words.txt")
writeLines(b_prov$bigrams[1], fileConn)
close(fileConn)

```

