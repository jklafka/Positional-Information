% Template for Cogsci submission with R Markdown

% Stuff changed from original Markdown PLOS Template
\documentclass[10pt, letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{float}
\usepackage{caption}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}

% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% hyperref package, useful for hyperlinks
\usepackage{hyperref}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}

% Sweave(-like)
\usepackage{fancyvrb}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontshape=sl}
\newenvironment{Schunk}{}{}
\DefineVerbatimEnvironment{Code}{Verbatim}{}
\DefineVerbatimEnvironment{CodeInput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{CodeOutput}{Verbatim}{}
\newenvironment{CodeChunk}{}{}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{apacite}

% KM added 1/4/18 to allow control of blind submission


\usepackage{color}

% Use doublespacing - comment out for single spacing
%\usepackage{setspace}
%\doublespacing


% % Text layout
% \topmargin 0.0cm
% \oddsidemargin 0.5cm
% \evensidemargin 0.5cm
% \textwidth 16cm
% \textheight 21cm

\title{Information Distribution Depends on Language-Specific Features}


\author{Josef Klafka \and Daniel Yurovsky \\
        \texttt{\{jklafka, yurovsky\}@uchicago.edu} \\
       Department of Psychology \\ University of Chicago}

\begin{document}

\maketitle

\begin{abstract}
We argue for a reformulation of the Uniform Information Density
hypothesis R. P. Levy \& Jaeger (2007) in which the grammatical,
phonological and morphological features of a language determine how
information is distributed throughout utterances. We expand on a method
from Yu, Cong, Liang, \& Liu (2016) to compute entropy based on word
position within utterances as a derivation of information distribution.
We analyze adult and child spoken corpora as well as text corpora drawn
from a variety of languages on Wikipedia. We find that each language has
a robust, characteristic entropy curve across word position, and present
evidence that these entropy curves are in part determined by typological
features. We conclude that speakers may tend towards communicating
information uniformly, but the distribution of information in their
utterances is conditioned on the language they communicate in.

\textbf{Keywords:}
Entropy; information; information theory; communication
\end{abstract}

\section{Introduction}\label{introduction}

Over 7,000 languages are spoken around the modern world (Simons \&
Fenning, 2018). These language vary along many dimensions, but all share
a core goal: communicating information. If speakers and writers of these
languages act near-optimally to achieve their communicative goals,
regularities of use across these diverse languages can be explained by a
rational theory of communication (Anderson, 1991). Information theory, a
mathematical framework developed by Shannon (1948) to describe the
transmission and decoding of signals, has been a unifying language for
the recent development of such theories in human and machine language
processing (Jelinek, 1976; R. P. Levy \& Jaeger, 2007).

These theories model the process of communication as transmission of
information over a noisy channel. The producer begins with an intended
meaning, packages this meaning into language, and then sends the meaning
to their intended receiver over a communcative channel. The receiver
must then decode from the signal they receive on their end of the
channel the producer's intended meaning. The problem is that the channel
is noisy, and sometimes the signal can get corrupted (e.g.~the producer
can misspeak, or the receiver can mishear). In order to maximize the
probability that the correct meaning is transmitted, these theories
predict that producers should choose linguistic messages that keep the
rate of information across words constant. The intuition is that if the
receiver misperceives a word, and that word contains most of the
information in the sentence, then the communication will have failed.
Because producers cannot predict which word a speaker will mishear,
their best strategy is spread the information evenly across all of the
words in a sentence, i.e.~maintain \emph{uniform information density}
(Genzel \& Charniak, 2002; R. P. Levy \& Jaeger, 2007).

The original evidence in R. P. Levy \& Jaeger (2007) finds that the
insertion of complementizers in relative clauses in English corresponds
to where neighboring words have high information content. Similarly,
Frank \& Jaeger (2008) argues that contradictions in English such as
``you're'' do not occur when neighboring words are highly informative.
The evidence in favor of UID largely been situation-specific and
English-language driven, while the hypothesis itself has been applied
broadly over the past decade. Applications include determining whether
linguistic alignment takes place (Jaeger \& Snider, 2013), Zipfian word
length distributions (Piantadosi, Tily, \& Gibson, 2011), communication
efficiency (Mahowald, Fedorenko, Piantadosi, \& Gibson, 2013), dialogue
and turn-taking (Xu \& Reitter, 2018) and the significance of ambiguity
in language (Piantadosi, Tily, \& Gibson, 2012), among other research.

However, other recent work has contradicted the UID hypothesis. Similar
to the original R. P. Levy \& Jaeger (2007), Zhan \& Levy (2018) focuses
on information distribution at particular points in sentences. Zhan \&
Levy (2018) finds that more information-rich classifiers in Mandarin
Chinese are produced when production of the neighboring noun is
difficult, not when the information content is high. Jain, Singh,
Ranjan, Rajkumar, \& Agarwal (2018) examine word order across spoken
sentences in Hindi, a freer word order language than English, and find
that information density has no significant effect on word order.

Recently, Yu et al. (2016) developed a more direct test of the Uniform
Information Density hypothesis, applying the logic used by Genzel \&
Charniak (2002) to look at the distribution of information \emph{within}
individual sentences. Because people process language
incrementally--using the previous words in a sentence to predict the
words that will come next--the amount of information that a word
contains when seen in isolation should increase over the course of a
sentence. Analyzing a large corpus of written English, they find a
different pattern: Entropy increases over the first few words of an
utterance and then remains constant until the final word where it again
jumps up (see Figure \ref{fig:our_replication}). Yu et al. (2016)
conclude that the Uniform Information Density hypothesis must not hold
for medial words in a sentence.

\begin{CodeChunk}
\begin{figure*}[h]

{\centering \includegraphics{figs/our_replication-1} 

}

\caption[Providence corpus unigram entropy]{Providence corpus unigram entropy}\label{fig:our_replication}
\end{figure*}
\end{CodeChunk}

We extend and generalize Yu et al. (2016) in three ways: we confirm that
this same pattern is found in spoken English--in both adult and child
speakers. We then examine entropy curves cross-linguistically, finding a
diversity of shapes across the world's languages. Finally, we show that
entropy curves are predictable from the structure of individual
languages, namely word order. Taken together, our results suggest a
refinement of the Uniform Information Density hypothesis: speakers may
structure their utterances to optimize information density, but they
must do so under the constraints of the language they use to
communicate.

The UID hypothesis predicts that information transmission rate will tend
towards uniformity in both speech and text. We begin by examining speech
to determine if we obtain the entropy curve shape we expect for spoken
communication. We begin with the same source as R. P. Levy \& Jaeger
(2007) and Jaeger (2010), the Switchboard corpus of adult telephone
conversations. We also examine child-adult conversations in the CHILDES
TalkBank (Brown, 1973; MacWhinney, 2014) corpora database of spoken
adult-child conversations in multiple languages. Children are not fully
developed speakers, so we want to compare the entropy curve we obtain by
computing over the utterances in the CHILDES corpora to the utterances
in the adult-adult Switchboard corpus.

\section{Experiment 1}\label{experiment-1}

Yu et al. (2016) challenges the UID hypothesis through examining an
entropy measure across sentence positions within the text portion of the
English-language British National Corpus (BNC) Clear (1993). They
partition the corpus by sentence length in number of words. For each
word position \(X\) of sentences of length \(k\), they define \(w\) as a
unique word occurring in position \(X\). They define \(p(w)\) as the
number of times word \(w\) occurs in position \(X\) divided by the
number of total words that occur in position \(X\) i.e.~the number of
sentences of length \(k\). Then \(p(w)\) is the probability of obtaining
word \(w\) by choosing a word at random in position \(X\) in sentences
of length \(k\).

\[H(X) = \sum\limits_w p(w)\log\big(p(w)\big)\] With this measure, Yu et
al. (2016) compute the unigram entropy at each position of sentences of
each length within the corpus. The result of this method can be plotted
for each utterance length as an \emph{entropy curve}, which can be
visually compared across utterance length to observe the how the unigram
entropy changes across absolute positions in each of the utterances.
Genzel \& Charniak (2002) similarly examine a unigram entropy measure on
sentences, and found that entropy at the sentence level increases
linearly with sentence index within a corpus. UID applies this
uniformity of entropy rate in sentences to all levels of speech, and so
the Yu et al. (2016), which examines text at the word level, should find
an affine function at the word level.

\section{Experiment 2}\label{experiment-2}

We used the Providence English corpus from CHILDES. The Providence
corpus recorded interactions between children between 1 and 3 years old
and their parents in the home. We divided the corpus by speaker into
child and non-child categories. We further divided the corpus by
utterance length, so that all sentences of length \(k\) (e.g. \(6\))
were grouped together. Finally, within each utterance length, we
computed the unigram entropy measure for each position.

The entropy curves capture individual variation across positions in
utterances of the same length. This allows us to directly observe and
judge the amount of variation in words that appear in an individual
position of a sentence. For speech data, which for the corpora in the
CHILDES database consists of short and often disconnected utterances
across hours of recordings, the unigram entropy measure is unaffected by
context or lack thereof in utterances by the adults and children in the
corpora. We can directly compare any two positions within utterances to
determine the amount of uncertainty, and therefore information, on
average contained by words within that position of utterances. We are
applying the same approach as in Genzel \& Charniak (2002), but within
sentences instead of across sentences.

\begin{CodeChunk}
\begin{figure*}[h]

{\centering \includegraphics{figs/providence_PE-1} 

}

\caption[Providence corpus unigram entropy]{Providence corpus unigram entropy}\label{fig:providence_PE}
\end{figure*}
\end{CodeChunk}

We also ran this analysis on Spanish and Mandarin corpora from CHILDES.
We used the Shiro corpus for Spanish Shiro (2000), which contains
prompted narratives individually collected from over a hundred
Venezualan schoolchildren, half from high SES backgrounds and half from
low SES backgrounds. We used the Zhou dinner corpus for Mandarin Chinese
(Li \& Zhou, 2015), which contains dinner conversations between 5 to
6-year-old children and their parents collected in Shanghai.

For each corpus, we accessed transcripts of the corpus provided through
the TalkBank system and computed over the Latin alphabet transcriptions
or transliterations of the original transcriptions. For Mandarin, we
used pinyin transliterations of the utterances in the corpus with
demarcated word boundaries. The Chinese characters used for writing
Mandarin do not normally demarcate word boundaries by spacing words
apart, and for normal Chinese writing including spaces between word
boundaries can have a negative effect on reading times Bai, Yan,
Liversedge, Zang, \& Rayner (2008).

\begin{CodeChunk}
\begin{figure*}[h]

{\centering \includegraphics{figs/shiro_PE-1} 

}

\caption[Shiro corpus entropy]{Shiro corpus entropy}\label{fig:shiro_PE}
\end{figure*}
\end{CodeChunk}

\begin{CodeChunk}
\begin{figure*}[h]

{\centering \includegraphics{figs/zhou_PE-1} 

}

\caption[Zhou Dinner corpus entropy]{Zhou Dinner corpus entropy}\label{fig:zhou_PE}
\end{figure*}
\end{CodeChunk}

\subsection{Results \& Analysis}\label{results-analysis}

The adult and child entropy curves track one another almost identically.
This is surprising because UID would predict that the young children in
the corpora who are not fully developed speakers would have more noise
in their distribution of information rates. This not only indicates a
robustness in the unigram entropy curve across speakers, but also across
ages and addressees. We also observed a robustness across corpora for
the same languages, and a robustness across utterance lengths within the
same corpus's entropy curve. This shows that the concept of an ``entropy
curve'' for a specific language is well-founded when considering speech
data.

We found a distinct three-step distribution for English and Spanish
CHILDES corpora, with a slight dip in the penultimate position of each
sentence. The final position of utterances in child-directed speech is
known to be important, dating back to Aslin (1993). The Mandarin corpus
entropy curve, by comparison, has a noticeably lower positional entropy
values in utterance-final positions than in utterance-penultimate
positions.

We attribute the penultimate dip in the English and Spanish entropy
curves to the fact that most of the utterances in the CHILDES English
and Spanish corpora we examined had a determiner such as ``the'' or
``a'' in the second-to-last position of utterances. The beginnings of
utterances in the English and Spanish CHILDES data were usually pronouns
or grammatical subjects, while the final words were grammatical objects
and had a great deal of variation in the exact word that appeared in the
utterance-final position.

These entropy curves are not what would be expected from UID. The
robustness of the three-step distribution for English and its
replication in Spanish do not resemble the affine function we would
expect from UID. The Mandarin entropy curve, which does not at all
resemble either the English/Spanish distribution or the predictions of
UID, suggests that the entropy curve can vary from language to language.
UID predicts that each language should have a similar distribution.

\section{Experiment 3}\label{experiment-3}

The UID hypothesis also applies to written communication: we expect
people to communicate information at a uniform rate through writing as
well. We use Wikipedia as a source for written data, which provides two
advantages. One, the quantity of data in Wikipedia is large for each
language and two, there are hundreds of languages with Wikipedias. This
allows us to perform the entropy analysis on a much greater scale and to
directly compare the results of the entropy analysis on each language to
one another, and ultimately to predict what typological features of a
language help determine its entropy curve, if any. We will describe our
method of harvesting and distilling text data from each Wikipedia as
well as how we compare the entropy curves from each language to one
another.

\subsection{Methods}\label{methods}

Using Giuseppe Attardi's Wikiextractor tool
\footnote{https://github.com/attardi/wikiextractor}, we extract text
corpora from Wikipedia by downloading a stored collection of Wikipedia
entries in each langauge and randomly selecting several thousand
articles from each Wikipedia language. Each language corpus was cleaned
and limited to sentences between \(6\) and \(50\) words. Similar to our
process for the Switchboard and CHILDES spoken corpora, we divided each
corpus by sentence length, and then computed the unigram entropy measure
on each word position within each sentence length. We wanted to directly
compare and classify the unigram entropy curves of the languages from
Wikipedia. We computed three slope treatments of each curve. In the
\emph{absolute} treatment, with sentence length denoted as \(n\), we
computed the slope between positions \(1\) and \(2\), positions \(2\)
and \(3\), positions \(3\) and \(n-2\), positions \(n-2\) and \(n-1\)
and positions \(n-1\) and \(n\). For the short utterances appearing the
CHILDES speech corpora we examined, these appeared to be important
junctions in the distributions, with a seeming plateau in the middle of
the unigram entropy curve for each of the language corpora we examined
in CHILDES.

However, because the portion of sentences of length greater than \(10\)
in the Wikipedia corpora were significantly larger than the CHILDES
corpora, then we also computed relative slope treatments. In the
\emph{relative 5} treatment, we computed the slopes between \(0\%\) and
\(20\%\), \(20\%\) and \(40\%\), \(40\%\) and \(60\%\), \(60\%\) and
\(80\%\) and \(80\%\) and \(100\%\) of the relative word positions in
each sentence. When one of these percentages was not a whole number,
then the closest whole number position was used instead for slope
calculation. In the \emph{relative 10} treatment, we computed the slopes
between every \(10\%\) of the relative word positions in each sentence.
Each comparative slope within each treatment was averaged together
between different sentence lengths, for example in the \emph{relative 5}
treatment then all of the \(0\%\) to \(20\%\) slopes were averaged
together. This created three treatments for the entropy curve for each
langauge in the Wikipedia database.

\subsection{Results and Analysis}\label{results-and-analysis}

In the \emph{absolute} and \emph{relative 5} treatments, each language
is embedded in \(5\)-dimensional space. In the \emph{relative 10}
treatment, each language is embedded in \(10\)-dimensional space. This
allows for direct comparison between languages on Wikipedia using cosine
similarity and moreover for unsupervised clustering analysis to compare
the results of our position-wise analysis on datasets from Wikipedia to
known typological features within the languages. To determine which
phonological, morphological and syntactic features affected the
embedding of a language in the Wikipedia dataset, we use the linguistic
features in World Atlas of Language Structures (Dryer \& Haspelmath,
2013).

{[}Ideally do everything, but missing values in WALS, and how to compare
every combination of features{]} An ideal approach using all of the
corpora we can download from Wikipedia and all \(144\) features from the
WALS database. Cluster the languages used in the Wikipedia analysis on
the basis of WALS features and then directly compare the WALS clustering
results with the results of the hierarchical clustering on the Wikipedia
slope data for each of the different treatments using clustering
similarity evaluation methods such as the Rand index (Rand, 1971).
However, the problem then arises of which combination of WALS features
and how many features to include in the clustering analysis. This is a
computationally intractable operation. Additionallly, deciding how many
clusters to use in an unsupervised clustering analysis is an unsolved
problem in machine learning.

We instead check the effects of individual features on the embeddings of
languages in the different treatments. We computed pairwise cosine
similarity between each pair of language vectors within a treatment. For
a subset of WALS features which had values entered for most of the
languages we obtained from Wikipedia, we used a generalized linear model
to see whether the cosine similarity between languages mattered in
predicting if the languages shared the same value for a WALS feature.
For eight features and \(45\) languages, we found this to be true. The
table below shows the results for the generalized linear model we
computed using WALS features and the absolute treatment.

\begin{table}[tb]
\centering
\begin{tabular}{llrrrr}
  \hline
feature & term & estimate & std.error & statistic & p.value \\ 
  \hline
83A & cosine & 1.84 & 0.01 & 178.22 & 0.00 \\ 
  95A & cosine & 1.90 & 0.01 & 170.98 & 0.00 \\ 
  81A & cosine & 1.51 & 0.01 & 153.70 & 0.00 \\ 
  97A & cosine & 1.58 & 0.01 & 136.81 & 0.00 \\ 
  144A & cosine & 0.88 & 0.01 & 74.43 & 0.00 \\ 
  138A & cosine & 0.40 & 0.01 & 46.12 & 0.00 \\ 
  87A & cosine & 0.33 & 0.01 & 39.91 & 0.00 \\ 
  143A & cosine & 0.37 & 0.01 & 39.74 & 0.00 \\ 
  82A & cosine & 0.03 & 0.01 & 3.23 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}

Cosine distance played some role in the feature determination. The top
four features all characterize word order, which indicates that for this
subset of features and languages that the embeddings in the slope space
are related to the WALS features. Therefore typological features play a
role in determining the positional entropy values for a language. This
means that the entropy curves are in some way structured by the
syntactic, morphological and phonological features of a language.

\section{Discussion}\label{discussion}

In this paper, we have extended and applied a model from Yu et al.
(2016) and derived from Genzel \& Charniak (2002) for distinguishing
entropy at each word position within sentences. We have argued that the
outcomes of this model are derived from the information distribution of
a language. As this model varies cross-linguistically in terms of

Follow-up with WALS features using missing-value imputation. However
here are some of the problems with that.

Our work complements the approach of studies such as Aylett \& Turk
(2004), where languages are characterized by a rate of semantic
information per syllable. This body of work attests to languages
possessing different rates of information transfer based on typological
features as well.

We also propose follow-up within linguistic processing using evidence
from cross-linguistic eye-tracking. Research has indicated the effects
of surprisal on fixation duration during eye-tracking studies (Boston,
Hale, Kliegl, Patil, \& Vasishth, 2008; Demberg \& Keller, 2008; Smith
\& Levy, 2013); higher surprisal of a word is on average predictive of
higher fixation duration. The \emph{wrap-up effect} states that when a
person reads written text, he or she will process sentence-final words
more slowly on average then sentence-medial or sentence-initial words,
due to integrating information from the entire sentence to form a final
understanding of the sentence's meaning (Kuperman, Dambacher, Nuthmann,
\& Kliegl, 2010; Stowe, Kaan, Sabourin, \& Taylor, 2018). The wrap-up
effect is drawn from evidence in languages with a large final increase
in their entropy curve from our study, so perhaps the wrap-up effect
derives from the sentence-final increase in entropy curves observed in
English and other Germanic languages.

\section{Acknowledgements}\label{acknowledgements}

{[}Not here yet.{]}

\section{References}\label{references}

\setlength{\parindent}{-0.1in} \setlength{\leftskip}{0.125in} \noindent

\hypertarget{refs}{}
\hypertarget{ref-anderson1991}{}
Anderson, J. R. (1991). The adaptive nature of human categorization.
\emph{Psychological Review}, \emph{98}(3), 409.

\hypertarget{ref-aylett2004}{}
Aylett, M., \& Turk, A. (2004). The smooth signal redundancy hypothesis:
A functional explanation for relationships between redundancy, prosodic
prominence, and duration in spontaneous speech. \emph{Language and
Speech}, \emph{47}(1), 31--56.

\hypertarget{ref-bai2008}{}
Bai, X., Yan, G., Liversedge, S. P., Zang, C., \& Rayner, K. (2008).
Reading spaced and unspaced chinese text: Evidence from eye movements.
\emph{Journal of Experimental Psychology: Human Perception and
Performance}, \emph{34}(5), 1277.

\hypertarget{ref-boston2008}{}
Boston, M. F., Hale, J., Kliegl, R., Patil, U., \& Vasishth, S. (2008).
Parsing costs as predictors of reading difficulty: An evaluation using
the potsdam sentence corpus. \emph{Journal of Eye Movement Research},
\emph{2}(1).

\hypertarget{ref-brown1973}{}
Brown, R. (1973). \emph{A first language: The early stages.} Harvard U.
Press.

\hypertarget{ref-clear1993british}{}
Clear, J. H. (1993). The british national corpus. \emph{The Digital
World}, 163--187.

\hypertarget{ref-demberg2008}{}
Demberg, V., \& Keller, F. (2008). Data from eye-tracking corpora as
evidence for theories of syntactic processing complexity.
\emph{Cognition}, \emph{109}(2), 193--210.

\hypertarget{ref-frank2008speaking}{}
Frank, A. F., \& Jaeger, T. F. (2008). Speaking rationally: Uniform
information density as an optimal strategy for language production. In
\emph{Proceedings of the annual meeting of the cognitive science
society} (Vol. 30).

\hypertarget{ref-genzel2002}{}
Genzel, D., \& Charniak, E. (2002). Entropy rate constancy in text. In
\emph{Proceedings of the 40th annual meeting on association for
computational linguistics} (pp. 199--206). Association for Computational
Linguistics.

\hypertarget{ref-jaeger2010}{}
Jaeger, T. F. (2010). Redundancy and reduction: Speakers manage
syntactic information density. \emph{Cognitive Psychology},
\emph{61}(1), 23--62.

\hypertarget{ref-jaeger2013}{}
Jaeger, T. F., \& Snider, N. E. (2013). Alignment as a consequence of
expectation adaptation: Syntactic priming is affected by the prime's
prediction error given both prior and recent experience.
\emph{Cognition}, \emph{127}(1), 57--83.

\hypertarget{ref-jain2018}{}
Jain, A., Singh, V., Ranjan, S., Rajkumar, R., \& Agarwal, S. (2018).
Uniform information density effects on syntactic choice in hindi. In
\emph{Proceedings of the workshop on linguistic complexity and natural
language processing} (pp. 38--48).

\hypertarget{ref-jelinek1976}{}
Jelinek, F. (1976). Continuous speech recognition by statistical
methods. \emph{Proceedings of the IEEE}, \emph{64}(4), 532--556.

\hypertarget{ref-kuperman2010}{}
Kuperman, V., Dambacher, M., Nuthmann, A., \& Kliegl, R. (2010). The
effect of word position on eye-movements in sentence and paragraph
reading. \emph{The Quarterly Journal of Experimental Psychology},
\emph{63}(9), 1838--1857.

\hypertarget{ref-levy2007}{}
Levy, R. P., \& Jaeger, T. F. (2007). Speakers optimize information
density through syntactic reduction. In \emph{Advances in neural
information processing systems} (pp. 849--856).

\hypertarget{ref-macwhinney2014}{}
MacWhinney, B. (2014). \emph{The childes project: Tools for analyzing
talk, volume ii: The database}. Psychology Press.

\hypertarget{ref-mahowald2013}{}
Mahowald, K., Fedorenko, E., Piantadosi, S. T., \& Gibson, E. (2013).
Info/information theory: Speakers choose shorter words in predictive
contexts. \emph{Cognition}, \emph{126}(2), 313--318.

\hypertarget{ref-piantadosi2011}{}
Piantadosi, S. T., Tily, H., \& Gibson, E. (2011). Word lengths are
optimized for efficient communication. \emph{Proceedings of the National
Academy of Sciences}, \emph{108}(9), 3526--3529.

\hypertarget{ref-piantadosi2012}{}
Piantadosi, S. T., Tily, H., \& Gibson, E. (2012). The communicative
function of ambiguity in language. \emph{Cognition}, \emph{122}(3),
280--291.

\hypertarget{ref-shannon1948}{}
Shannon, C. E. (1948). A mathematical theory of communication.
\emph{Bell System Technical Journal}, \emph{27}(3), 379--423.

\hypertarget{ref-shiro2000}{}
Shiro, M. (2000). Diferencias sociales en la construcci칩n del`` yo'' y
del`` otro'': Expresiones evaluativas en la narrativa de ni침os
caraque침os en edad escolar. In \emph{Lengua, discurso, texto: I simposio
internacional de an치lisis del discurso} (pp. 1303--1318). Visor.

\hypertarget{ref-simons2018}{}
Simons, G. F., \& Fenning, C. D. (Eds.). (2018). \emph{Ethnologue:
Languages of the world, 21st edition}. Dallas, Texas: SIL International.

\hypertarget{ref-smith2013}{}
Smith, N. J., \& Levy, R. (2013). The effect of word predictability on
reading time is logarithmic. \emph{Cognition}, \emph{128}(3), 302--319.

\hypertarget{ref-stowe2018}{}
Stowe, L. A., Kaan, E., Sabourin, L., \& Taylor, R. C. (2018). The
sentence wrap-up dogma. \emph{Cognition}, \emph{176}, 232--247.

\hypertarget{ref-xu2018}{}
Xu, Y., \& Reitter, D. (2018). Information density converges in
dialogue: Towards an information-theoretic model. \emph{Cognition},
\emph{170}, 147--163.

\hypertarget{ref-yu2016}{}
Yu, S., Cong, J., Liang, J., \& Liu, H. (2016). The distribution of
information content in english sentences. \emph{arXiv Preprint
arXiv:1609.07681}.

\hypertarget{ref-zhan2018}{}
Zhan, M., \& Levy, R. (2018). Comparing theories of speaker choice using
a model of classifier production in mandarin chinese. In
\emph{Proceedings of the 2018 conference of the north american chapter
of the association for computational linguistics: Human language
technologies, volume 1 (long papers)} (Vol. 1, pp. 1997--2005).

\bibliographystyle{apacite}


\end{document}
