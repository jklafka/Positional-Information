---
title: "Information is Not Communicated Uniformly: Evidence from Spoken and Written Corpora"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Josef Klafka \and Daniel Yurovsky \\
         \texttt{\{jklafka, yurovsky\}@uchicago.edu} \\
        Department of Psychology \\ University of Chicago}

abstract: >
    We provide evidence against the popular Uniform Information Density hypothesis (Levy & Jaeger, 2007) which proposes that information is transmitted at a constant rate close to channel capacity in human communication. Using a method based on the original Genzel & Charniak (2002) entropy measure, we construct a word-level model for entropy applicable to both spoken and written corpora. We apply this model to corpora from Wikipedia in well over a hundred languages. We find that not only is the Uniform Information Density hypothesis prediction wrong, but also that the by-word entropy distribution of a language is related to the typological features of the language. We use this evidence to suggest that people do not communicate information at a uniform rate, but that information distribution varies from language to language based on phonogical, morphological and syntactic features.
    
keywords: >
    Entropy; information; information theory; communication
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/', echo=F, warning=F, cache=T, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(tidyverse)
library(xtable)
```

Over 7,000 languages are spoken around the modern world [@simons2018]. These language vary along many dimensions, but all share a core goal: communicating information. If speakers and writers of these languages act near-optimally to achieve their communicative goals, regularities of use across these diverse languages can be explained by a rational theory of communication [@anderson1991]. 
Information theory, a mathematical framework developed by @shannon1948 to describe the transmission and decoding of signals, has been a unifying language for the recent development of such theories in human and machine language processing [@jelinek1976;@levy2007]. 

These theories model the process of communication as transmission of information over a noisy channel. The producer begins with an intended meaning, packages this meaning into a linguistic format, and then sends it to the receiver over a communcative channel. The receiver must then decode from the signal they receive on their end of the chanel the producer's intended meaning. The problem is that the channel is noisy, and sometimes the signal can get corrupted (e.g. the producer can misspeak, or the receiver can mishear). In order to maximize the probability that the correct meaning is transmitted, these theories predict that producers should choose linguistic messages that keep the rate of information across words constant. The intuition is that if the receiver misperceives a word, and that word contains most of the information in the sentence, then the communication will have failed. Because producers cannot predict which word a speaker will mishear, their best strategy is spread the information evenly across all of the words in a sentence, i.e. maintain *uniform information density* [@genzel2002; @levy2007].


**Evidence for UID here. Maybe see if you can present this work at a slightly higher level**
Their evidence concerns the use or lack of use of “that”-complementizers in introducing English relative clauses. When the word preceding the relative clause or the initial word of the relative clause has high information content, then the authors argue that speakers are more likely to use the complementizer. The authors use adult-adult conversations from the Penn Treebank corpus. 

UID has been applied broadly over the past decade, to determining whether linguistic alignment takes place [@jaeger2013], Zipfian word length distributions [@piantadosi2011], communication efficiency [@mahowald2013], dialogue and turn-taking [@xu2018] and the significance of ambiguity in language [@piantadosi2012], among other research. 

**Evidence against UID here. Again, can you present this at a higher level?**
However, other recent work has contradicted the UID hypothesis. @zhan2018 study Mandarin Chinese classifier use for specific and general Mandarin classifiers. Specific classifiers are specific to certain nouns, and UID would predict that specific classifiers would appear when the production of the corresponding noun is easier, to spread the information contained by the classifier and noun less densely over time. General classifiers can be applied to any noun and contain no special information content. Zhan and Levy find that speakers use specific classifiers more often in cases where the production of the corresponding noun is more difficult than when the production is easier; speakers do not avoid unexpected peaks in information content for their listeners, but instead maximize their ease of production. 
Similar to the original @levy2007, @zhan2018 focuses on information distribution at particular points in a sentence. By contrast, @jain2018 examine word order across spoken sentences in Hindi, a freer word order language than English, and find that information density has no significant effect on word order. 

Recently, @yu2016 developed a more direct test of the Uniform Information Density hypothesis, applying the logic used by @genzel2002 to look at the distribution of information *within* individual sentences. **SOME HIGHER LEVEL DESCRIPTION HERE, WE'll GET INTO THE DETAILS BELOW**

Because people process language incrementally--using the previous words in a sentence to predict the words that will come next, the amount of information that a word contains when seen in isolation should increase over the course of a sentence. Analyzing a large corpus of written English, they find a different pattern: Entropy increases over the first few words of an utterance and then remains constant until the final word where it again jumps up (see Figure \ref{fig:our_replication}). @yu2016 conclude that the Uniform Information Density hypothesis must not hold for medial words in a sentence.

We extend and generalize @yu2016 in two ways: we confirm that this same pattern is found in spoken English--in both adult and child speakers. We then examine entropy curves cross-linguistically, finding a diversity of shapes across the world's languages. Finally, we show that entropy curves are predictable from the structure of individual languages, namely word order. Taken together, our results suggest a refinement of the Uniform information density hypothesis: Speakers may choose to order words in order to preserve uniform uniformation, but they must do so under prior constraints on word order imposed by the language they speak.




The UID hypothesis predicts that information transmission rate will tend towards uniformity in both speech and text. We begin by examining speech to determine if we obtain the entropy curve shape we expect for spoken communication. We begin with the same source as @levy2007 and @jaeger2010, the Switchboard corpus of adult telephone conversations. We also examine child-adult conversations in the CHILDES TalkBank [@brown1973; @macwhinney2014] corpora database of spoken adult-child conversations in multiple languages. Children are not fully developed speakers, so we want to compare the entropy curve we obtain by computing over the utterances in the CHILDES corpora to the utterances in the adult-adult Switchboard corpus. 

## Methods


@yu2016 challenges the UID hypothesis through examining an entropy measure across sentence positions within the text portion of the English-language British National Corpus (BNC, CITE). They partition the corpus by sentence length in number of words. For each word position $X$ of sentences of length $k$, they define $w$ as a unique word occurring in position $X$. They define $p(w)$ as the number of times word $w$ occurs in position $X$ divided by the number of total words that occur in position $X$ i.e. the number of sentences of length $k$. Then $p(w)$ is the probability of obtaining word $w$ by choosing a word at random in position $X$ in sentences of length $k$. 

$$H(X) = \sum\limits_w p(w)\log\big(p(w)\big)$$
With this measure, @yu2016  compute the unigram entropy at each position of sentences of each length within the corpus. The result of this method can be plotted for each utterance length as an *entropy curve*, which can be visually compared across utterance length to observe the how the unigram entropy changes across absolute positions in each of the utterances. @genzel2002 similarly examine a unigram entropy measure on sentences, and found that entropy at the sentence level increases linearly with sentence index within a corpus. UID applies this uniformity of entropy rate in sentences to all levels of speech, and so the @yu2016, which examines text at the word level, should find an affine function at the word level. 

Switchboard [To be put in]

We used the Brown and Providence English corpora from CHILDES. The Brown corpus contains individual recordings of conversations between three children between 1.5 and 6-years-old and their families in their homes. The Providence corpus recorded interactions between children between 1 and 3 years old and their parents in the home. We divided each corpus by speaker into child and non-child categories. We further divided the corpora by utterance length, so that all sentences of length $k$ (e.g. $6$) were grouped together. Finally, within each utterance length, we computed the unigram entropy measure for each position.  

In the English CHILDES corpora. The entropy curves capture individual variation across positions in utterances of the same length. This allows us to directly observe and judge the amount of variation in words that appear in an individual position of a sentence. For speech data, which for the corpora in the CHILDES database consists of short and often disconnected utterances across hours of recordings, the unigram entropy measure is unaffected by context or lack thereof in utterances by the adults and children in the corpora. We can directly compare any two positions within utterances to determine the amount of uncertainty, and therefore information, on average contained by words within that position of utterances. We are applying the same approach as in @genzel2002, but within sentences instead of across sentences. 

Plots for the entropy distributions are below. 

```{r brown_PE, fig.env = "figure*", fig.pos = "h", fig.width=6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Brown corpus entropy"}
img <- png::readPNG("figs/positional_entropy_brown.png")
grid::grid.raster(img)
```

```{r providence_PE, fig.env = "figure*", fig.pos = "h", fig.width=6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Providence corpus unigram entropy"}
img <- png::readPNG("figs/Prov_corpus_result.png")
grid::grid.raster(img)
```

We also ran this analysis on Spanish and Mandarin corpora from CHILDES. We used the Shiro corpus for Spanish (Shiro, 2000), which contains prompted narratives individually collected from over a hundred Venezualan schoolchildren, half from high SES backgrounds and half from low SES backgrounds. We used the Zhou dinner corpus for Mandarin Chinese (Li & Zhou, 2015), which contains dinner conversations between 5 to 6-year-old children and their parents collected in Shanghai. 

For each corpus, we accessed transcripts of the corpus provided through the TalkBank system and computed over Roman alphabet transcriptions or transliterations of the original transcriptions. For Mandarin, we used pinyin transliterations of the utterances in the corpus with demarcated word boundaries, and for Japanese we used romanji (Roman alphabet) transliterations of words in the corpus. The Chinese characters used for writing Mandarin do not normally demarcate word boundaries by spacing words apart, and for normal Chinese writing including spaces between word boundaries can have a negative effect on reading times (Bai et al, 2008). 

```{r shiro_PE, fig.env = "figure*", fig.pos = "h", fig.width=6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Shiro corpus entropy"}
img <- png::readPNG("figs/German_unigram_entropies_with_stemming.png")
grid::grid.raster(img)
```


```{r zhou_PE, fig.env = "figure*", fig.pos = "h", fig.width=6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Zhou Dinner corpus entropy"}
img <- png::readPNG("figs/Mandarin_unigram_entropies.png")
grid::grid.raster(img)
```

## Results & Analysis
The adult and child entropy curves track one another almost identically. This is surprising because UID would predict that the young children in the corpora who are not fully developed speakers would have more noise in their distribution of information rates. This not only indicates a robustness in the unigram entropy curve across speakers, but also across ages and addressees. We also observed a robustness across corpora for the same languages, and a robustness across utterance lengths within the same corpus's entropy curve. This shows that the concept of an "entropy curve" for a specific language is well-founded when considering speech data.  

We found a distinct three-step distribution for English and Spanish CHILDES corpora, with a slight dip in the penultimate position of each sentence. The final position of utterances in child-directed speech is known to be important, dating back to Aslin (1993). The Mandarin corpus entropy curve, by comparison, has a noticeably lower positional entropy values in utterance-final positions than in utterance-penultimate positions. 

We attribute the penultimate dip in the English and Spanish entropy curves to the fact that most of the utterances in the CHILDES English and Spanish corpora we examined had a determiner such as "the" or "a" in the second-to-last position of utterances. The beginnings of utterances in the English and Spanish CHILDES data were usually pronouns or grammatical subjects, while the final words were grammatical objects and had a great deal of variation in the exact word that appeared in the utterance-final position. 

These entropy curves are not what would be expected from UID. The robustness of the three-step distribution for English and its replication in Spanish do not resemble the affine function we would expect from UID. The Mandarin entropy curve, which does not at all resemble either the English/Spanish distribution or the predictions of UID, suggests that the entropy curve can vary from language to language. UID predicts that each language should have a similar distribution. 

# Written data

The UID hypothesis also applies to written communication: we expect people to communicate information at a uniform rate through writing as well. We use Wikipedia as a source for written data, which provides two advantages. One, the quantity of data in Wikipedia is large for each language and two, there are hundreds of languages with Wikipedias. This allows us to perform the entropy analysis on a much greater scale and to directly compare the results of the entropy analysis on each language to one another, and ultimately to predict what typological features of a language help determine its entropy curve, if any. We will describe our method of harvesting and distilling text data from each Wikipedia as well as how we compare the entropy curves from each language to one another. 

## Methods

Using Giuseppe Attardi's Wikiextractor tool \footnote{https://github.com/attardi/wikiextractor}, we extracted the text corpora for $165$ languages from Wikipedia by downloading a stored collection of Wikipedia entries in each langauge and randomly selecting several thousand articles from each Wikipedia language. Each language corpus was cleaned and limited to sentences between $6$ and $50$ words. Similar to our process for the Switchboard and CHILDES spoken corpora, we divided each corpus by sentence length, and then computed the unigram entropy measure on each word position within each sentence length. We wanted to directly compare and classify the unigram entropy curves of the languages from Wikipedia. We computed three slope treatments of each curve. In the *absolute* treatment, with sentence length denoted as $n$, we computed the slope between positions $1$ and $2$, positions $2$ and $3$, positions $3$ and $n-2$, positions $n-2$ and $n-1$ and positions $n-1$ and $n$. For the short utterances appearing the CHILDES speech corpora we examined, these appeared to be important junctions in the distributions, with a seeming plateau in the middle of the unigram entropy curve for each of the language corpora we examined in CHILDES. 

However, because the portion of sentences of length greater than $10$ in the Wikipedia corpora were significantly larger than the CHILDES corpora, then we also computed relative slope treatments. In the *relative 5* treatment, we computed the slopes between $0\%$ and $20\%$, $20\%$ and $40\%$, $40\%$ and $60\%$, $60\%$ and $80\%$ and $80\%$ and $100\%$ of the relative word positions in each sentence. When one of these percentages was not a whole number, then the closest whole number position was used instead for slope calculation. In the *relative 10* treatment, we computed the slopes between every $10\%$ of the relative word positions in each sentence. Each comparative slope within each treatment was averaged together between different sentence lengths, for example in the *relative 5* treatment then all of the $0\%$ to $20\%$ slopes were averaged together. This created three treatments for the entropy curve for each langauge in the Wikipedia database. 

[Put in an illustration of the slope sections and treatments here?]

## Results and Analysis

In the *absolute* and *relative 5* treatments, each language is embedded in $5$-dimensional space. In the *relative 10* treatment, each language is embedded in $10$-dimensional space. This allows for direct comparison between languages on Wikipedia using cosine similarity and moreover for unsupervised clustering analysis to compare the results of the Wikipedia entropy analysis to known typological features within the langauges in the Wikipedia dataset. To determine which phonological, morphological and syntactic features affected the embedding of a language in the Wikipedia dataset, we looked at the list of $144$ linguistic features in World Atlas of Language Structures (Dryer & Haspelmath, 2013). We limited the languages in the WALS database to only those in our Wikipedia dataset and performed missing-value imputation to obtain the features not coded in WALS for the languages from Wikipedia. 

One approach is to cluster the languages used in the Wikipedia analysis on the basis of WALS features and then directly compare the WALS clustering results with the results of the hierarchical clustering on the Wikipedia slope data for each of the different treatments using clustering similarity evaluation methods such as the Rand index (Rand, 1971). However, the problem then arises of which combination of WALS features and how many features to include in the clustering analysis. This is a computationally intractable operation. Additionallly, deciding how many clusters to use in an unsupervised clustering analysis is an unsolved problem in machine learning. 

We instead check the effects of individual features on the embeddings of languages in the different treatments. We computed pairwise cosine similarity between each pair of language vectors within a treatment. For a subset of WALS features which had values entered for most of the languages we obtained from Wikipedia, we used a generalized linear model to see whether the cosine similarity between languages mattered in predicting if the languages shared the same value for a WALS feature. For eight features and $45$ languages, we found this to be true. The table below shows the results for the generalized linear model we computed using WALS features and the absolute treatment. 

```{r tables, results="asis", tab.env = "table"}
glm_model <- read_csv("glm.csv") %>%
    select(-X1)

tab1 <- xtable::xtable(glm_model)
print(tab1, type="latex", comment = F, table.placement = "tb",
      include.rownames = F)
```

Cosine distance played some role in the feature determination. The top features were $83$, $81$, $95$ and $97$, which all characterize word order. This makes intuitive sense, and indicates that for this small subset of features and subset of languages that the embeddings in the slope space are related to the WALS features. Therefore typological features play a role in determining the positional entropy values for a language. This means that the entropy curves are in some way structured by the syntactic, morphological and phonological features of a language. 

# Conclusions 
In this paper, we have characterized and applied a model based on Genzel & Charniak (2002) for distinguishing entropy at each word position within sentences. In doing so, we have provided evidence against the Uniform Information Density hypothesis. Our work indicates that information is not distributed uniformly throughout an utterance regardless of languages, but that languages possess a characteristic information distribution determined by their syntactic, morphological and phonological features. 

Our work has focused on words within sentences in both speech and text across typologically diverse languages. Other researchers have explored information rate cross-linguistically as a single number characterizing each language, most notably including Aylett & Turk (2004). That work characterizes information rate as information per unit time, and is complementary to the work we accomplish here. Languages may possess both an overall information transfer rate, showing the average amount of semantic information transmitted per syllable or word, and an entropy distribution indicative of the language's information distribution. 

Research has indicated the effects of surprisal on fixation duration during eye-tracking studies (Demberg & Keller, 2008) (Boston et al., 2008) (Smith & Levy, 2013). Higher surprisal of a word is on average predictive of higher fixation duration. The *wrap-up effect* states that when a person reads written text, he or she will process sentence-final words more slowly on average then sentence-medial or sentence-initial words, due to integrating information from the entire sentence to form a final understanding of the sentence's meaning (Stowe et al., 2018) (Kuperman et al., 2010). The wrap-up effect is drawn from evidence in English, German and Dutch, all of which are languages with a large final increase in their entropy curve from our study. We aim to see in the future if as a consequence of our work in this paper, the wrap-up effect falls out from the final jump in the entropy curves in some languages. We would analyze eye-tracking data from languages with low positional entropy in sentence-final positions. Since the lexical surprisal metric in eye-tracking and our positional entropy metric are both measures of log-probability surprisal, then in languages such as Hindi where the sentence-final positional entropy values are low we expect to find no wrap-up effect.   

# Acknowledgements

[Not here yet.]

# References

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
