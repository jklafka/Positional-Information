---
title: "Information Distribution Depends on Language-Specific Features"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Josef Klafka \and Daniel Yurovsky \\
         \texttt{\{jklafka, yurovsky\}@uchicago.edu} \\
        Department of Psychology \\ University of Chicago}

abstract: >
    We argue for a reformulation of the Uniform Information Density hypothesis @levy2007 in which the grammatical, phonological and morphological features of a language determine how information is distributed throughout utterances. We expand on a method from @yu2016 to compute entropy based on word position within utterances as a derivation of information distribution. We analyze adult and child spoken corpora as well as text corpora drawn from a variety of languages on Wikipedia. We find that each language has a robust, characteristic entropy curve across word position, and present evidence that these entropy curves are in part determined by typological features. We conclude that speakers may tend towards communicating information uniformly, but the distribution of information in their utterances is conditioned on the language they communicate in. 
    
keywords: >
    Entropy; information; information theory; communication
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/', echo=F, warning=F, cache=T, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(tidyverse)
library(xtable)
library(tidytext)
library(tidyboot)
library(entropy)
library(here)
library(feather)
library(magrittr)
library(childesr)
library(ggthemes)
library(gridExtra)
library(directlabels)

theme_set(theme_classic(base_size = 10))
```

# Introduction

Over 7,000 languages are spoken around the modern world [@simons2018]. These language vary along many dimensions, but all share a core goal: communicating information. If speakers and writers of these languages act near-optimally to achieve their communicative goals, regularities of use across these diverse languages can be explained by a rational theory of communication [@anderson1991]. 
Information theory, a mathematical framework developed by @shannon1948 to describe the transmission and decoding of signals, has been a unifying language for the recent development of such theories in human and machine language processing [@jelinek1976;@levy2007]. 

These theories model the process of communication as transmission of information over a noisy channel. The producer begins with an intended meaning, packages this meaning into language, and then sends the meaning to their intended receiver over a communcative channel. The receiver must then decode from the signal they receive on their end of the channel the producer's intended meaning. The problem is that the channel is noisy, and sometimes the signal can get corrupted (e.g. the producer can misspeak, or the receiver can mishear). In order to maximize the probability that the correct meaning is transmitted, these theories predict that producers should choose linguistic messages that keep the rate of information across words constant. The intuition is that if the receiver misperceives a word, and that word contains most of the information in the sentence, then the communication will have failed. Because producers cannot predict which word a speaker will mishear, their best strategy is spread the information evenly across all of the words in a sentence, i.e. maintain *uniform information density* [@genzel2002; @levy2007].

<!-- **Evidence for UID here. Maybe see if you can present this work at a slightly higher level** -->
The original evidence in @levy2007 finds that the insertion of complementizers in relative clauses in English corresponds to where neighboring words have high information content. Similarly, @frank2008 argues that contradictions in English such as "you're" do not occur when neighboring words are highly informative. The evidence in favor of UID largely been situation-specific and English-language driven, while the hypothesis itself has been applied broadly over the past decade. Applications include determining whether linguistic alignment takes place [@jaeger2013], Zipfian word length distributions [@piantadosi2011], communication efficiency [@mahowald2013], dialogue and turn-taking [@xu2018] and the significance of ambiguity in language [@piantadosi2012], among other research. 

<!-- **Evidence against UID here. Again, can you present this at a higher level?** -->
However, other recent work has contradicted the UID hypothesis. Similar to the original @levy2007, @zhan2018 focuses on information distribution at particular points in sentences. @zhan2018 finds that more information-rich classifiers in Mandarin Chinese are produced when production of the neighboring noun is difficult, not when the information content is high. @jain2018 examine word order across spoken sentences in Hindi, a freer word order language than English, and find that information density has no significant effect on word order. 

Recently, @yu2016 developed a more direct test of the Uniform Information Density hypothesis, applying the logic used by @genzel2002 to look at the distribution of information *within* individual sentences. <!-- **SOME HIGHER LEVEL DESCRIPTION HERE, WE'll GET INTO THE DETAILS BELOW** -->Because people process language incrementally--using the previous words in a sentence to predict the words that will come next--the amount of information that a word contains when seen in isolation should increase over the course of a sentence. Analyzing a large corpus of written English, they find a different pattern: Entropy increases over the first few words of an utterance and then remains constant until the final word where it again jumps up (see Figure \ref{fig:our_replication}). @yu2016 conclude that the Uniform Information Density hypothesis must not hold for medial words in a sentence.

We extend and generalize @yu2016 in three ways: we confirm that this same pattern is found in spoken English--in both adult and child speakers. We then examine entropy curves cross-linguistically, finding a diversity of shapes across the world's languages. Finally, we show that entropy curves are predictable from the structure of individual languages, namely word order. Taken together, our results suggest a refinement of the Uniform Information Density hypothesis: speakers may structure their utterances to optimize information density, but they must do so under the constraints of the language they use to communicate.

The UID hypothesis predicts that information transmission rate will tend towards uniformity in both speech and text. We begin by examining speech to determine if we obtain the entropy curve shape we expect for spoken communication. We begin with the same source as @levy2007 and @jaeger2010, the Switchboard corpus of adult telephone conversations. We also examine child-adult conversations in the CHILDES TalkBank [@demuth2006; @macwhinney2014] corpora database of spoken adult-child conversations in multiple languages. Children are not fully developed speakers, so we want to compare the entropy curve we obtain by computing over the utterances in the CHILDES corpora to the utterances in the adult-adult Switchboard corpus. Finally, we extend our cross-linguistic findings on a small number of languages from the CHILDES experiment to a direct comparison of several dozen languages in Wikipedia. 

## General methods

Here we will describe our adaptation of the by-word entropy method in @yu2016, which uses the text portion of the British National Corpus [@clear1993]. We replicate their results at the end of this subsection. Given a text or speech corpus divided into individual utterances, we partition the corpus by utterance length in number of words. For each word position $X$ of utterances of length $k$, we define $w$ as a unique word occurring in position $X$. We further define $p(w)$ as the number of times word $w$ occurs in position $X$, divided by the number of total words that occur in position $X$ i.e. the number of sentences of length $k$. This creates a probability distribution over the words occurring in position $X$, and computing the Shannon entropy $H(X)$ @shannon1948 of this probability distribution gives the positional entropy of position $X$ in utterances of length $k$.

$$H(X) = \sum\limits_w p(w)\log\big(p(w)\big)$$

With this measure, we compute the unigram entropy at each position of sentences of each length within the corpus. The result of this method can be plotted for each utterance length as an *entropy curve*, which can be visually compared across utterance length to observe the how the unigram entropy changes across absolute positions in each of the utterances. @genzel2002 similarly examine a unigram entropy measure on sentences, and found that entropy at the sentence level increases linearly with sentence index within a corpus. UID applies this uniformity of entropy rate in sentences to all levels of speech, and so our method obtained from @yu2016, which examines text at the word level, should find an affine function at the word level. 

The entropy curves capture individual variation across positions in utterances of the same length. This allows us to directly observe and judge the amount of variation in words that appear in an individual position of a sentence. We can directly compare any two positions within utterances to determine the amount of uncertainty, and therefore information, on average contained by words within that position of utterances. We are applying the same approach as in @genzel2002, but within sentences instead of across sentences. 

The positional entropy at the beginning of sentences is low, then rises and plateaus for sentence-medial word positions before dropping slightly in the second-to-last position and rising again. However, this curve could simply be a result of the written words in the BNC. We will expand this analysis and replicate the result with a spoken American English corpus.

# Experiment 1

```{r entropy_func}
unigram_entropy <- function(sen_length, tokens) {
  tokens %>%
    filter(length == sen_length) %>%
    group_by(word_order, word) %>%
    summarise(n = n()) %>%
    tidyboot(summary_function = function(x) x %>% 
               summarise(entropy = entropy(n, unit = "log2")),
             statistics_functions = function(x) x %>%
             summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
    mutate(length = sen_length)
}
```

```{r bnc_and_switchboard, eval = F}

sb_tokens <- read_feather(here("../switchboard/switchboard.feather")) %>%
  mutate(length = str_count(value, pattern = " +") + 1) %>% 
  mutate(utterance_id = 1:n()) %>%
  unnest_tokens(word, value, token = stringr::str_split, pattern = " +") %>%
  group_by(utterance_id) %>%
  mutate(word_order = 1:n()) 

bnc10_tokens <- read_feather(here("../bnc/bnc10.feather")) %>%
  mutate(word = as.numeric(as.factor(word)))
bnc20_tokens <- read_feather(here("../bnc/bnc20.feather")) %>%
  mutate(word = as.numeric(as.factor(word)))
bnc30_tokens <- read_feather(here("../bnc/bnc30.feather")) %>%
  mutate(word = as.numeric(as.factor(word)))

switchboard_entropies <- map_df(2:10, ~unigram_entropy(.x, sb_tokens)) 

bnc10_entropies <- unigram_entropy(10, bnc10_tokens)
bnc20_entropies <- unigram_entropy(20, bnc20_tokens)
bnc30_entropies <- unigram_entropy(30, bnc30_tokens)
bnc_entropies <- bind_rows(bnc10_entropies, bnc20_entropies, bnc30_entropies)

write_csv(bnc_entropies, here("data/bnc_entropies.csv"))
write_csv(switchboard_entropies, here("data/switchboard_entropies.csv"))
```


We replicated the @yu2016 results and found the same characteristic three-step distribution as they did for the BNC entropy curves. 

We first used the same source for data as @levy2007, the Switchboard corpus of American telephone conversations @godfrey1992. The Switchboard corpus contains telephone conversations of American English between strangers, with individual utterances ranging from one word to several dozen words. We divided the Switchboard corpus by utterance length and computed the positional entropy for each position within each sentence length, then plotted the result as an entropy curve.  

## Results and Analysis

Our plots for three representative utterance lengths are below. We found the same characteristic shape for the American English Switchboard corpus as in our replication of the @yu2016 results for the BNC. 

```{r read_and_plot_switchboard, fig.env = "figure*", fig.width=7, fig.height=4, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Representative Entropy Curves for the British National Corpus (top) and Switchboard (bottom). Points average entropies, error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}

bnc_entropies <- read_csv(here("data/bnc_entropies.csv"))

switchboard_entropies <- read_csv(here("data/switchboard_entropies.csv")) %>%
  filter(length %in% c(5, 7, 9))

bnc_plot <- ggplot(bnc_entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length, scales = "free_x") + 
  xlab("British National Corpus utterance position") +
  ylab("Positional entropy") +
  geom_pointrange(size = .25) +
  geom_line(size = .25) +
  theme_classic(base_size = 10)

sw_plot <- ggplot(switchboard_entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length, scales = "free_x") + 
  xlab("Switchboard utterance position") +
  ylab("Positional entropy") +
  geom_pointrange(size = .25) +
  geom_line(size = .25) +
  theme_classic(base_size = 10)

grid.arrange(bnc_plot, sw_plot, nrow = 2)
```

The shape we find in these two English data is notable for a number of reasons, but what we find most interesting is one, the shape of the distribution does not follow our predictions from UID and two, the distribution is robust across an unrelated pair of spoken and written English corpora. This suggests that the entropy curve is characteristic of the English language at least, and is not a specific effect of computing positional entropies in the written British English texts in the BNC. For our next step, we want to expand the positional entropy analysis to child-adult conversational corpora and languages beyond English to see if the same three-step distribution appears. 

# Experiment 2

After replicating the @yu2016 results using the spoken conversation corpus from @levy2007, we want to determine whether the entropy curves will remain robust over child-adult conversational corpora. We also wanted to see whether the curves would show the same characteristic three-step shape across languages. To accomplish both of these goals, we used data from the CHILDES Talkbank system @macwhinney2014. 

The corpora in the CHILDES database consists of short and often disconnected utterances across hours of recordings, the unigram entropy measure is unaffected by context or lack thereof in utterances by the adults and children in the corpora. For our first corpus, we used the Providence English corpus @demuth2006. The Providence corpus recorded interactions between children between 1 and 3 years old and their parents in the home. We divided the corpus by speaker into child and non-child categories. We further divided the corpus by utterance length, so that all sentences of length $k$ (e.g. $6$) were grouped together. Finally, within each utterance length, we computed the unigram entropy measure for each position. Computation of positional entropy was identical to our general method after dividing for utterance length. 

We also ran this analysis on Spanish and Mandarin corpora from CHILDES. Spanish is an Indo-European language like English, possessing similar grammar, word order and numerous cognate word. Mandarin Chinese is typologically completed unrelated to English. We used the Shiro corpus for Spanish @shiro2000, which contains prompted narratives individually collected from over a hundred Venezualan schoolchildren, half from high SES backgrounds and half from low SES backgrounds. We used the Zhou dinner corpus for Mandarin Chinese @li2015, which contains dinner conversations between 5 to 6-year-old children and their parents collected in Shanghai. 

For each corpus, we accessed transcripts of the corpus provided through the TalkBank system and computed over the Latin alphabet transcriptions or transliterations of the original transcriptions. For Mandarin, we used pinyin transliterations of the utterances in the corpus with demarcated word boundaries. The Chinese characters used for writing Mandarin do not normally demarcate word boundaries by spacing words apart, and for normal Chinese writing including spaces between word boundaries can have a negative effect on reading times @bai2008. 

We expect that our entropy analysis on the Providence corpus will produce an identical entropy curve shape to the BNC and Switchboard curves. We expect similar results to English in the Spanish Shiro entropy curve, while we expect that our Mandarin corpus will produce a different entropy curve shape. 

## Results and Analysis

```{r childes_entropy, eval = F}

extract_stem_tokens <- function(role, min_length, max_length, utterances) {
  
  if (role == "child") {
    lengths <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1)

} else {
    lengths <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1)
}
  
  tokens <- lengths %>% 
    filter(length >= min_length) %>%
    filter(length <= max_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stem, token = stringr::str_split, pattern = " +") %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    filter(word_order <= length) %>% 
    summarise(n = n()) 
}

extract_gloss_tokens <- function(role,  min_length, max_length,  utterances) {
  
  if (role == "child") {
    lengths <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1)

} else {
    lengths <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1)
}
  
  tokens <- lengths %>% 
    filter(length >= min_length) %>%
    filter(length <= max_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, gloss, token = stringr::str_split, pattern = " +") %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    filter(word_order <= length) %>% 
    summarise(n = n()) 
}
 

prov_utterances <- get_utterances(corpus = "Providence") #stem or gloss
shiro_utterances <- get_utterances(corpus = "Shiro") #use gloss
zhou_utterances <- get_utterances(corpus = "ZhouDinner") #use stem, NOT gloss

prov_child_tokens <- extract_stem_tokens("child", 5, 9, prov_utterances) 
prov_adult_tokens <- extract_stem_tokens("not child",5, 9, prov_utterances) 

prov_child_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, prov_child_tokens)) %>%
  mutate(language = "English", person = "Child")
prov_adult_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, prov_adult_tokens)) %>%
  mutate(language = "English", person = "Adult")

shiro_child_tokens <- extract_gloss_tokens("child", 5, 9, shiro_utterances) 
shiro_adult_tokens <- extract_gloss_tokens("not child",5, 9, shiro_utterances) 

shiro_child_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, shiro_child_tokens)) %>%
  mutate(language = "Spanish", person = "Child")
shiro_adult_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, shiro_adult_tokens)) %>%
  mutate(language = "Spanish", person = "Adult")

zhou_child_tokens <- extract_stem_tokens("child", 5, 9, zhou_utterances) 
zhou_adult_tokens <- extract_stem_tokens("not child",5, 9, zhou_utterances) 

zhou_child_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, zhou_child_tokens)) %>%
  mutate(language = "Mandarin", person = "Child")
zhou_adult_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, zhou_adult_tokens)) %>%
  mutate(language = "Mandarin", person = "Adult")

childes_entropies <- bind_rows(prov_child_entropies, prov_adult_entropies, 
                               shiro_child_entropies, shiro_adult_entropies,
                               zhou_child_entropies, zhou_adult_entropies)

write_csv(childes_entropies, here("data/childes_entropies.csv"))
```

```{r plot_childes, fig.env = "figure*", fig.width=7, fig.height=4, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Representative Entropy Curves for Three Childes Corpora in English, Mandarin, and Spanish. Points average entropies, error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}
childes_entropies <- read_csv(here("data/childes_entropies.csv"))

ggplot(childes_entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper, color = person,
                      label = person)) +
  facet_grid(language ~ length, scales = "free") + 
  xlab("Utterance position") +
  ylab("Positional entropy") +
  geom_pointrange(size = .25) +
  geom_line(size = .25) +
  theme_classic(base_size = 10) + 
  scale_color_ptol() + 
  theme(legend.position = "none") + 
  geom_dl(method = list(dl.trans(x=x +2.2), "last.qp", cex=.7)) 
```

The adult and child entropy curves track one another almost identically. This is surprising because UID would predict that the young children in the corpora who are not fully developed speakers would have more noise in their distribution of information rates. This not only indicates a robustness in the unigram entropy curve across speakers, but also across ages and addressees. We also observed a robustness across corpora for the same languages, and a robustness across utterance lengths within the same corpus's entropy curve. This shows that the concept of an "entropy curve" for a specific language is well-founded when considering speech data.  

We found a distinct three-step distribution for English and Spanish CHILDES corpora, with a slight dip in the penultimate position of each sentence. The Mandarin corpus entropy curve, by comparison, has a noticeably lower positional entropy values in utterance-final positions than in utterance-penultimate positions. We attribute the penultimate dip in the English and Spanish entropy curves to the fact that most of the utterances in the CHILDES English and Spanish corpora we examined had a determiner such as "the" or "a" in the second-to-last position of utterances. The beginnings of utterances in the English and Spanish CHILDES data were usually pronouns or grammatical subjects, while the final words were grammatical objects and had a great deal of variation in the exact word that appeared in the utterance-final position. 

These entropy curves are not what would be expected from UID. The robustness of the three-step distribution for English and its replication in Spanish do not resemble the affine function we would expect from UID. The Mandarin entropy curve, which does not at all resemble either the English/Spanish distribution or the predictions of UID, suggests that the entropy curve can vary from language to language. UID predicts that each language should have a similar distribution, which should approximate an affine function.

We found that all of the English corpora followed the same three-step entropy curve shape, regardless of whether the corpus contained spoken or written data. The Spanish corpus we used also followed this same shape, which we expected. However, Mandarin did not have the same shape, indicating that the shape of the entropy curves may arise from cross-linguistic variation. Similar grammatical and other features between two languages, such as Spanish and English, may produce a similar entropy curve.

# Experiment 3

The UID hypothesis also applies to written communication: we expect people to communicate information at a uniform rate through writing as well. We use Wikipedia as a source for written data, which provides two advantages. One, the quantity of data in Wikipedia is very large for each language, at least several megabytes, and two, there are hundreds of languages with their own Wikipedia corpora. We can therefore run our entropy analysis on a much greater scale than using small spoken CHILDES corpora and directly compare the results of the entropy analysis on each language to one another. We expect to find linguistic features that determine the shape of a language's entropy curve. We treated sentences in the Wikipedia text corpora as equivalent to utterances in the Switchboard and CHILDES corpora. 

## Methods

Using Giuseppe Attardi's Wikiextractor tool \footnote{https://github.com/attardi/wikiextractor}, we extract text corpora from Wikipedia by downloading a stored collection of Wikipedia entries in each langauge and randomly selecting several thousand articles from each Wikipedia language. Each language corpus was cleaned and limited to sentences between six and $50$ words. Similar to our process for the Switchboard and CHILDES spoken corpora, we divided each corpus by sentence length, and then computed our positional entropy measure on each word position within each sentence length.

We computed two slope treatments of each curve. In the *absolute* treatment, with sentence length denoted as $k$, we computed the slope between positions $1$ and $2$, positions $2$ and $3$, positions $3$ and $k-2$, positions $k-2$ and $k-1$ and positions $k-1$ and $k$. For the short utterances appearing the CHILDES speech corpora, the slopes between the first and second slopes on either end of the distributions appeared to be more characteristic of the distribution as a whole, with a plateau in the middle of the entropy curve for each of the language corpora we examined in CHILDES.

However, sentences of length greater than ten in the Wikipedia corpora were significantly more common than in the CHILDES. Therefore we also computed relative slope treatments. In the *relative 5* treatment, we computed the slopes between every $20\%$ of the relative word positions in each sentence length, e.g. the slope between $0\%$ and $20\%$ of sentences of length $10$ is the slope between the position entropies at the sentence-initial word position and the position representing the third word. When computing the word position index to make those cuts, if the slope was not a whole number, then the closest whole number position was used instead for slope calculation. Each comparative slope within each treatment was averaged together between different sentence lengths, for example all of the $0\%$ to $20\%$ slopes over all sentence lengths were averaged together. 

In both the absolute and relative $5$ treatments, each language is embedded in $5$-dimensional space. This permitted us to use cosine similarity for direct comparison between the languages we pulled from Wikipedia. To determine which phonological, morphological and syntactic features affected the embedding of a language in the Wikipedia dataset, we use the linguistic features in World Atlas of Language Structures @dryer2013. We checked the effects of individual features on the embeddings of languages in the different treatments. We computed pairwise cosine similarity between each pair of language vectors within each treatment. For a subset of eight WALS features, we used a generalized linear model to see whether the cosine similarity between languages mattered in predicting if the languages shared the same value for a WALS feature. 

## Results and Analysis

For eight features and $45$ languages we pulled from Wikipedia, we found that the cosine similarity between WALS features. The table below shows the results for the generalized linear model we computed using WALS features and the absolute treatment. 

```{r tables, results="asis", tab.env = "table"}
glm_model <- read_csv("glm.csv") %>%
    select(-X1)

tab1 <- xtable::xtable(glm_model)
print(tab1, type="latex", comment = F, table.placement = "tb",
      include.rownames = F)
```

We also used the linear model approach to evaluate the effects of cosine distance in the relative $5$ treatment on the values of the same WALS features and found similar outcomes. 

From these results, we argue that cosine distance played some role in the feature determination. The top four features in determining the WALS feature value all characterize word order, which indicates that for this subset of features and languages that the embeddings in the slope space are related to the WALS features. We argue therefore that typological features play a role in determining the positional entropy values for a language. This indicates that the entropy curves are in some way structured by the syntactic, morphological and phonological features of a language. 

# Discussion 
In this paper, we have extended and applied a model from @yu2016 and derived from @genzel2002 for distinguishing entropy at each word position within sentences. We have argued that the outcomes of this model are derived from the distribution of information transmission in human communication. Language is not a uniform noisy channel of communication, but each language individually represents a different noisy channel of communication characterized in part by typological features. 

Our work complements the approach of studies such as @aylett2004, where languages are characterized by a single number, representing the rate of semantic information transfer per syllable. This body of work attests to languages possessing different rates of information transfer based on typological features as well. Within the information transfer rate literature, the rate is an overall feature of language that does not vary corpus to corpus. Similarly, in our work we have obtained a characteristic entropy distribution for each language. Both information rate and entropy distribution are characterized by typological features. This indicates that the structure and rate of how people communicate information varies cross-linguistically based on typological features. 

We initially wanted to use several hundred language corpora pulled from Wikipedia and all $144$ linguistic features from WALS. We considered an computing unsupervized clusterings of the $5$-dimensional vectors of our Wikipedia entropy curves, and comparing those clusterings to clusterings of the WALS features directly using a cluster similarity measure such as the Rand index @rand1971. However, the problem then arises of which combination of WALS features and how many features to include in the clustering analysis, with $144!$ different combinations. Two additional problems presented themselves: one, deciding how many clusters to use in an unsupervised clustering analysis is an unsolved problem in machine learning; and two, not all of the languages on Wikipedia have values inputted for their WALS features. As a follow-up, we are considering missing-data imputation on the WALS features in order to run the generalized linear model analysis on a large dataset of languages.  

On a more mechanical level, we expect that our entropy model will have implications for how long people take to read individual words at different positions of a sentence. Eye-tracking research has indicated the effects of surprisal on fixation duration during eye-tracking studies [@demberg2008; @boston2008; @smith2013]; higher surprisal of a word is on average predictive of higher fixation duration. The *wrap-up effect* states that people process sentence-final words more slowly on average then sentence-medial or sentence-initial words when reading, due to integrating information to form a final understanding of the sentence's meaning [@stowe2018; @kuperman2010]. The wrap-up effect is drawn from evidence in languages with a large final increase in their entropy curve from our study, so we hypothesize that the supposed wrap-up effect derives from the same source sentence-final increase in entropy curves observed in English and other Germanic languages. 

# Acknowledgements

[Not here yet.]

# References

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
