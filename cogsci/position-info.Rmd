---
title: "Information Distribution Depends on Language-Specific Features"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Josef Klafka \and Daniel Yurovsky \\
         \texttt{\{jklafka, yurovsky\}@uchicago.edu} \\
        Department of Psychology \\ University of Chicago}

abstract: >
    Although languages vary widely in their structure, all are vehicles for the transmission of information. Consequently, aspects of speakers' word choice can be understood through the models of optimal communication. One prediction of these models is that speakers should keep the density of information constant over the duration of an utterance [@levy2007]. However, different languages have different coding models that constrain the space of a speaker's choices (e.g. canonical word order). We build on a method developed by @yu2016 to analyze the word-level entropy curves of natural language productions across a diverse set of languages and in diverse written and spoken contexts. We show languages impose characteristic constraints on speaker's choices that predict deviations from Uniform Information Density, and that cross-linguistic variability in these deviations is predictable in part from syntactic properties of those languages. 
    
keywords: >
    Uniform information density; language structure; corpus analysis
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/', echo=F, warning=F, cache=T, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(tidyverse)
library(xtable)
library(tidytext)
library(tidyboot)
library(entropy)
library(here)
library(feather)
library(magrittr)
library(childesr)
library(ggthemes)
library(gridExtra)
library(directlabels)

theme_set(theme_classic(base_size = 10))
```

Over 7,000 languages are spoken around the modern world [@simons2018]. These language vary along many dimensions, but all share a core goal: communicating information. If speakers and writers of these languages act near-optimally to achieve their communicative goals, regularities of use across these diverse languages can be explained by a rational theory of communication [@anderson1991]. 
Information theory, a mathematical framework developed by @shannon1948 to describe the transmission and decoding of signals, has been a unifying language for the recent development of such theories in human and machine language processing [@jelinek1976;@levy2007]. 

These theories model the process of communication as transmission of information over a noisy channel. The producer begins with an intended meaning, packages this meaning into language, and then sends the encoded meaning to their intended receiver over a communicative channel. The receiver must then decode the producer's intended meaning from the signal they receive on their end of the channel. The problem is that the channel is noisy, and sometimes the signal can get corrupted (e.g. the producer can misspeak, or the receiver can mishear). In order to maximize the probability that the correct meaning is transmitted, these theories predict that producers should choose linguistic messages that keep the rate of information constant across words. The intuition is that if the receiver misperceives a word, and that word contains most of the information in the sentence, then the communication will have failed. Because producers cannot predict which word a speaker will mishear, their best strategy is spread the information evenly across all of the words in a sentence, i.e. maintain *Uniform Information Density* [@genzel2002; @levy2007].

<!-- **Evidence for UID here. Maybe see if you can present this work at a slightly higher level** -->
The evidence in favor of Uniform Information Density largely been situation-specific and English-language driven, while the hypothesis itself has been applied broadly over the past decade. The original evidence in favor of Uniform Information Density in @levy2007 finds that the insertion of complementizers (e.g. "that") in relative clauses in English corresponds to where neighboring words have high information content. Similarly, @frank2008 argues that contractions in English such as "you're" do not occur when neighboring words are highly informative. Applications of Uniform Information Density include determining whether linguistic alignment takes place [@jaeger2013], Zipfian word length distributions [@piantadosi2011], communication efficiency [@mahowald2013], dialogue and turn-taking [@xu2018] and the significance of ambiguity in language [@piantadosi2012], among other research. 

<!-- **Evidence against UID here. Again, can you present this at a higher level?** -->
However, other recent work has contradicted the Uniform Information Density hypothesis. Similar to the original @levy2007, @zhan2018 focuses on information distribution at particular points in sentences. @zhan2018 finds that more information-rich classifiers in Mandarin Chinese are produced when production of the neighboring noun is difficult, not when the information content is high. @jain2018 examine word order across spoken sentences in Hindi, a freer word order language than English, and find that information density has no significant effect on determining a Hindi's speaker's word order. 

Recently, @yu2016 developed a more direct test of the Uniform Information Density hypothesis, applying the logic used by @genzel2002 to look at the distribution of information *within* individual sentences. Because people process language incrementally--using the previous words in a sentence to predict the words that will come next--the amount of information that a word contains when seen in isolation should increase over the course of a sentence [@ferrer-i-cancho2013]. Analyzing a large corpus of written English, they find a different pattern: Entropy increases over the first few words of an utterance and then remains constant until the final word where it again jumps up (see top of Figure \ref{fig:read_and_plot_exp1}). @yu2016 conclude that the Uniform Information Density hypothesis must not hold for medial words in a sentence.

We extend and generalize @yu2016 in three ways: We confirm that this same pattern is found in spoken English--in written language, and in conversational speech between adults and between parents and their children. Thus, this entropy curve is a robust feature of English productions. We then examine entropy curves cross-linguistically, and show that characteristic curves vary across the world's languages. Finally, we show that this variation is predictable in part from the structure of individual languages (i.e. word order). Taken together, our results suggest a refinement of the Uniform Information Density hypothesis: speakers may structure their utterances to optimize information density, but they must do so under the predictable constraints of their language.

## Calculating entropy curves

In all of our studies, we used an adaptation of the by-word entropy method developed by @yu2016.
Given a text or speech corpus divided into individual sentences, we partition the corpus by sentence length in number of words. For each word position $X$ of sentences of length $k$, we define $w$ as a unique word occurring in position $X$. We further define $p(w)$ as the number of times word $w$ occurs in position $X$, divided by the number of total words that occur in position $X$ i.e. the number of sentences of length $k$. This creates a probability distribution over the words occurring in position $X$, and computing the @shannon1948 entropy $H(X)$ of this probability distribution gives the positional entropy of position $X$ in sentences of length $k$.

$$H(X) = \sum\limits_w p(w)\log\big(p(w)\big)$$

With this measure, we compute the unigram entropy at each word position of sentences of each length within the corpus. The result of this method can be plotted for each length as an *entropy curve*, which can be visually compared across sentence lengths to observe the how the unigram entropy changes across absolute positions in each of the sentences. @genzel2002 similarly examine a unigram entropy measure on sentences, and found that entropy at the sentence level increases linearly with sentence index within a corpus. Uniform Information Density applies this uniformity of entropy rate in sentences to all levels of speech, and so our method obtained from @yu2016, which examines text at the word level, should find a monotonically increasing affine function at the word level. 

The entropy curves capture individual variation across positions in utterances of the same length. This allows us to directly observe and judge the amount of variation in words that appear in an individual position of a sentence. We can directly compare any two positions within utterances to determine the amount of uncertainty, and therefore information, on average contained by words within that position of utterances. This method is thus identical in logic to @genzel2002, but within sentences instead of across sentences. 

# Study 1

We began by replicating Yu et al.'s [-@yu2016] analysis, computing entropy curves on the British National Corpus--a collection of predominantly written English [@clear1993]. We then applied the same method to the Switchboard corpus--a collection of spoken language [@godfrey1992]. This allows us to ask whether the characteristic function identified by @yu2016 is a general feature of English, or instead a function of written language.

# Data and Analysis

The British National Corpus consists of predominantly (90%) written language documents collected in the early 1980s and 1990s across a variety of genres (scientific articles, newspapers, fiction, etc). It also contains a small collection of corpora from spoken language. All together, it contains $\sim$ 100 million tokens. The Switchboard corpus is a collection of $\sim$ 2,400 telephone conversations between unacquainted adults prompted with subjects of conversation. Switchboard is the corpus used in Levy and Jaeger's [-@levy2007] original demonstration of the Uniform Information Density Hypothesis.

For each corpus, we computed entropy curves using the method described above for all sentences from length 4 to 30. 

```{r entropy_func}
unigram_entropy <- function(sen_length, tokens) {
  tokens %>%
    filter(length == sen_length) %>%
    group_by(word_order, word) %>%
    summarise(n = n()) %>%
    tidyboot(summary_function = function(x) x %>% 
               summarise(entropy = entropy(n, unit = "log2")),
             statistics_functions = function(x) x %>%
             summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
    mutate(length = sen_length)
}
```

```{r bnc_and_switchboard, eval = F}

sb_tokens <- read_feather(here("../switchboard/switchboard.feather")) %>%
  mutate(length = str_count(value, pattern = " +") + 1) %>% 
  mutate(utterance_id = 1:n()) %>%
  unnest_tokens(word, value, token = stringr::str_split, pattern = " +") %>%
  group_by(utterance_id) %>%
  mutate(word_order = 1:n()) 

bnc10_tokens <- read_feather(here("../bnc/bnc10.feather")) %>%
  mutate(word = as.numeric(as.factor(word)))
bnc20_tokens <- read_feather(here("../bnc/bnc20.feather")) %>%
  mutate(word = as.numeric(as.factor(word)))
bnc30_tokens <- read_feather(here("../bnc/bnc30.feather")) %>%
  mutate(word = as.numeric(as.factor(word)))

switchboard_entropies <- map_df(2:10, ~unigram_entropy(.x, sb_tokens)) 

bnc10_entropies <- unigram_entropy(10, bnc10_tokens)
bnc20_entropies <- unigram_entropy(20, bnc20_tokens)
bnc30_entropies <- unigram_entropy(30, bnc30_tokens)
bnc_entropies <- bind_rows(bnc10_entropies, bnc20_entropies, bnc30_entropies)

write_csv(bnc_entropies, here("data/bnc_entropies.csv"))
write_csv(switchboard_entropies, here("data/switchboard_entropies.csv"))
```

# Results

The entropy curves computed in both the British National Corpus and Switchboard were remarkably consistent both across corpora and across the range of sentence lengths we analyzed (Figure \ref{fig:read_and_plot_exp1}). Confirming Yu et al.'s [-@yu2016] findings, we find that
the positional entropy at the beginning of sentences is low, then rises and plateaus for sentence-medial word positions before dropping slightly in the second-to-last position and rising again.

```{r read_and_plot_exp1, fig.env = "figure*", fig.width=7, fig.height=4, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Representative Entropy Curves for the British National Corpus (top) and Switchboard (bottom). Points average entropies, error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}

bnc_entropies <- read_csv(here("data/bnc_entropies.csv"))

switchboard_entropies <- read_csv(here("data/switchboard_entropies.csv")) %>%
  filter(length %in% c(5, 7, 9))

bnc_plot <- ggplot(bnc_entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length, scales = "free_x") + 
  xlab("British National Corpus utterance position") +
  ylab("Positional entropy") +
  geom_pointrange(size = .25) +
  geom_line(size = .25) +
  theme_classic(base_size = 10)

sw_plot <- ggplot(switchboard_entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length, scales = "free_x") + 
  xlab("Switchboard utterance position") +
  ylab("Positional entropy") +
  geom_pointrange(size = .25) +
  geom_line(size = .25) +
  theme_classic(base_size = 10)

grid.arrange(bnc_plot, sw_plot, nrow = 2)
```

The shape of positional entropy curves we find in these two corpora is notable for two reasons: (1) it does not follow our predictions from Uniform Information Density, and (2) the distribution is robust across written and spoken Engilsh. This suggests that the entropy curve is characteristic of the English language. We next asked whether this shape is a feature even of children's speech, and also whether it varies cross-linguistically.

# Study 2

To understand how robust this entropy curve is, we turned to conversational speech from parent-child interactions. If we find the same shape even in children's productions, we have even stronger evidence that the 3-step curve is a charcateristic feature of English. We thus turned to the Child Language Data Exchange System (CHILDES), a collection of transcripts of parent-child interactions [@macwhinney2014].

## Data and Analysis

We analyzed three seperate corpora in CHILDES: The Providence Corpus [@demuth2006], The Shiro corpus [@shiro2000], and the Zhou Dinner Corpus [@li2015]. The Providence corpus consists of conversations between six 1--3-year-old American English-speaking children and their parents recorded in their homes. The Shiro Corpus consists of prompted Spanish-language narratives individually collected from over a hundred Venezualan schoolchildren, half from high SES backgrounds and half from low SES backgrounds. The Zhou Dinner Corpus contains dinner conversations between 5--6-year-old Mandarin-speaking children and their parents collected in Shanghai. Spanish is an Indo-European language like English, possessing similar grammar, word order and numerous cognate words. In contrast, Mandarin Chinese is typologically unrelated to English.

We accessed the transcripts from each corpus using \texttt{childesr}, an R-interface to a database formatted version of CHILDES [@sanchez2019]. We divided all utterances from each transcript into those produced by target child, and those produced by all other speakers. We then applied the same entropy curve method described above. For Mandarin, we used pinyin transliterations of the utterances in the corpus with demarcated word boundaries. The Chinese characters used for writing Mandarin do not normally demarcate word boundaries by spacing words apart, and for normal Chinese writing including spaces between word boundaries is unnatural and can have a negative effect on reading times [@bai2008]. 

## Results

```{r childes_entropy, eval = F}

extract_stem_tokens <- function(role, min_length, max_length, utterances) {
  
  if (role == "child") {
    lengths <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1)

} else {
    lengths <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1)
}
  
  tokens <- lengths %>% 
    filter(length >= min_length) %>%
    filter(length <= max_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stem, token = stringr::str_split, pattern = " +") %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    filter(word_order <= length) %>% 
    summarise(n = n()) 
}

extract_gloss_tokens <- function(role,  min_length, max_length,  utterances) {
  
  if (role == "child") {
    lengths <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1)

} else {
    lengths <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1)
}
  
  tokens <- lengths %>% 
    filter(length >= min_length) %>%
    filter(length <= max_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, gloss, token = stringr::str_split, pattern = " +") %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    filter(word_order <= length) %>% 
    summarise(n = n()) 
}
 

prov_utterances <- get_utterances(corpus = "Providence") #stem or gloss
shiro_utterances <- get_utterances(corpus = "Shiro") #use gloss
zhou_utterances <- get_utterances(corpus = "ZhouDinner") #use stem, NOT gloss

prov_child_tokens <- extract_stem_tokens("child", 5, 9, prov_utterances) 
prov_adult_tokens <- extract_stem_tokens("not child",5, 9, prov_utterances) 

prov_child_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, prov_child_tokens)) %>%
  mutate(language = "English", person = "Child")
prov_adult_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, prov_adult_tokens)) %>%
  mutate(language = "English", person = "Adult")

shiro_child_tokens <- extract_gloss_tokens("child", 5, 9, shiro_utterances) 
shiro_adult_tokens <- extract_gloss_tokens("not child",5, 9, shiro_utterances) 

shiro_child_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, shiro_child_tokens)) %>%
  mutate(language = "Spanish", person = "Child")
shiro_adult_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, shiro_adult_tokens)) %>%
  mutate(language = "Spanish", person = "Adult")

zhou_child_tokens <- extract_stem_tokens("child", 5, 9, zhou_utterances) 
zhou_adult_tokens <- extract_stem_tokens("not child",5, 9, zhou_utterances) 

zhou_child_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, zhou_child_tokens)) %>%
  mutate(language = "Mandarin", person = "Child")
zhou_adult_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, zhou_adult_tokens)) %>%
  mutate(language = "Mandarin", person = "Adult")

childes_entropies <- bind_rows(prov_child_entropies, prov_adult_entropies, 
                               shiro_child_entropies, shiro_adult_entropies,
                               zhou_child_entropies, zhou_adult_entropies)

write_csv(childes_entropies, here("data/childes_entropies.csv"))
```

```{r plot_childes, fig.env = "figure*", fig.width=6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Representative Entropy Curves for Three Childes Corpora in English, Mandarin, and Spanish. Points average entropies, error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}
childes_entropies <- read_csv(here("data/childes_entropies.csv"))

label_data <- data_frame(length = 9, language = "Spanish", word_order = 6,
                         person = c("Adult", "Child"), empirical_entropy = c(7,9.5))


ggplot(childes_entropies, aes(x = word_order, y = empirical_entropy,
                      color = person,
                      label = person)) +
  facet_grid(language ~ length, scales = "free") + 
  xlab("Utterance position") +
  ylab("Positional entropy") +
  geom_pointrange(size = .25, aes(ymin = ci_lower, ymax = ci_upper)) +
  geom_line(size = .25) +
  theme_classic(base_size = 10) + 
  scale_color_ptol() +
  theme(legend.position = "nones") + 
  geom_text(data = label_data)
```

Across corpora, we found similar entropy curve shapes for adults and children, but distinct shapes for each language. Figure \ref{fig:plot_childes} shows representative curves across corpora, but shapes were robust across the full range utterances we analyzed. We found a distinct three-step distribution for English and Spanish CHILDES corpora, with a slight dip in the penultimate position of each sentence. The Mandarin corpus entropy curve, by comparison, has a noticeably lower positional entropy values in utterance-final positions than in utterance-penultimate positions. 

These results present two important pieces of information. First, our analysis of the Providence corpus broadly replicates the shape we found in both the British National Corpus and in Switchboard. Thus, the entropy curve of English appears not just in adult-adult conversation, but even in speech produced by parents to their children, and speech produced by very young children to their parents. This suggests that it is a highly robust feature of English langauge, as it structures the productions of even pre-schooled aged children. 

Second, the entropy curves of English, Spanish, and Mandarin were not identical. None of these shapes resembled the affine function predicted naively from Uniform Information Density, but also Mandarin was quite different from both English and Spanish. This suggests that the entropy curve can vary from language to language. One possibility is that this variation arises from typological features of languages, such as characteristic word order. We noticed, for instance, a relatively high density of determiners in the penultimate position of English and Spanish utterances, which could account for the penultimate dip in the entropy curves of those two languages. In our final analysis, we explored this possibility directly, analyzing a large set of Wikipedia corpora from diverse languages, and asking whether variability in their entropy curves was related to variability in their syntactic structure.

# Experiment 3

The UID hypothesis also applies to written communication: we expect people to communicate information at a uniform rate through writing as well. We use Wikipedia as a source for written data, which provides two advantages. One, the quantity of data in Wikipedia is very large for each language, at least several megabytes, and two, there are hundreds of languages with their own Wikipedia corpora. We can therefore run our entropy analysis on a much greater scale than using small spoken CHILDES corpora and directly compare the results of the entropy analysis on each language to one another. We expect to find linguistic features that determine the shape of a language's entropy curve. We treated sentences in the Wikipedia text corpora as equivalent to utterances in the Switchboard and CHILDES corpora. 

## Methods

Using Giuseppe Attardi's Wikiextractor tool \footnote{https://github.com/attardi/wikiextractor}, we extract text corpora from Wikipedia by downloading a stored collection of Wikipedia entries in each langauge and randomly selecting several thousand articles from each Wikipedia language. Each language corpus was cleaned and limited to sentences between six and $50$ words. Similar to our process for the Switchboard and CHILDES spoken corpora, we divided each corpus by sentence length, and then computed our positional entropy measure on each word position within each sentence length.

We computed two slope treatments of each curve. In the *absolute* treatment, with sentence length denoted as $k$, we computed the slope between positions $1$ and $2$, positions $2$ and $3$, positions $3$ and $k-2$, positions $k-2$ and $k-1$ and positions $k-1$ and $k$. For the short utterances appearing the CHILDES speech corpora, the slopes between the first and second slopes on either end of the distributions appeared to be more characteristic of the distribution as a whole, with a plateau in the middle of the entropy curve for each of the language corpora we examined in CHILDES.

However, sentences of length greater than ten in the Wikipedia corpora were significantly more common than in the CHILDES. Therefore we also computed relative slope treatments. In the *relative 5* treatment, we computed the slopes between every $20\%$ of the relative word positions in each sentence length, e.g. the slope between $0\%$ and $20\%$ of sentences of length $10$ is the slope between the position entropies at the sentence-initial word position and the position representing the third word. When computing the word position index to make those cuts, if the slope was not a whole number, then the closest whole number position was used instead for slope calculation. Each comparative slope within each treatment was averaged together between different sentence lengths, for example all of the $0\%$ to $20\%$ slopes over all sentence lengths were averaged together. 

In both the absolute and relative $5$ treatments, each language is embedded in $5$-dimensional space. This permitted us to use cosine similarity for direct comparison between the languages we pulled from Wikipedia. To determine which phonological, morphological and syntactic features affected the embedding of a language in the Wikipedia dataset, we use the linguistic features in World Atlas of Language Structures @dryer2013. We checked the effects of individual features on the embeddings of languages in the different treatments. We computed pairwise cosine similarity between each pair of language vectors within each treatment. For a subset of eight WALS features, we used a generalized linear model to see whether the cosine similarity between languages mattered in predicting if the languages shared the same value for a WALS feature. 

## Results and Analysis

For eight features and $45$ languages we pulled from Wikipedia, we found that the cosine similarity between WALS features. The table below shows the results for the generalized linear model we computed using WALS features and the absolute treatment. 

```{r }
library(tidyverse)
library(googlesheets)
library(lingtypology)
library(knitr)
library(missMDA)
library(cluster)
library(widyr)
library(lsa)
library(magrittr)
library(broom)

wiki_gs <- gs_title("Wikipedia_relative5")
wkpd <- gs_read(wiki_gs) 
LANGUAGES <- wkpd$Language

FEATURES <- c(1:144)
for (i in c(1:144)) {
  FEATURES[i] <- paste(FEATURES[i], "A", sep = "")
} #get all WALS features in correct syntax

wf <- wals.feature(FEATURES)

wf_df <- wf %>%
  as_data_frame() %>%
  filter(language %in% LANGUAGES) %>%
  group_by(language) %>%
  slice(1) %>%
  gather(feature, value, `1A`:`144A`) %>%
  group_by(feature) %>%
  mutate(value = factor(value))

cosines <- wkpd %>%
  column_to_rownames("Language") %>%
  t() %>%
  cosine() %>%
  as_data_frame(rownames = "language1") %>%
  gather(language2, cosine, -language1)

wf_many_langs <- wf_df %>%
  group_by(feature) %>%
  summarise(n = sum(!is.na(value))) %>%
  filter(n > 60)

wf_sub_langs <- wf_df %>%
  filter(feature %in% wf_many_langs$feature) %>%
  group_by(language) %>%
  summarise(keep = sum(is.na(value)) == 0) %>%
  filter(keep)

wf_sub <- wf_df %>%
  filter(feature %in% wf_many_langs$feature,
         language %in% wf_sub_langs$language) %>%
  select(language, feature, value) %>%
  mutate(value = as.numeric(as.factor(value)))


wf_pairwise <- expand.grid(language1 = wf_sub$language,
                           language2 = wf_sub$language) %>%
  left_join(wf_sub, by = c("language1" = "language")) %>%
  rename(value1 = value) %>%
  left_join(wf_sub, by = c("language2" = "language", "feature")) %>%
  rename(value2 = value) %>%
  mutate(same = value1 == value2) %>%
  select(-value1, -value2) %>%
  left_join(cosines, by = c("language1", "language2"))


models <- wf_pairwise %>%
  group_by(feature) %>%
  nest() %>%
  mutate(model = map(data, ~glm(same ~ cosine, 
                                family = "binomial", data = .)))

models %>%
  mutate(coeffs = map(model, tidy)) %>%
  select(-data, -model) %>%
  unnest() %>%
  filter(term == "cosine") %>%
  arrange(desc(statistic))

### generate dendrogram plot
dendro_langs <- wf_sub_langs$language

wk_clust_tree <- wkpd %>% 
              filter(Language %in% dendro_langs) %>%
              column_to_rownames(var = "Language") %>%
              dist() %>%
              hclust("ward.D2") 

wk_clust_tree %>% plot(hang = -1)
```

```{r tables, results="asis", tab.env = "table"}
glm_model <- read_csv("glm.csv") %>%
    select(-X1)

tab1 <- xtable::xtable(glm_model)
print(tab1, type="latex", comment = F, table.placement = "tb",
      include.rownames = F)
```

We also used the linear model approach to evaluate the effects of cosine distance in the relative $5$ treatment on the values of the same WALS features and found similar outcomes. 

From these results, we argue that cosine distance played some role in the feature determination. The top four features in determining the WALS feature value all characterize word order, which indicates that for this subset of features and languages that the embeddings in the slope space are related to the WALS features. We argue therefore that typological features play a role in determining the positional entropy values for a language. This indicates that the entropy curves are in some way structured by the syntactic, morphological and phonological features of a language. 

# Discussion 
In this paper, we have extended and applied a model from @yu2016 and derived from @genzel2002 for distinguishing entropy at each word position within sentences. We have argued that the outcomes of this model are derived from the distribution of information transmission in human communication. Language is not a uniform noisy channel of communication, but each language individually represents a different noisy channel of communication characterized in part by typological features. 

Our work complements the approach of studies such as @aylett2004, where languages are characterized by a single number, representing the rate of semantic information transfer per syllable. This body of work attests to languages possessing different rates of information transfer based on typological features as well. Within the information transfer rate literature, the rate is an overall feature of language that does not vary corpus to corpus. Similarly, in our work we have obtained a characteristic entropy distribution for each language. Both information rate and entropy distribution are characterized by typological features. This indicates that the structure and rate of how people communicate information varies cross-linguistically based on typological features. 

We initially wanted to use several hundred language corpora pulled from Wikipedia and all $144$ linguistic features from WALS. We considered an computing unsupervized clusterings of the $5$-dimensional vectors of our Wikipedia entropy curves, and comparing those clusterings to clusterings of the WALS features directly using a cluster similarity measure such as the Rand index @rand1971. However, the problem then arises of which combination of WALS features and how many features to include in the clustering analysis, with $144!$ different combinations. Two additional problems presented themselves: one, deciding how many clusters to use in an unsupervised clustering analysis is an unsolved problem in machine learning; and two, not all of the languages on Wikipedia have values inputted for their WALS features. As a follow-up, we are considering missing-data imputation on the WALS features in order to run the generalized linear model analysis on a large dataset of languages.  

On a more mechanical level, we expect that our entropy model will have implications for how long people take to read individual words at different positions of a sentence. Eye-tracking research has indicated the effects of surprisal on fixation duration during eye-tracking studies [@demberg2008; @boston2008; @smith2013]; higher surprisal of a word is on average predictive of higher fixation duration. The *wrap-up effect* states that people process sentence-final words more slowly on average then sentence-medial or sentence-initial words when reading, due to integrating information to form a final understanding of the sentence's meaning [@stowe2018; @kuperman2010]. The wrap-up effect is drawn from evidence in languages with a large final increase in their entropy curve from our study, so we hypothesize that the supposed wrap-up effect derives from the same source sentence-final increase in entropy curves observed in English and other Germanic languages. 

# Acknowledgements

[Not here yet.]

# References

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
