---
title: "Information Distribution Depends on Language-Specific Features"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
 \author{Josef Klafka \and Daniel Yurovsky \\
         \texttt{\{jklafka, yurovsky\}@uchicago.edu} \\
        Department of Psychology \\ University of Chicago}

abstract: >
    Although languages vary widely in their structure, all are vehicles for the transmission of information. Consequently, aspects of speakers' word choice can be understood through the models of optimal communication. One prediction of these models is that speakers should keep the density of information constant over the duration of an utterance [@levy2007]. However, different languages have different coding models that constrain the space of a speaker's choices (e.g. canonical word order). We build on a method developed by @yu2016 to analyze the word-level entropy curves of natural language productions across a diverse set of languages and in diverse written and spoken contexts. We show languages impose characteristic constraints on speaker's choices that predict deviations from Uniform Information Density, and that cross-linguistic variability in these deviations is predictable in part from syntactic properties of those languages. 
    
keywords: >
    Uniform information density; language structure; corpus analysis
    
output: cogsci2016::cogsci_paper
#final-submission: \cogscifinalcopy
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/', echo=F, warning=F, cache=T, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(tidyverse)
library(xtable)
library(tidytext)
library(tidyboot)
library(entropy)
library(here)
library(feather)
library(magrittr)
library(childesr)
library(ggthemes)
library(gridExtra)
library(directlabels)
library(ggdendro)

theme_set(theme_classic(base_size = 10))
```

Over 7,000 languages are spoken around the modern world [@simons2018]. These language vary along many dimensions, but all share a core goal: communicating information. If speakers and writers of these languages act near-optimally to achieve their communicative goals, regularities of use across these diverse languages can be explained by a rational theory of communication [@anderson1991]. 
Information theory, a mathematical framework developed by @shannon1948 to describe the transmission and decoding of signals, has been a unifying language for the recent development of such theories in human and machine language processing [@jelinek1976;@levy2007]. 

These theories model the process of communication as transmission of information over a noisy channel. The producer begins with an intended meaning, packages this meaning into language, and then sends the encoded meaning to their intended receiver over a communicative channel. The receiver must then decode the producer's intended meaning from the signal they receive on their end of the channel. The problem is that the channel is noisy, and sometimes the signal can get corrupted (e.g. the producer can misspeak, or the receiver can mishear). In order to maximize the probability that the correct meaning is transmitted, these theories predict that producers should choose linguistic messages that keep the rate of information constant across words. The intuition is that if the receiver misperceives a word, and that word contains most of the information in the sentence, then the communication will have failed. Because producers cannot predict which word a speaker will mishear, their best strategy is spread the information evenly across all of the words in a sentence, i.e. maintain *Uniform Information Density* [@genzel2002; @levy2007].

<!-- **Evidence for UID here. Maybe see if you can present this work at a slightly higher level** -->
The evidence in favor of Uniform Information Density largely been situation-specific and English-language driven, while the hypothesis itself has been applied broadly over the past decade. The original evidence in favor of Uniform Information Density in @levy2007 finds that the insertion of complementizers (e.g. "that") in relative clauses in English corresponds to where neighboring words have high information content. Similarly, @frank2008 argues that contractions in English such as "you're" do not occur when neighboring words are highly informative. Applications of Uniform Information Density include determining whether linguistic alignment takes place [@jaeger2013], Zipfian word length distributions [@piantadosi2011], communication efficiency [@mahowald2013], dialogue and turn-taking [@xu2018] and the significance of ambiguity in language [@piantadosi2012], among other research. 

<!-- **Evidence against UID here. Again, can you present this at a higher level?** -->
However, other recent work has contradicted the Uniform Information Density hypothesis. Similar to the original @levy2007, @zhan2018 focuses on information distribution at particular points in sentences. @zhan2018 finds that more information-rich classifiers in Mandarin Chinese are produced when production of the neighboring noun is difficult, not when the information content is high. @jain2018 examine word order across spoken sentences in Hindi, a freer word order language than English, and find that information density has no significant effect on determining a Hindi's speaker's word order. 

Recently, @yu2016 developed a more direct test of the Uniform Information Density hypothesis, applying the logic used by @genzel2002 to look at the distribution of information *within* individual sentences. Because people process language incrementally--using the previous words in a sentence to predict the words that will come next--the amount of information that a word contains when seen in isolation should increase over the course of a sentence [@ferrer-i-cancho2013]. Analyzing a large corpus of written English, they find a different pattern: Entropy increases over the first few words of an utterance and then remains constant until the final word where it again jumps up (see top of Figure \ref{fig:read_and_plot_exp1}). @yu2016 conclude that the Uniform Information Density hypothesis must not hold for medial words in a sentence.

We extend and generalize @yu2016 in three ways: We confirm that this same pattern is found in spoken English--in written language, and in conversational speech between adults and between parents and their children. Thus, this entropy curve is a robust feature of English productions. We then examine entropy curves cross-linguistically, and show that characteristic curves vary across the world's languages. Finally, we show that this variation is predictable in part from the structure of individual languages (i.e. word order). Taken together, our results suggest a refinement of the Uniform Information Density hypothesis: speakers may structure their utterances to optimize information density, but they must do so under the predictable constraints of their language.

## Calculating entropy curves

In all of our studies, we used an adaptation of the by-word entropy method developed by @yu2016.
Given a text or speech corpus divided into individual sentences, we partition the corpus by sentence length in number of words. For each word position $X$ of sentences of length $k$, we define $w$ as a unique word occurring in position $X$. We further define $p(w)$ as the number of times word $w$ occurs in position $X$, divided by the number of total words that occur in position $X$ i.e. the number of sentences of length $k$. This creates a probability distribution over the words occurring in position $X$, and computing the @shannon1948 entropy $H(X)$ of this probability distribution gives the positional entropy of position $X$ in sentences of length $k$.

$$H(X) = \sum\limits_w p(w)\log\big(p(w)\big)$$

With this measure, we compute the unigram entropy at each word position of sentences of each length within the corpus. The result of this method can be plotted for each length as an *entropy curve*, which can be visually compared across sentence lengths to observe the how the unigram entropy changes across absolute positions in each of the sentences. @genzel2002 similarly examine a unigram entropy measure on sentences, and found that entropy at the sentence level increases linearly with sentence index within a corpus. Uniform Information Density applies this uniformity of entropy rate in sentences to all levels of speech, and so our method obtained from @yu2016, which examines text at the word level, should find a monotonically increasing affine function at the word level. 

The entropy curves capture individual variation across positions in utterances of the same length. This allows us to directly observe and judge the amount of variation in words that appear in an individual position of a sentence. We can directly compare any two positions within utterances to determine the amount of uncertainty, and therefore information, on average contained by words within that position of utterances. This method is thus identical in logic to @genzel2002, but within sentences instead of across sentences. 

# Study 1

We began by replicating Yu et al.'s [-@yu2016] analysis, computing entropy curves on the British National Corpus--a collection of predominantly written English [@clear1993]. We then applied the same method to the Switchboard corpus--a collection of spoken language [@godfrey1992]. This allows us to ask whether the characteristic function identified by @yu2016 is a general feature of English, or instead a function of written language.

# Data and Analysis

The British National Corpus consists of predominantly (90%) written language documents collected in the early 1980s and 1990s across a variety of genres (scientific articles, newspapers, fiction, etc). It also contains a small collection of corpora from spoken language. All together, it contains $\sim$ 100 million tokens. The Switchboard corpus is a collection of $\sim$ 2,400 telephone conversations between unacquainted adults prompted with subjects of conversation. Switchboard is the corpus used in Levy and Jaeger's [-@levy2007] original demonstration of the Uniform Information Density Hypothesis.

For each corpus, we computed entropy curves using the method described above for all sentences from length 4 to 30. 

```{r entropy_func}
unigram_entropy <- function(sen_length, tokens) {
  tokens %>%
    filter(length == sen_length) %>%
    group_by(word_order, word) %>%
    summarise(n = n()) %>%
    tidyboot(summary_function = function(x) x %>% 
               summarise(entropy = entropy(n, unit = "log2")),
             statistics_functions = function(x) x %>%
             summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
    mutate(length = sen_length)
}
```

```{r bnc_and_switchboard, eval = F}

sb_tokens <- read_feather(here("../switchboard/switchboard.feather")) %>%
  mutate(length = str_count(value, pattern = " +") + 1) %>% 
  mutate(utterance_id = 1:n()) %>%
  unnest_tokens(word, value, token = stringr::str_split, pattern = " +") %>%
  group_by(utterance_id) %>%
  mutate(word_order = 1:n()) 

bnc10_tokens <- read_feather(here("../bnc/bnc10.feather")) %>%
  mutate(word = as.numeric(as.factor(word)))
bnc20_tokens <- read_feather(here("../bnc/bnc20.feather")) %>%
  mutate(word = as.numeric(as.factor(word)))
bnc30_tokens <- read_feather(here("../bnc/bnc30.feather")) %>%
  mutate(word = as.numeric(as.factor(word)))

switchboard_entropies <- map_df(2:10, ~unigram_entropy(.x, sb_tokens)) 

bnc10_entropies <- unigram_entropy(10, bnc10_tokens)
bnc20_entropies <- unigram_entropy(20, bnc20_tokens)
bnc30_entropies <- unigram_entropy(30, bnc30_tokens)
bnc_entropies <- bind_rows(bnc10_entropies, bnc20_entropies, bnc30_entropies)

write_csv(bnc_entropies, here("data/bnc_entropies.csv"))
write_csv(switchboard_entropies, here("data/switchboard_entropies.csv"))
```

# Results

The entropy curves computed in both the British National Corpus and Switchboard were remarkably consistent both across corpora and across the range of sentence lengths we analyzed (Figure \ref{fig:read_and_plot_exp1}). Confirming Yu et al.'s [-@yu2016] findings, we find that
the positional entropy at the beginning of sentences is low, then rises and plateaus for sentence-medial word positions before dropping slightly in the second-to-last position and rising again.

```{r read_and_plot_exp1, fig.env = "figure*", fig.width=6, fig.height=4, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Representative Entropy Curves for the British National Corpus (top) and Switchboard (bottom). Points average entropies, error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}

bnc_entropies <- read_csv(here("data/bnc_entropies.csv"))

switchboard_entropies <- read_csv(here("data/switchboard_entropies.csv")) %>%
  filter(length %in% c(5, 7, 9))

bnc_plot <- ggplot(bnc_entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length, scales = "free_x") + 
  xlab("British National Corpus utterance position") +
  ylab("Positional entropy") +
  geom_pointrange(size = .25) +
  geom_line(size = .25) +
  theme_classic(base_size = 10)

sw_plot <- ggplot(switchboard_entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length, scales = "free_x") + 
  xlab("Switchboard utterance position") +
  ylab("Positional entropy") +
  geom_pointrange(size = .25) +
  geom_line(size = .25) +
  theme_classic(base_size = 10)

grid.arrange(bnc_plot, sw_plot, nrow = 2)
```

The shape of positional entropy curves we find in these two corpora is notable for two reasons: (1) it does not follow our predictions from Uniform Information Density, and (2) the distribution is robust across written and spoken Engilsh. This suggests that the entropy curve is characteristic of the English language. We next asked whether this shape is a feature even of children's speech, and also whether it varies cross-linguistically.

# Study 2

To understand how robust this entropy curve is, we turned to conversational speech from parent-child interactions. If we find the same shape even in children's productions, we have even stronger evidence that the 3-step curve is a charcateristic feature of English. We thus turned to the Child Language Data Exchange System (CHILDES), a collection of transcripts of parent-child interactions [@macwhinney2014].

## Data and Analysis

We analyzed three seperate corpora in CHILDES: The Providence Corpus [@demuth2006], The Shiro corpus [@shiro2000], and the Zhou Dinner Corpus [@li2015]. The Providence corpus consists of conversations between six 1--3-year-old American English-speaking children and their parents recorded in their homes. The Shiro Corpus consists of prompted Spanish-language narratives individually collected from over a hundred Venezualan schoolchildren, half from high SES backgrounds and half from low SES backgrounds. The Zhou Dinner Corpus contains dinner conversations between 5--6-year-old Mandarin-speaking children and their parents collected in Shanghai. Spanish is an Indo-European language like English, possessing similar grammar, word order and numerous cognate words. In contrast, Mandarin Chinese is typologically unrelated to English.

We accessed the transcripts from each corpus using \texttt{childesr}, an R-interface to a database formatted version of CHILDES [@sanchez2019]. We divided all utterances from each transcript into those produced by target child, and those produced by all other speakers. We then applied the same entropy curve method described above. For Mandarin, we used pinyin transliterations of the utterances in the corpus with demarcated word boundaries. The Chinese characters used for writing Mandarin do not normally demarcate word boundaries by spacing words apart, and for normal Chinese writing including spaces between word boundaries is unnatural and can have a negative effect on reading times [@bai2008]. 

## Results

```{r childes_entropy, eval = F}

extract_stem_tokens <- function(role, min_length, max_length, utterances) {
  
  if (role == "child") {
    lengths <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1)

} else {
    lengths <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(stem, " ") + 1)
}
  
  tokens <- lengths %>% 
    filter(length >= min_length) %>%
    filter(length <= max_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stem, token = stringr::str_split, pattern = " +") %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    filter(word_order <= length) %>% 
    summarise(n = n()) 
}

extract_gloss_tokens <- function(role,  min_length, max_length,  utterances) {
  
  if (role == "child") {
    lengths <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1)

} else {
    lengths <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1)
}
  
  tokens <- lengths %>% 
    filter(length >= min_length) %>%
    filter(length <= max_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, gloss, token = stringr::str_split, pattern = " +") %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    filter(word_order <= length) %>% 
    summarise(n = n()) 
}
 

prov_utterances <- get_utterances(corpus = "Providence") #stem or gloss
shiro_utterances <- get_utterances(corpus = "Shiro") #use gloss
zhou_utterances <- get_utterances(corpus = "ZhouDinner") #use stem, NOT gloss

prov_child_tokens <- extract_stem_tokens("child", 5, 9, prov_utterances) 
prov_adult_tokens <- extract_stem_tokens("not child",5, 9, prov_utterances) 

prov_child_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, prov_child_tokens)) %>%
  mutate(language = "English", person = "Child")
prov_adult_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, prov_adult_tokens)) %>%
  mutate(language = "English", person = "Adult")

shiro_child_tokens <- extract_gloss_tokens("child", 5, 9, shiro_utterances) 
shiro_adult_tokens <- extract_gloss_tokens("not child",5, 9, shiro_utterances) 

shiro_child_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, shiro_child_tokens)) %>%
  mutate(language = "Spanish", person = "Child")
shiro_adult_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, shiro_adult_tokens)) %>%
  mutate(language = "Spanish", person = "Adult")

zhou_child_tokens <- extract_stem_tokens("child", 5, 9, zhou_utterances) 
zhou_adult_tokens <- extract_stem_tokens("not child",5, 9, zhou_utterances) 

zhou_child_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, zhou_child_tokens)) %>%
  mutate(language = "Mandarin", person = "Child")
zhou_adult_entropies <-  map_df(c(5,7,9), ~unigram_entropy(.x, zhou_adult_tokens)) %>%
  mutate(language = "Mandarin", person = "Adult")

childes_entropies <- bind_rows(prov_child_entropies, prov_adult_entropies, 
                               shiro_child_entropies, shiro_adult_entropies,
                               zhou_child_entropies, zhou_adult_entropies)

write_csv(childes_entropies, here("data/childes_entropies.csv"))
```

```{r plot_childes, fig.env = "figure*", fig.width=6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Representative Entropy Curves for Three Childes Corpora in English, Mandarin, and Spanish. Points average entropies, error bars show 95\\% confidence intervals computed by non-parametric bootstrap."}
childes_entropies <- read_csv(here("data/childes_entropies.csv"))

label_data <- data_frame(length = 9, language = "Spanish", word_order = 6,
                         person = c("Adult", "Child"), empirical_entropy = c(7,9.5))


ggplot(childes_entropies, aes(x = word_order, y = empirical_entropy,
                      color = person,
                      label = person)) +
  facet_grid(language ~ length, scales = "free") + 
  xlab("Utterance position") +
  ylab("Positional entropy") +
  geom_pointrange(size = .25, aes(ymin = ci_lower, ymax = ci_upper)) +
  geom_line(size = .25) +
  theme_classic(base_size = 10) + 
  scale_color_ptol() +
  theme(legend.position = "nones") + 
  geom_text(data = label_data)
```

Across corpora, we found similar entropy curve shapes for adults and children, but distinct shapes for each language. Figure \ref{fig:plot_childes} shows representative curves across corpora, but shapes were robust across the full range utterances we analyzed. We found a distinct three-step distribution for English and Spanish CHILDES corpora, with a slight dip in the penultimate position of each sentence. The Mandarin corpus entropy curve, by comparison, has a noticeably lower positional entropy values in utterance-final positions than in utterance-penultimate positions. 

These results present two important pieces of information. First, the Providence corpus entropy curve broadly replicates the curves found in both the British National Corpus and in Switchboard. Thus, the entropy curve of English appears not just in adult-adult conversation, but even in speech produced by parents to their children, and speech produced by very young children to their parents. This suggests that it is a highly robust feature of English language, as it structures the productions of even pre-schooled aged children. 

Second, the entropy curves of English, Spanish, and Mandarin were not identical. No shape resembled the affine function predicted naively from Uniform Information Density, but also Mandarin was quite different from both English and Spanish. This suggests that the entropy curve can vary from language to language. One possibility is that this variation arises from typological features of languages, such as characteristic word order. We noticed, for instance, a relatively high density of determiners in the penultimate position of English and Spanish utterances, which could account their characteristic penultimate dip. In our final analysis, we explored this possibility directly, analyzing a large set of Wikipedia corpora from diverse languages, and asking whether variability in their entropy curves was related to variability in their syntactic structure.

# Study 3

In order to understand how entropy curves are related to linguistic structure, we turned to Wikipedia for a larger and more diverse of set of languages.

## Method 

### Data and Analysis

Wikipedia has two primary advantages: (1) Individual language corpora are generally large, and (2) hundreds of languages are represented. To understand how variability in entropy curves is related to the structure of language, we used features from the World Atlas of Language Structures [WALS, @dryer2013]. WALS is The World Atlas of Language Structures is a curated collection of typological features across the world's languages. However, as the collection includes contributions from many authors studying a heterogeneous set of topic and languages, most features are missing for most languages. For our sample, we sought to balance diversity of languages with coverage in WALS. We selected $45$ languages for which we $8$ WALS features were coded (Figure \ref{fig:clust_tree}). Unfortunately, these features all 
reflected the order of words in a sentence in each langauge (e.g. whether objects come before verbs, whether adjectives come before nouns). A consistent set of results across these features would be consistent with an effect of order on entropy curves, but unfortunately no other feature types are available for comparison.

We computed entropy curves separately for each language as before. In order to understand how these curves varied across languages, we developed a method for compressing the curves into a few representative values. We noticed that curves tended to vary in predictable utterance positions: The majority were flat in the middle of utterances, but had either rising or falling slopes at the beginnings and ends. For each language, for each utterance length $k$ we computed 5 slopes: The change between the 1st and 2nd position, the change between the 2nd and 3rd position, the slope between the 3rd position and the 3rd-to-last position ($k-1$), the slope between the 3rd-to-last and the 2nd-to-last position ($k-1$), and the slope between the 2nd-to-last and final positions. We then averaged these slopes across utterance lengths within a language to estimate a language-typical signature. We performed an additional analysis dividing each utterance into 6ths evenly and computing the slopes between these 6ths. Results from this analysis were qualitatively similar.

## Results and Analysis

We treated these 5 slopes as a vector, allowing us to compute the pairwise similarity between languages with cosine--a standard measure of vector similarity [@landauer1997]. Intuitively, vectors with with a smaller angle between them are more similar. Figure \ref{fig:clust_tree} shows a hierarchical clustering of these 5 dimensional vectors, demonstrating that entropy curve similarity appears to capture typological relatedly, e.g. Slavic languages are similar to each-other. 

```{r load_features}
features <- read_csv(here("data/wals_features.csv"))

glm_model <- read_csv(here("data/glm.csv")) %>%
  select(-X1) %>%
  left_join(features) %>%
  filter(meaning != "Tea") %>%
  select(-term, -std.error) %>%
  mutate(r2 = (statistic^2) / (statistic^2 + 164024))

max_r2 <- glm_model %>% 
  mutate(r2 = r2 * 100) %>% 
  pull(r2) %>% 
  max() %>% 
  round(2)
```

We then used asked whether these pairwise similarities are related to overlap in linguistic features. For each of the 8 features we measured, we fit a logistic regression predicting whether two languages would have the same value for it from their pairwise entropy curve similarity. Overlap on all 8 features was reliably predicted by entropy curve similarity (largest $p = .001$), with the amount of variance in feature overlap predicted varying across predictors \ref{tab:tab_features}. Order of object and verb ($83A$) was the strongest predictor, explaining `r max_r2`% of the variance in pairwise entropy curve similarity. 

```{r, eval = F}
library(tidyverse)
library(googlesheets)
library(lingtypology)
library(knitr)
library(missMDA)
library(cluster)
library(widyr)
library(lsa)
library(magrittr)
library(broom)

wiki_gs <- gs_title("Wikipedia_relative5")
wkpd <- gs_read(wiki_gs) 
LANGUAGES <- wkpd$Language

FEATURES <- c(1:144)
for (i in c(1:144)) {
  FEATURES[i] <- paste(FEATURES[i], "A", sep = "")
} #get all WALS features in correct syntax

wf <- wals.feature(FEATURES)

wf_df <- wf %>%
  as_data_frame() %>%
  filter(language %in% LANGUAGES) %>%
  group_by(language) %>%
  slice(1) %>%
  gather(feature, value, `1A`:`144A`) %>%
  group_by(feature) %>%
  mutate(value = factor(value))

cosines <- wkpd %>%
  column_to_rownames("Language") %>%
  t() %>%
  cosine() %>%
  as_data_frame(rownames = "language1") %>%
  gather(language2, cosine, -language1)

wf_many_langs <- wf_df %>%
  group_by(feature) %>%
  summarise(n = sum(!is.na(value))) %>%
  filter(n > 60)

wf_sub_langs <- wf_df %>%
  filter(feature %in% wf_many_langs$feature) %>%
  group_by(language) %>%
  summarise(keep = sum(is.na(value)) == 0) %>%
  filter(keep)

wf_sub <- wf_df %>%
  filter(feature %in% wf_many_langs$feature,
         language %in% wf_sub_langs$language) %>%
  select(language, feature, value) %>%
  mutate(value = as.numeric(as.factor(value)))


wf_pairwise <- expand.grid(language1 = wf_sub$language,
                           language2 = wf_sub$language) %>%
  left_join(wf_sub, by = c("language1" = "language")) %>%
  rename(value1 = value) %>%
  left_join(wf_sub, by = c("language2" = "language", "feature")) %>%
  rename(value2 = value) %>%
  mutate(same = value1 == value2) %>%
  select(-value1, -value2) %>%
  left_join(cosines, by = c("language1", "language2"))


models <- wf_pairwise %>%
  group_by(feature) %>%
  nest() %>%
  mutate(model = map(data, ~glm(same ~ cosine, 
                                family = "binomial", data = .)))

models %>%
  mutate(coeffs = map(model, tidy)) %>%
  select(-data, -model) %>%
  unnest() %>%
  filter(term == "cosine") %>%
  arrange(desc(statistic))

### generate dendrogram plot
dendro_langs <- wf_sub_langs$language

wk_clust_tree <- wkpd %>% 
              filter(Language %in% dendro_langs) %>%
              column_to_rownames(var = "Language") %>%
              dist() %>%
              hclust("ward.D2") 

wk_clust_tree %>% plot(hang = -1)
```

```{r clust_tree, fig.width = 3.5, fig.align='center', set.cap.width=T, fig.cap = "A dendrogram estimated from hierarchically clustering the slopes of our 45 languages", fig.show='hold', cache = T}
wk_clust_tree <- readRDS("data/clust_tree.RDS")

ggdendrogram(wk_clust_tree, rotate = F, size = 1) + 
 theme(axis.text.y = element_blank(),
       axis.text.x = element_text(size = 7))
  
```

```{r tab_features, results="asis", tab.env = "table"}
tab1 <- glm_model %>%
  rename(`t-value` = statistic, `p-value` = p.value,
         `#` = feature) %>%
  mutate(r2 = round(r2,3)) %>%
  select(-estimate,-meaning)  %>%
  xtable(caption = "Capptoin here",
         label = "tab:tab_features")

print(tab1, type="latex", comment = F, table.placement = "tb",
      include.rownames = F)
```

# General Discussion 

Taken together, the results of our studies yield 3 main findings. First, languages have characteristic entropy curves that can be found in their productions whether written, spoken, or even spoken by children. These curves diverge in reliabile ways from the monotonically increasing functions predicting by a naive Uniform Information Density hypothesis. Second, these characteristic curves vary across languages--some languages like English have increasing curves, but others like Mandarin are characterized by decreasing entropy of the course of an utterance. Finally, these characteristic curves are related to structural properties of those languages, for instance their word orders. This work presents an important rejoinder to models of linguistic productions as optimizing communicative success. While speakers may do their best to structure the information in their productions to minimize spikes in entropy, they operate under coding models they inherit from their native language, and these coding models constrain the choices they make.

# References

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
