---
title: Wikipedia Text Mining and Entropy Analysis
author: Josef Klafka and Dan Yurovsky
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: false
    number_sections: false
    theme: lumen
    toc_float: false
    code_folding: show 
---

```{r setup, include=FALSE}
# load packages
library(knitr)
library(tidyverse)
library(directlabels)
library(dplyr)
library(tidytext)
library(entropy)
library(tidyboot)
library(tokenizers)


knitr::opts_chunk$set(echo = TRUE)

opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
               error = FALSE, cache = FALSE, tidy = FALSE)

theme_set(theme_classic(base_size = 16))
```

```{r python_background}
# Here's where to find the languages of Wikipedia and their prefixes: https://en.wikipedia.org/wiki/List_of_Wikipedias#Detailed_list

LANGUAGE <- "Alemannic" # This is the language you want to examine

system(paste("python get_wiki_df.py", LANGUAGE, sep = " "))
```


Extract tokens
```{r extract_tokens}
extract_wiki_tokens <- function(min_length, max_length, utterances) {
  
  tokens <- utterances %>% 
    mutate(gloss = str_trim(gloss)) %>%
    mutate(length = str_count(gloss, pattern = " +") + 1) %>% 
    filter(length >= min_length) %>%
    filter(length <= max_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, gloss, token = stringr::str_split, pattern = " +") %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    summarise(n = n()) 
}
```

normal estimation
```{r}
estimate_slope_normal <- function(df, first_pos, second_pos) {
  
   mutate(word_order = if_else(word_order == length-1, 2,
                                if_else(word_order == length -2, 1,
                                        0))) %>%   
  
  
  df %>%
    ungroup() %>%
    filter(word_order >= first_pos, word_order <= second_pos) %>%
    mutate(word_order = word_order - max(word_order)) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    lm(entropy ~ length + I(length^2) + word_order, data  = .) %>%
    tidy() %>%
    filter(term == "word_order")

}
```

relative estimation
```{r}


df <- read_csv("wiki_df.csv")

df_tokens <- extract_wiki_tokens(6, 50, df)

NUM_SECTIONS = 5

get_quantiles <- function(df, num_sections) {
  quantile(df$word_order, probs = seq(0, 1, 1/num_sections)) %>% 
    round() %>%
    as_data_frame() %>%
    rename(word_order = value)
}


pos_list <- df_tokens %>%
  group_by(length) %>%
  distinct(word_order) %>%
  split(.$length) %>%
  map(~get_quantiles(.x, NUM_SECTIONS)) %>%
  bind_rows(.id = "length") %>%
  mutate(length = as.numeric(length))

relative_slopes <- function(df, pos_list) {
 
  get_slope <- function(x) {
    
    selected_pos <- pos_list %>%
      group_by(length) %>%
      slice(x:(x+1)) %>%
      group_by(length) %>%
      summarise(min = min(word_order),
                max = max(word_order)) %>%
      split(.$length) %>%
      map(~seq(.x$min, .x$max, 1) %>% as_data_frame) %>%
      bind_rows(.id = "length") %>%
      rename(word_order = value) %>%
      mutate(length = as.numeric(length))
    
    df %>%
      inner_join(selected_pos, by = c("length", "word_order")) %>% 
      group_by(length, word_order) %>%
      summarise(entropy = entropy(n, unit = "log2")) %>%
      lm(entropy ~ length + I(length^2) + word_order, data  = .) %>%
      tidy() %>%
      filter(term == "word_order")
  }
  
  divides <- pos_list %>%
    group_by(length) %>%
    summarise(n = n()) %>%
    summarise(n = mean(n)) %>%
    pull()
  
  map(1:((divides)-1), get_slope) %>%
    bind_rows(.id = "cut")
  
}

relative_slopes(df_tokens, pos_list)  

```

Slope helpers
```{r slope_helpers, include = F, eval=F}
start_slope <- function(df) {
 
 df %>%
    filter(word_order %in% c(1,2)) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `2` - `1`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

second_slope <- function(df) {
 
 df %>%
    ungroup() %>%
    mutate(word_order %in% c(2, 3)) %>%
    filter(word_order > 0) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `3` - `2`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

#note this measurement only makes sense if utterance length is at least 6
mid_slope <- function(df) {
 
 df %>%
    ungroup() %>%
    mutate(word_order = if_else(word_order == length-2, 2,
                                if_else(word_order == 3, 1,
                                        0))) %>%
    filter(word_order > 0) %>%
    group_by(length, word_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `2` - `1`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

penult_slope <- function(df) {
 
 df %>%
    ungroup() %>%
    mutate(word_order = if_else(word_order == length-1, 2,
                                if_else(word_order == length -2, 1,
                                        0))) %>%
    filter(word_order > 0) %>%
    group_by(length, word_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `2` - `1`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

end_slope <- function(df) {
 
 df %>%
    ungroup() %>%
    mutate(word_order = if_else(word_order == length, 2,
                                if_else(word_order == length -1 , 1,
                                        0))) %>%
    filter(word_order > 0) %>%
    group_by(length, word_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(word_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `2` - `1`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}
```

```{r get_slopes}
get_wiki_slopes <- function(min_length, max_length, utterances, REPS) {
  
  tokens <- extract_wiki_tokens(min_length, max_length, utterances)  
  
  start_slopes <- replicate(REPS, start_slope(tokens)) %>% 
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975), 
              ci_lower = quantile(value, .025))
  
  second_slopes <- replicate(REPS, second_slope(tokens)) %>% 
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975), 
              ci_lower = quantile(value, .025))
    
  mid_slopes <- replicate(REPS, mid_slope(tokens)) %>% 
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975), 
              ci_lower = quantile(value, .025))
    
  penult_slopes <- replicate(REPS, penult_slope(tokens)) %>% 
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975), 
              ci_lower = quantile(value, .025))
    
  end_slopes <- replicate(REPS, end_slope(tokens)) %>% 
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975), 
              ci_lower = quantile(value, .025))
  
  rbind(start_slopes, second_slopes, mid_slopes, penult_slopes, end_slopes)
}

```

```{r slope_analysis, include=FALSE, eval = F}
df <- read_csv("wiki_df.csv")

wiki_tokens <- extract_wiki_tokens(6, 50, df)

# plot the entropy slope for sentences of length min_length to max_length
wiki_tokens %>%
    filter(word_order %in% c(1, 2)) %>%
    group_by(length, word_order) %>%
    summarise(entropy = entropy(n, unit = "log2"), n = sum(n)) %>%
    spread(word_order, entropy) %>%
    group_by(length) %>%
    mutate(diff = `2` - `1`) %>%
  ggplot(aes(x = length, y = diff)) +
  geom_point(aes(size = n)) +
  geom_smooth(se = F) +
  scale_x_log10() #log length stats

REPS <- 100

ru_slopes <- get_wiki_slopes(min_length = 6, max_length = 50, utterances=df, REPS)
```


```{r analyze_entropies, include=FALSE, eval = F}
get_entropies <- function(sen_length, df) {
   
  tokens <- utterances %>% 
    mutate(gloss = str_trim(gloss)) %>%
    mutate(length = str_count(gloss, pattern = " +") + 1) %>% 
    filter(length >= min_length) %>%
    filter(length <= max_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, gloss, token = stringr::str_split, pattern = " +") %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(length, word_order, word) %>%
    summarise(n = n()) 
   
  tokens %>%
    summarise(n = n()) %>%
    tidyboot(summary_function = function(x) x %>% 
               summarise(entropy = entropy(n, unit = "log2")),
             statistics_functions = function(x) x %>%
             summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
    mutate(length = sen_length)
}
```