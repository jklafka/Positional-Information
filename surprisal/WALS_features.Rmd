---
title: "Wikipedia WALS Feature Analysis"
author: "Josef Klafka and Dan Yurovsky"
date: "7/9/2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(lingtypology)
library(feather)
library(here)
library(missMDA)
library(lsa)
library(reshape2)
library(broom)
library(janitor)
library(directlabels)
knitr::opts_chunk$set(echo = TRUE)

theme_set(theme_classic(base_size = 18))
```

```{r read in data}
# wals <- read_csv(here("Data/wals_isos.csv"))
wals <- read_csv(here("Data/from_website.csv"))

feature_names <- read_feather(here("Wikipedia/feature_names.feather"))
clean_names <- feature_names %>% mutate(feature = make_clean_names(feature))

codes <- read_csv(here("Wikipedia/isos.csv"))
wiki <- read_csv(here("Data/relative_ngrams.csv"))
LANGUAGES <- wiki %>% pull(language) %>% unique()
```

```{r get only languages with enough features}
all_languages <- wals %>% 
  mutate(missing = rowSums(is.na(.))) %>% 
  group_by(language) %>% 
  mutate(minimum = min(missing)) %>%
  filter(missing == minimum) %>% 
  select(-missing, minimum) %>%
  ungroup() 
```

```{r geographic area comparison}
languages <- read_csv(here("Data/languages.csv")) %>%
  select(Latitude, Longitude, ISO639P3code)

wiki <- read_csv(here("Data/relative_ngrams.csv"))

labs <- read_csv(here("Data/wiki_wals.csv")) %>% 
  group_by(language) %>% 
  slice(1) %>% 
  ungroup() %>% 
  select(language, iso)

latlong <- labs %>% 
  left_join(imputed_df) %>% 
  left_join(languages, by = c("iso" = "ISO639P3code")) %>%
  group_by(language) %>%
  slice(1) %>% 
  ungroup() %>%
  select(language, Latitude, Longitude) %>%
  filter(complete.cases(.))

sub_langs <- latlong %>% 
  pull(language)

unigram_cosines <- wiki %>%
  filter(language %in% sub_langs, gram == "Unigram") %>%
  select(-gram) %>% 
  column_to_rownames("language") %>%
  t() %>%
  cosine() %>%
  as_data_frame(rownames = "language1") %>%
  gather(language2, unigram_cosine, -language1) 

wf_pairwise <- expand.grid(language1 = sub_langs,
                           language2 = sub_langs) %>%
  filter(language1 != language2)


compute_sim <- function(language1, language2) {
  
  wf_sub %>%
    filter(language == language1) %>%
    rename(value1 = value) %>% 
    left_join(wf_sub %>% filter(language == language2) %>% select(feature, value), 
              by = "feature") %>%
    mutate(same = value == value1) %>%
    ungroup() %>%
    group_by(language) %>%
    summarise(feature = sum(same)) %>%
    mutate(language2 = language2) %>%
    rename(language1 = language)
}

wf_sub <- imputed_df %>% 
  filter(language %in% sub_langs) %>%
  group_by(language) %>%
  slice(1) %>%
  gather(feature, value, -language) %>%
  group_by(feature) %>%
  mutate(value = as.numeric(as.factor(value)))

feature_space <- map_dfr(1:nrow(wf_pairwise), ~compute_sim(wf_pairwise[.x,"language1"],
                                          wf_pairwise[.x,"language2"]))

compute_geo_sim <- function(language1, language2) {
  
  latlong %>%
    filter(language == language1) %>%
    bind_cols(latlong %>% filter(language == language2)) %>%
    mutate(geo = sqrt((Latitude1 - Latitude)^2 + (Latitude1 - Latitude)^2)) %>%
    ungroup() %>%
    rename(language2 = language1, language1 = language) %>% 
    select(language1, language2, geo)
}

geo_space <- map_dfr(1:nrow(wf_pairwise), ~compute_geo_sim(wf_pairwise[.x,"language1"],
                                          wf_pairwise[.x,"language2"]))


```

```{r imputation}
df <- read_csv(here("Data/wiki_wals.csv"))

wals <- df %>% 
  select(language, ends_with('A')) %>%
  group_by(language) %>%
  slice(1) %>% 
  ungroup()

imputed_df <- wals %>% 
  select(-`139A`, -`141A`, -`140A`) %>% # these features have no values
  select(language, `121A`:`120A`) %>%
  gather(feature, value, -language) %>%
  group_by(feature) %>%
  mutate(value = factor(value)) %>%
  select(language, feature, value) %>%
  ungroup() %>% 
  spread(feature, value) %>% 
  sapply(as.factor) %>%
  as.data.frame() %>%
  column_to_rownames("language") %>% 
  MIMCA(ncp = 2, threshold = 1e-02)
```

```{r models}
imputed_df <- read_csv(here("Data/imputed_all.csv"))

wf_sub <- imputed_df %>% 
  group_by(language) %>%
  slice(1) %>%
  gather(feature, value, -language) %>%
  group_by(feature) %>%
  mutate(value = as.numeric(as.factor(value)))

cosines <- wiki %>%
  filter(language %in% common_languages, gram == "Unigram") %>%
  select(-gram) %>% 
  column_to_rownames("language") %>%
  t() %>%
  cosine() %>%
  as_data_frame(rownames = "language1") %>%
  gather(language2, cosine, -language1)

sub_langs <- wf_sub %>%
  pull(language) %>%
  unique()

wf_pairwise <- expand.grid(language1 = sub_langs,
                           language2 = sub_langs) %>%
  filter(language1 != language2)


compute_sim <- function(language1, language2) {
  
  wf_sub %>%
    filter(language == language1) %>%
    rename(value1 = value) %>% 
    left_join(wf_sub %>% filter(language == language2) %>% select(feature, value), 
              by = "feature") %>%
    mutate(same = value == value1) %>%
    ungroup() %>%
    group_by(language) %>%
    summarise(feature = sum(same)) %>%
    mutate(language2 = language2) %>%
    rename(language1 = language)
}


feature_space <- map_dfr(1:nrow(wf_pairwise), ~compute_sim(wf_pairwise[.x,"language1"],
                                          wf_pairwise[.x,"language2"]))


models <- wf_pairwise %>%
  group_by(feature) %>%
  nest() %>%
  mutate(model = map(data, ~glm(same ~ cosine, 
                                family = "binomial", data = .)))


model_df <- models %>%
  mutate(coeffs = map(model, tidy)) %>%
  select(-data, -model) %>%
  unnest() %>%
  filter(term == "cosine") %>%
  arrange(desc(statistic)) %>%
  left_join(clean_names, by = "feature")
```

```{r cosines, eval = F}
unigram_cosines <- wiki %>%
  filter(language %in% sub_langs, gram == "Unigram") %>%
  select(-gram) %>% 
  column_to_rownames("language") %>%
  t() %>%
  cosine() %>%
  as_data_frame(rownames = "language1") %>%
  gather(language2, unigram_cosine, -language1) 

# bigram_cosines <- wiki %>%
#   filter(language %in% sub_langs, gram == "Bigram") %>%
#   select(-gram) %>% 
#   column_to_rownames("language") %>%
#   t() %>%
#   cosine() %>%
#   as_data_frame(rownames = "language1") %>%
#   gather(language2, bigram_cosine, -language1) %>%
#   mutate(bigram_cosine = 1 - abs(bigram_cosine)) 

trigram_cosines <- wiki %>%
  filter(language %in% sub_langs, gram == "Trigram") %>%
  select(-gram) %>% 
  column_to_rownames("language") %>%
  t() %>%
  cosine() %>%
  as_data_frame(rownames = "language1") %>%
  gather(language2, trigram_cosine, -language1)

all_distances <- feature_space %>%
  left_join(unigram_cosines) %>%
  left_join(trigram_cosines)

all_distances %>% 
  ggplot(aes(x = feature)) +
    geom_smooth(aes(y = unigram_cosine, color = "#0033CC"), 
                method = 'lm', se = F) + 
    # geom_smooth(aes(y = bigram_cosine, color = "#6699FF"), 
    #             method = 'lm', se = F) +
    geom_smooth(aes(y = trigram_cosine, color = "#33CCFF"), 
                method = 'lm', se = F) +
    geom_hex(aes(y = unigram_cosine, color = "#0033CC", alpha=.2)) + 
    # geom_point(aes(y = bigram_cosine, color = "#6699FF")) + 
    geom_hex(aes(y = trigram_cosine, color = "#33CCFF", alpha=.2)) + 
    ylab("Information Curve Difference") + 
    xlab("Difference in WALS Features") + 
    scale_color_manual(name = "Gram", 
                       values = c("#0033CC", "#33CCFF"), 
                       labels = c("Unigrams", "Trigrams")) 
```

```{r correlations}
correlation_data <- read_feather(here("Data/feature_cosine_spaces.feather")) %>% 
  mutate(unigram_cosine = unigram_cosine * 180) %>% 
  mutate(trigram_cosine = trigram_cosine * 180)

tidy_corr_data <- correlation_data %>%
  gather(gram, angle, unigram_cosine, trigram_cosine) %>%
  mutate(gram = if_else(gram == "unigram_cosine", "unigram", "trigram"))

ggplot(tidy_corr_data, aes(x = feature, y = angle, fill = gram,
                           color = gram, group = gram,
                           label = gram)) + 
  geom_smooth(method = "lm") +
  geom_jitter(alpha = .05) +
  geom_dl(method = "smart.grid") + 
  theme(legend.position = "none") + 
  xlab("Number of WALS features in common") + 
  ylab("Information curve distance")

library(lme4)
lmer(feature ~ angle * gram + (1|language1) + (1|language2),
     data = tidy_corr_data) %>%
  tidy()
  
  

```

## Errata

```{r get values from website}
languages <- read_csv(here("wals_dataset/languages.csv"))

values <- read_csv(here("wals_dataset/values.csv"))

values %>% 
  left_join(languages, by = c("Language_ID" = "ID")) %>% 
  select(Language_ID, Parameter_ID, Value, Name, Glottocode, ISO639P3code) %>% 
  rename(wals_code = Language_ID, feature = Parameter_ID, value = Value, 
         language = Name, glottocode = Glottocode, iso = ISO639P3code) %>% 
  spread(feature, value)

wf %>% write_csv(here("Data/from_website.csv"))
```

```{r pairing up languages with wikipedia features}
wals <- read_csv(here("Data/from_website.csv"))

all_data <- wiki %>% 
  left_join(codes, by = "language") %>% 
  left_join(wals %>% select(-language), by = "iso") %>% 
  mutate(n = rowSums(is.na(.))) %>% 
  group_by(language) %>% 
  filter(n == min(n)) %>% 
  group_by(language, gram) %>%
  slice(1) %>% 
  ungroup() 

all_data %>% 
  write_csv(here("Data/wiki_wals.csv"))
```


```{r cleaning, eval = F}
wiki <- read_feather(here("Data/relative_ngrams.feather"))
wiki %>% 
  group_by(language) %>% 
  mutate(order = 1:n()) %>% 
  ungroup() %>% 
  filter(order <= 15) %>% 
  select(-order) %>%
  select(language, gram, estimate, cut) %>% 
  spread(cut, estimate) %>% 
  rename(Slope1 =`1`, Slope2 =`2`, Slope3 =`3`, Slope4 =`4`, Slope5 =`5`) %>%
  write_csv(here("Data/relative_ngrams.csv"))
```

```{r feature space}
imputed_df <- read_csv(here("Data/imputed_all.csv"))

feature_space <- all_imputed %>% 
  gather(feature, value, -language) %>% 
  group_by(feature) %>% 
  mutate(value = as.numeric(as.factor(value))) %>% 
  spread(feature, value) %>%
  column_to_rownames("language") %>%
  dist(method = "manhattan") %>%
  as.matrix() %>%
  melt(value.name = "feature") %>%
  as_tibble() %>%
  rename(language1 = Var1, language2 = Var2) 
```

```{r get common languages between the imputations and models}
nom_categories <- all_languages %>%
  select(language, x30a:x57a) %>%
  filter(rowSums(is.na(.)) < 12)

nom_syntax <- all_languages %>%
  select(language, x58a:x64a) %>%
  filter(rowSums(is.na(.)) < 5)

verb_categories <- all_languages %>%
  select(language, x65a:x80a) %>%
  filter(rowSums(is.na(.)) < 8)

word_order <- all_languages %>%
  select(language, x81a:x97a) 

clauses <- all_languages %>%
  select(language, x98a:x121a) %>%
  filter(rowSums(is.na(.)) < 12)

common_df <- nom_categories %>%
  inner_join(nom_syntax) %>%
  inner_join(verb_categories) %>%
  inner_join(word_order) %>%
  inner_join(clauses)

# common_languages <- common_df %>%
#   pull(language)
```