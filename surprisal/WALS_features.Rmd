---
title: "Wikipedia WALS Feature Analysis"
author: "Josef Klafka and Dan Yurovsky"
date: "7/9/2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(lingtypology)
library(feather)
library(here)
library(missMDA)
library(lsa)
library(reshape2)
library(broom)
library(janitor)
knitr::opts_chunk$set(echo = TRUE)
```

```{r read in data}
# wals <- read_csv(here("Data/wals_isos.csv"))
wals <- read_csv(here("Data/from_website.csv"))

feature_names <- read_feather(here("Wikipedia/feature_names.feather"))
clean_names <- feature_names %>% mutate(feature = make_clean_names(feature))

codes <- read_csv(here("Wikipedia/isos.csv"))
wiki <- read_csv(here("Data/relative_ngrams.csv"))
LANGUAGES <- wiki %>% pull(language) %>% unique()
```


```{r get only languages with enough features}
all_languages <- wals %>% 
  mutate(missing = rowSums(is.na(.))) %>% 
  group_by(language) %>% 
  mutate(minimum = min(missing)) %>%
  filter(missing == minimum) %>% 
  select(-missing, minimum) %>%
  ungroup() 
```

```{r get common languages between the imputations and models}
nom_categories <- all_languages %>%
  select(language, x30a:x57a) %>%
  filter(rowSums(is.na(.)) < 12)

nom_syntax <- all_languages %>%
  select(language, x58a:x64a) %>%
  filter(rowSums(is.na(.)) < 5)

verb_categories <- all_languages %>%
  select(language, x65a:x80a) %>%
  filter(rowSums(is.na(.)) < 8)

word_order <- all_languages %>%
  select(language, x81a:x97a) 

clauses <- all_languages %>%
  select(language, x98a:x121a) %>%
  filter(rowSums(is.na(.)) < 12)

common_df <- nom_categories %>%
  inner_join(nom_syntax) %>%
  inner_join(verb_categories) %>%
  inner_join(word_order) %>%
  inner_join(clauses)

# common_languages <- common_df %>%
#   pull(language)
```

```{r feature space}
imputed_df <- read_csv(here("Data/imputed_wals.csv"))

feature_space <- imputed_df %>% 
  gather(feature, value, -language) %>% 
  group_by(feature) %>% 
  mutate(value = as.numeric(as.factor(value))) %>% 
  spread(feature, value) %>%
  column_to_rownames("language") %>%
  dist(method = "manhattan") %>%
  as.matrix() %>%
  melt(value.name = "feature") %>%
  as_tibble() %>%
  rename(language1 = Var1, language2 = Var2) 
```

```{r cosines}
common_languages <- read_csv(here("Data/imputed_wals.csv")) %>% 
  pull(language)

unigram_cosines <- wiki %>%
  filter(language %in% common_languages, gram == "Unigram") %>%
  select(-gram) %>% 
  column_to_rownames("language") %>%
  t() %>%
  cosine() %>%
  as_data_frame(rownames = "language1") %>%
  gather(language2, unigram_cosine, -language1) %>%
  mutate(unigram_cosine = 1 - abs(unigram_cosine)) 

bigram_cosines <- wiki %>%
  filter(language %in% common_languages, gram == "Bigram") %>%
  select(-gram) %>% 
  column_to_rownames("language") %>%
  t() %>%
  cosine() %>%
  as_data_frame(rownames = "language1") %>%
  gather(language2, bigram_cosine, -language1) %>%
  mutate(bigram_cosine = 1 - abs(bigram_cosine)) 

trigram_cosines <- wiki %>%
  filter(language %in% common_languages, gram == "Trigram") %>%
  select(-gram) %>% 
  column_to_rownames("language") %>%
  t() %>%
  cosine() %>%
  as_data_frame(rownames = "language1") %>%
  gather(language2, trigram_cosine, -language1) %>%
  mutate(trigram_cosine = 1 - abs(trigram_cosine)) 

all_distances <- feature_space %>%
  left_join(unigram_cosines) %>%
  left_join(bigram_cosines) %>%
  left_join(trigram_cosines)

all_distances %>% 
  select(-bigram_cosine) %>% 
  ggplot(aes(x = feature)) +
    geom_smooth(aes(y = unigram_cosine, color = "#0033CC"), 
                method = 'lm', se = F) + 
    # geom_smooth(aes(y = bigram_cosine, color = "#6699FF"), 
    #             method = 'lm', se = F) +
    geom_smooth(aes(y = trigram_cosine, color = "#33CCFF"), 
                method = 'lm', se = F) +
    geom_point(aes(y = unigram_cosine, color = "#0033CC")) + 
    # geom_point(aes(y = bigram_cosine, color = "#6699FF")) + 
    geom_point(aes(y = trigram_cosine, color = "#33CCFF")) + 
    ylab("Information Curve Difference") + 
    xlab("Difference in WALS Features") + 
    scale_color_manual(name = "Gram", 
                       values = c("#0033CC", "#33CCFF"), 
                       labels = c("Unigrams", "Trigrams")) 
```

```{r models}
imputed_df <- read_csv(here("Data/imputed_wals.csv"))

wf_sub <- imputed_df %>% 
  group_by(language) %>%
  slice(1) %>%
  gather(feature, value, -language) %>%
  group_by(feature) %>%
  mutate(value = as.numeric(as.factor(value)))

cosines <- wiki %>%
  filter(language %in% common_languages, gram == "Unigram") %>%
  select(-gram) %>% 
  column_to_rownames("language") %>%
  t() %>%
  cosine() %>%
  as_data_frame(rownames = "language1") %>%
  gather(language2, cosine, -language1)

sub_langs <- wf_sub %>%
  pull(language) %>%
  unique()

wf_pairwise <- expand.grid(language1 = sub_langs,
                           language2 = sub_langs) %>%
  filter(language1 != language2)


compute_sim <- function(language1, language2) {
  
  wf_sub %>%
    filter(language == language1) %>%
    rename(value1 = value) %>% 
    left_join(wf_sub %>% filter(language == language2) %>% select(feature, value), 
              by = "feature") %>%
    mutate(same = value == value1) %>%
    ungroup() %>%
    group_by(language) %>%
    summarise(feature = sum(same)) %>%
    mutate(language2 = language2) %>%
    rename(language1 = language)
}


feature_space <- map_dfr(1:nrow(wf_pairwise), ~compute_sim(wf_pairwise[.x,"language1"],
                                          wf_pairwise[.x,"language2"]))


models <- wf_pairwise %>%
  group_by(feature) %>%
  nest() %>%
  mutate(model = map(data, ~glm(same ~ cosine, 
                                family = "binomial", data = .)))


model_df <- models %>%
  mutate(coeffs = map(model, tidy)) %>%
  select(-data, -model) %>%
  unnest() %>%
  filter(term == "cosine") %>%
  arrange(desc(statistic)) %>%
  left_join(clean_names, by = "feature")
```

```{r imputation}
imputed_df <- common_df %>% 
  gather(feature, value, -language) %>%
  group_by(feature) %>%
  mutate(value = factor(value)) %>%
  select(language, feature, value) %>%
  ungroup() %>% 
  spread(feature, value) %>% 
  sapply(as.factor) %>%
  as.data.frame() %>%
  column_to_rownames("language") %>% 
  MIMCA(ncp = 1)

imputed_df <- as.data.frame(imputed_df[[1]][[99]]) 
```

## Errata

```{r get values from website}
languages <- read_csv(here("wals_dataset/languages.csv"))

values <- read_csv(here("wals_dataset/values.csv"))

values %>% 
  left_join(languages, by = c("Language_ID" = "ID")) %>% 
  select(Language_ID, Parameter_ID, Value, Name, Glottocode, ISO639P3code) %>% 
  rename(wals_code = Language_ID, feature = Parameter_ID, value = Value, 
         language = Name, glottocode = Glottocode, iso = ISO639P3code) %>% 
  spread(feature, value)

wf %>% write_csv(here("Data/from_website.csv"))
```

```{r pairing up languages with wikipedia features}
wals <- read_csv(here("Data/from_website.csv"))

all_data <- wiki %>% 
  left_join(codes, by = "language") %>% 
  left_join(wals %>% select(-language), by = "iso") %>% 
  mutate(n = rowSums(is.na(.))) %>% 
  group_by(language) %>% 
  filter(n == min(n)) %>% 
  group_by(language, gram) %>%
  slice(1) %>% 
  ungroup() 

all_data %>% 
  write_csv(here("Data/wiki_wals.csv"))
```


```{r cleaning, eval = F}
wiki <- read_feather(here("Data/relative_ngrams.feather"))
wiki %>% 
  group_by(language) %>% 
  mutate(order = 1:n()) %>% 
  ungroup() %>% 
  filter(order <= 15) %>% 
  select(-order) %>%
  select(language, gram, estimate, cut) %>% 
  spread(cut, estimate) %>% 
  rename(Slope1 =`1`, Slope2 =`2`, Slope3 =`3`, Slope4 =`4`, Slope5 =`5`) %>%
  write_csv(here("Data/relative_ngrams.csv"))
```
