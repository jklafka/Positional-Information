---
title: Wikipedia Text Mining and Surprisal Analysis
author: Josef Klafka and Dan Yurovsky
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: false
    number_sections: false
    theme: lumen
    toc_float: false
    code_folding: show 
---

```{r setup, include=FALSE}
library(tidyverse)
library(googlesheets)
library(lingtypology)
library(knitr)
library(missMDA)
library(cluster)
library(widyr)
library(lsa)
library(magrittr)
library(broom)
library(here)
library(feather)
library(FactoMineR)
library(feather)
library(janitor)
library(ggdendro)
library(jsonlite)

knitr::opts_chunk$set(echo = TRUE)

opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
               error = FALSE, cache = FALSE, tidy = FALSE)

theme_set(theme_classic(base_size = 16))
```

The section below reads in the Wikipedia unigram surprisal data and creates a dendrogram of the slope embeddings. The dendrogram qualitatively resembles a tree of language families with some degree of accuracy. In the rest of this part of the project, we investigate what determines the placement of a language in this dendrogram. 

```{r make dendrogram}
wiki <- read.csv(here("Data/wikipedia_unigrams.csv")) %>%
  select(-X) %>% 
  column_to_rownames("Language")

wiki %>% 
  dist() %>% 
  hclust() %>% 
  ggdendrogram(rotate = FALSE, size = .1) + 
  ylim(0, .10)

  # plot(hang = -1, 
  #      main = "Dendrogram of slope embeddings")

```

Always run this section. It takes time, but this gets all of the WALS features for every language in WALS along with the unigram slope data for all of the Wikipedia languages we've run. 

```{r get features, eval = F}
wkpd <- read_csv(here("Data/wikipedia_unigrams.csv"))
LANGUAGES <- wkpd$Language

FEATURES <- c(1:144)
for (i in c(1:length(FEATURES))) {
  FEATURES[i] <- paste(FEATURES[i], "A", sep = "")
} #get all word order WALS features in correct syntax

wals_isos <- wals %>%
  mutate(iso = iso.gltc(glottocode)) %>%
  select(-glottocode)

wiki_isos <- read_csv(here("Wikipedia/wiki_isos.csv")) %>%
  clean_names() %>%
  select(x639_1, x639_3)

wiki_langs <- fromJSON(here("/Wikipedia/language_dict.json")) %>%
  as_tibble() %>%
  gather(language, x639_1) %>%
  left_join(wiki_isos)


%>%
  left_join(wals_isos) %>%
  filter(is.na(wals.code)) %>%
  View()

wf <- wals.feature(FEATURES) 

write_csv(wf, here("Data/wals.csv"))
```

```{r load-wals, eval = T}
wf <- read_csv(here("Data/wals.csv"))
```


Run only _one_ of the following five sections, depending on which typological features you are interested in. Look here (https://wals.info/feature) for more information about the features. 

```{r nominal categories}
wo_wf <- wf %>%
  filter(language %in% LANGUAGES) %>%
  select(language, `30A`:`57A`) #%>%
wo_wf$non <- rowSums(is.na(wo_wf))
wo_wf %<>%
  filter(non < 12) %>%
  select(-non)
```

```{r nominal syntax}
wo_wf <- wf %>%
  filter(language %in% LANGUAGES) %>%
  select(language, `58A`:`64A`) #%>%
wo_wf$non <- rowSums(is.na(wo_wf))
wo_wf %<>%
  filter(non < 5) %>%
  select(-non)
```
```{r verbal categories}
wo_wf <- wf %>%
  filter(language %in% LANGUAGES) %>%
  select(language, `65A`:`80A`) #%>%
wo_wf$non <- rowSums(is.na(wo_wf))
wo_wf %<>%
  filter(non < 8) %>%
  select(-non)
```
```{r word order}
wo_wf <- wf %>%
  filter(language %in% LANGUAGES) %>%
  select(language, `81A`:`97A`) %>%
  select(-`84A`)
```
```{r clauses}
wo_wf <- wf %>%
  filter(language %in% LANGUAGES) %>%
  select(language, `98A`:`121A`) #%>%
wo_wf$non <- rowSums(is.na(wo_wf))
wo_wf %<>%
  filter(non < 12) %>%
  select(-non)
```

Run this section for imputation and model fitting. The models will be written into a tiny dataframe: `model_df`. Each row of this dataframe has the id of the feature along with a short, informative name about the feature. Again, see (https://wals.info/feature) for in-depth information about each feature. The estimates, standard errors, statistics and p-values of the models are also contained in `model_df`. 

You can adjust the number of components and/or the number of iterations of the imputation algorithm in the `MIMCA` command at the bottom of the first code chunk in this section. What we leave in the code we know to converge for each feature category. 

```{r imputation and models}
imputed_wo <- wo_wf %>%
  as_data_frame() %>%
  group_by(language) %>%
  slice(1) %>%
  gather(feature, value, -language) %>%
  group_by(feature) %>%
  mutate(value = factor(value)) %>%
  select(language, feature, value) %>%
  ungroup() %>% 
  spread(feature, value) %>% 
  sapply(as.factor) %>%
  as.data.frame() %>%
  column_to_rownames("language") %>%
  MIMCA(ncp = 1)

imputed_wo <- as.data.frame(imputed_wo[[1]][[99]]) 

wo_sub <- imputed_wo %>% 
  rownames_to_column("language") %>%
  group_by(language) %>%
  slice(1) %>%
  gather(feature, value, -language) %>%
  group_by(feature) %>%
  mutate(value = as.numeric(as.factor(value)))

cosines <- wkpd %>%
  column_to_rownames("Language") %>%
  t() %>%
  cosine() %>%
  as_data_frame(rownames = "language1") %>%
  gather(language2, cosine, -language1)

wf_pairwise <- expand.grid(language1 = wo_sub$language,
                           language2 = wo_sub$language) %>%
  left_join(wo_sub, by = c("language1" = "language")) %>%
  rename(value1 = value) %>%
  left_join(wo_sub, by = c("language2" = "language", "feature")) %>%
  rename(value2 = value) %>%
  mutate(same = value1 == value2) %>%
  select(-value1, -value2) %>%
  left_join(cosines, by = c("language1", "language2"))

models <- wf_pairwise %>%
  group_by(feature) %>%
  nest() %>%
  mutate(model = map(data, ~glm(same ~ cosine, 
                                family = "binomial", data = .)))

feature_names <- read_feather(here("Wikipedia/feature_names.feather"))

model_df <- models %>%
  mutate(coeffs = map(model, tidy)) %>%
  select(-data, -model) %>%
  unnest() %>%
  filter(term == "cosine") %>%
  arrange(desc(statistic)) %>%
  left_join(feature_names)
```