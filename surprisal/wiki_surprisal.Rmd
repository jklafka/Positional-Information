---
title: Wikipedia Text Mining and Surprisal Analysis
author: Josef Klafka and Dan Yurovsky
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: false
    number_sections: false
    theme: lumen
    toc_float: false
    code_folding: show 
---

```{r setup, include=FALSE}
library(tidyverse)
library(googlesheets)
library(lingtypology)
library(knitr)
library(missMDA)
library(cluster)
library(widyr)
library(lsa)
library(magrittr)
library(broom)
library(here)
library(feather)
library(FactoMineR)
library(feather)
library(janitor)
library(ggdendro)

knitr::opts_chunk$set(echo = TRUE)

opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
               error = FALSE, cache = FALSE, tidy = FALSE)

theme_set(theme_classic(base_size = 16))
```

The section below reads in the Wikipedia unigram surprisal data and creates a dendrogram of the slope embeddings. The dendrogram qualitatively resembles a tree of language families with some degree of accuracy. In the rest of this part of the project, we investigate what determines the placement of a language in this dendrogram. 

```{r make dendrogram}
wiki_unigrams <- read.csv(here("Data/relative_ngrams.csv")) %>%
  filter(gram == "Unigram") %>%
  sample_n(100) %>%
  column_to_rownames("language")

wiki_unigrams %>% 
  dist() %>% 
  hclust() %>% 
  ggdendrogram(rotate = FALSE, size = .1) + 
  ylim(0, .10)

  # plot(hang = -1, 
  #      main = "Dendrogram of slope embeddings")

```

Always run this section. This gets all of the WALS features, for every language, along with the unigram slope data for all of the Wikipedia languages we've run. 

```{r get features, eval=T}
wkpd <- read.csv(here("Data/wikipedia_unigrams.csv"))
LANGUAGES <- wkpd$language

wf <- read_csv(here("Data/wals.csv")) 
feature_names <- read_feather(here("Wikipedia/feature_names.feather"))
```

Run only _one_ of the following five sections, depending on which typological features you are interested in. Look here (https://wals.info/feature) for more information about the features. 

```{r nominal categories, eval=F}
wo_wf <- wf %>%
  filter(language %in% LANGUAGES) %>%
  select(language, x30A:x57a) %>%
  filter(rowSums(is.na(.)) < 12)

```
```{r nominal syntax, eval=F}
wo_wf <- wf %>%
  filter(language %in% LANGUAGES) %>%
  select(language, x58a:x64a) %>%
  filter(rowSums(is.na(.)) < 5)

```
```{r verbal categories, eval=F}
wo_wf <- wf %>%
  filter(language %in% LANGUAGES) %>%
  select(language, x65a:x80a) %>%
  filter(rowSums(is.na(.)) < 8)

```
```{r word order, eval=F}
wo_wf <- wf %>%
  filter(language %in% LANGUAGES) %>%
  select(language, x81Aa:x97a) 
```
```{r clauses, eval=F}
wo_wf <- wf %>%
  filter(language %in% LANGUAGES) %>%
  select(language, x98a:x121a) %>%
  filter(rowSums(is.na(.)) < 12)

```

Run this section for imputation and model fitting. The models will be written into a tiny dataframe: `model_df`. Each row of this dataframe has the id of the feature along with a short, informative name about the feature. Again, see (https://wals.info/feature) for in-depth information about each feature. The estimates, standard errors, statistics and p-values of the models are also contained in `model_df`. 

You can adjust the number of components and/or the number of iterations of the imputation algorithm in the `MIMCA` command at the bottom of the first code chunk in this section. What we leave in the code we know to converge for each feature category. 

```{r imputation and models}
imputed_wo <- wo_wf %>%
  as_data_frame() %>%
  group_by(language) %>%
  slice(1) %>%
  gather(feature, value, -language) %>%
  group_by(feature) %>%
  mutate(value = factor(value)) %>%
  select(language, feature, value) %>%
  ungroup() %>% 
  spread(feature, value) %>% 
  sapply(as.factor) %>%
  as.data.frame() %>%
  column_to_rownames("language") %>%
  MIMCA(ncp = 1)

imputed_wo <- as.data.frame(imputed_wo[[1]][[99]]) 

wo_sub <- imputed_wo %>% 
  rownames_to_column("language") %>%
  group_by(language) %>%
  slice(1) %>%
  gather(feature, value, -language) %>%
  group_by(feature) %>%
  mutate(value = as.numeric(as.factor(value)))

cosines <- wkpd %>%
  column_to_rownames("language") %>%
  t() %>%
  cosine() %>%
  as_data_frame(rownames = "language1") %>%
  gather(language2, cosine, -language1)

wf_pairwise <- expand.grid(language1 = wo_sub$language,
                           language2 = wo_sub$language) %>%
  left_join(wo_sub, by = c("language1" = "language")) %>%
  rename(value1 = value) %>%
  left_join(wo_sub, by = c("language2" = "language", "feature")) %>%
  rename(value2 = value) %>%
  mutate(same = value1 == value2) %>%
  select(-value1, -value2) %>%
  left_join(cosines, by = c("language1", "language2"))

models <- wf_pairwise %>%
  group_by(feature) %>%
  nest() %>%
  mutate(model = map(data, ~glm(same ~ cosine, 
                                family = "binomial", data = .)))

model_df <- models %>%
  mutate(coeffs = map(model, tidy)) %>%
  select(-data, -model) %>%
  unnest() %>%
  filter(term == "cosine") %>%
  arrange(desc(statistic)) %>%
  left_join(feature_names)
```