---
title: "Switchboard and BNC Suprisal"
author: "Josef Klafka & Dan Yurovsky"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: false
    number_sections: false
    theme: lumen
    toc_float: false
    code_folding: show 
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(directlabels)
library(tidytext)
library(tidyboot)
library(dplyr)
library(tokenizers)
library(gtools)
library(here)
library(feather)
library(gridExtra)
library(tidyboot)

knitr::opts_chunk$set(echo = TRUE)
```

```{r get switchboard}
switchboard <- read_feather(here("../switchboard/switchboard.feather")) %>%
  rename(text = value) %>%
  mutate(length = str_count(text, pattern = "[ +]+") + 1) %>%
  mutate(utterance_id = 1:n()) %>%
  unnest_tokens(word, text, token = stringr::str_split, pattern = "[ +]+") %>%
  group_by(utterance_id) %>%
  mutate(word_order = 1:n())
```

```{r switchboard unigrams}
switch_unigrams <- switchboard %>%
  group_by(word) %>%
  count() %>%
  ungroup() %>%
  mutate(p = n / sum(n))
  
switch_surprisals <- switchboard %>%
  left_join(switch_unigrams) %>%
  mutate(s = -log(p)) %>%
  group_by(length, word_order) %>%
  summarise(s = mean(s))

# 
# sw <- switchboard %>%
#   left_join(switch_unigrams) %>%
#   mutate(s = -log(p)) %>%
#   ungroup() %>%
#   select(length, word_order, s) %>%
#   nest(s)
# 
# swc <- sw %>% filter(length %in% c(10, 20, 30))
# switch_surprisals <- map_dfr(1:nrow(swc), 
#         ~bind_cols(select(slice(swc, .x), length, word_order), 
#                                tidyboot_mean(data = unnest(slice(swc, .x)), 
#                                              column = s, nboot = 1000)))

```

```{r switchboard bigrams}
switch_bigrams <- switchboard %>%
  group_by(utterance_id) %>%
  mutate(lag_word = lag(word)) %>%
  group_by(lag_word, word) %>%
  count() %>%
  filter(!is.na(lag_word)) %>%
  ungroup() %>%
  left_join(switch_unigrams, by = c("lag_word" = "word")) %>%
  mutate(cond_p = n.x / n.y) %>%
  select(-n.y, -p) %>%
  rename(n = n.x)


bigram_surprisals_prep <- switchboard %>%
  group_by(utterance_id) %>%
  mutate(lag_word = lag(word)) %>%
  left_join(switch_bigrams) %>%
  left_join(switch_unigrams)


switch_bigram_surprisals <- bigram_surprisals_prep %>%
  mutate(s = ifelse(is.na(lag_word), -log(p), -log(cond_p))) %>%
  group_by(length, word_order) %>%
  summarise(s = mean(s)) %>% 
  filter(word_order <= length)

```

```{r switchboard trigrams}
switch_trigrams <- switchboard %>%
  group_by(utterance_id) %>%
  mutate(lag_word1 = lag(word)) %>%
  mutate(lag_word2 = lag(lag_word1)) %>%
  group_by(lag_word2, lag_word1, word) %>%
  count() %>%
  filter(!is.na(lag_word1), !is.na(lag_word2)) %>%
  ungroup() %>%
  left_join(switch_bigrams, by = c("lag_word2" = "lag_word", "lag_word1" = "word")) %>%
  mutate(tri_cond_p = n.x / n.y) %>%
  select(-n.x, -n.y, -cond_p) 

trigram_surprisals_prep <- switchboard %>%
  group_by(utterance_id) %>%
  mutate(lag_word1 = lag(word)) %>%
  mutate(lag_word2 = lag(lag_word1)) %>%
  left_join(switch_trigrams) %>% 
  left_join(switch_bigrams, by = c("lag_word1" = "lag_word", "word" = "word")) %>% 
  left_join(switch_unigrams, by = c("word" = "word"))


switch_trigram_surprisals <- trigram_surprisals_prep %>%
  mutate(s = ifelse(is.na(lag_word2), # check if it's not the first word
                    ifelse(is.na(lag_word1), # check if not the second word
                           -log(p), # trigram if third or beyond
                           -log(cond_p)), # bigram if second word
                    -log(tri_cond_p))) %>% # unigram if first word
  group_by(length, word_order) %>%
  summarise(s = mean(s)) %>% 
  filter(word_order <= length)

```

```{r saving switchboard surprisals}
switch_surprisals %>% 
  mutate(context = 0) %>% 
  bind_rows(switch_bigram_surprisals %>% mutate(context = 1), 
            switch_trigram_surprisals %>% mutate(context = 2)) %>%
  mutate(corpus = "Switchboard") %>% 
  write_feather(here("Data/switchboard_words.feather"))
```

A pre-requisite for the next section is that you run the "process_bnc.Rmd" pipeline for preprocessing the British National Corpus (bnc). The files are too large to store in the GitHub repository. 

```{r bnc}
bnc <- read_feather(here("../bnc/bnc10.feather")) %>%
  bind_rows(read_feather(here("../bnc/bnc20.feather"))) %>%
  bind_rows(read_feather(here("../bnc/bnc30.feather")))
```

```{r bnc unigrams}
bnc_unigrams <- bnc %>%
  group_by(word) %>%
  count() %>%
  ungroup() %>%
  mutate(p = n / sum(n))
  
bnc_surprisals <- bnc %>%
  left_join(bnc_unigrams) %>%
  mutate(s = -log(p)) %>%
  group_by(length, word_order) %>%
  summarise(s = mean(s))



bnc_surprisals_prep <- bnc %>% 
  left_join(bnc_unigrams) %>%
  mutate(s = -log(p)) %>%
  ungroup() %>%
  select(length, word_order, s) %>%
  nest(s)

bnc_surprisals <- map_dfr(1:nrow(bnc_surprisals_prep), 
        ~bind_cols(select(slice(bnc_surprisals_prep, .x), length, word_order), 
                               tidyboot_mean(data = unnest(slice(bnc_surprisals_prep, .x)), 
                                             column = s, nboot = 100)))  


# bnc_surprisals <- bnc %>%
#   left_join(bnc_unigrams) %>%
#   mutate(s = -log(p)) %>%
#   select(length, word_order, s) %>%
#   filter(length == 10) %>%
#   group_by(length, word_order) %>%
#   # summarise(s = mean(s)) 
#   tidyboot_mean(s)

```

```{r bnc bigrams}
bnc_bigrams <- bnc %>%
  group_by(utterance_id) %>%
  mutate(lag_word = lag(word)) %>%
  group_by(lag_word, word) %>%
  count() %>%
  filter(!is.na(lag_word)) %>%
  ungroup() %>%
  mutate(joint_p = n / sum(n)) %>%
  select(-n) %>%
  left_join(bnc_unigrams, by = c("lag_word" = "word")) %>%
  mutate(cond_p = joint_p / p) %>%
  select(-n, -p)


bigram_surprisals_prep <- bnc %>%
  group_by(utterance_id) %>%
  mutate(lag_word = lag(word)) %>%
  left_join(bnc_bigrams) %>%
  left_join(bnc_unigrams)

bnc_bigram_surprisals <- bigram_surprisals_prep %>%
  mutate(s = ifelse(is.na(lag_word), -log(p), -log(cond_p))) %>%
  group_by(length, word_order) %>%
  summarise(s = mean(s)) %>% 
  filter(word_order <= length)
```

```{r bnc trigrams}
bnc_trigrams <- bnc %>%
  group_by(utterance_id) %>%
  mutate(lag_word1 = lag(word)) %>%
  mutate(lag_word2 = lag(lag_word1)) %>%
  group_by(lag_word2, lag_word1, word) %>%
  count() %>%
  filter(!is.na(lag_word1), !is.na(lag_word2)) %>%
  ungroup() %>%
  mutate(tri_joint_p = n / sum(n)) %>%
  select(-n) %>%
  left_join(bnc_bigrams, by = c("lag_word2" = "lag_word", "lag_word1" = "word")) %>%
  mutate(tri_cond_p = tri_joint_p / joint_p) %>%
  select(-joint_p, -tri_joint_p, -cond_p) # %>% 


trigram_surprisals_prep <- bnc %>%
  group_by(utterance_id) %>%
  mutate(lag_word1 = lag(word)) %>%
  mutate(lag_word2 = lag(lag_word1)) %>%
  left_join(bnc_trigrams) %>% 
  left_join(select(bnc_bigrams, -joint_p), by = c("lag_word1" = "lag_word", "word" = "word")) %>% 
  left_join(bnc_unigrams)


bnc_trigram_surprisals <- trigram_surprisals_prep %>%
  mutate(s = ifelse(is.na(lag_word2), # check if it's not the first word
                    ifelse(is.na(lag_word1), # check if not the second word
                           -log(p), # trigram if third or beyond
                           -log(cond_p)), # bigram if second word
                    -log(tri_cond_p))) %>% # unigram if first word
  group_by(length, word_order) %>%
  summarise(s = mean(s)) %>% 
  filter(word_order <= length)

bnc_trigram_surprisals %>%
  ggplot(aes(x = word_order, y = s)) + 
  facet_wrap(~ length) + 
  geom_point() + 
  geom_smooth(se = F)
```


```{r plotting}
bnc_plot <- ggplot(bnc_surprisals, aes(x = word_order, y = s)) +
  facet_wrap(~ length, scales = "free_x") + 
  xlab("British National Corpus word position") +
  ylab("Mean surprisal") +
  geom_line() + 
  geom_point() + 
  theme_classic(base_size = 10)

sw_plot <- switch_surprisals %>%
  filter(length %in% c(10, 20, 30)) %>% 
  ggplot(aes(x = word_order, y = s)) +
  facet_wrap(~ length, scales = "free_x") + 
  xlab("Switchboard word position") +
  ylab("Mean surprisal") +
  geom_line() +
  geom_point() +
  theme_classic(base_size = 10)

grid.arrange(bnc_plot, sw_plot, nrow = 2)
```