---
title: "Information within words in English, French and German speech"
author: Josef Klafka and Dan Yurovsky 
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: false
    number_sections: true
    theme: lumen
    toc_float: false
    code_folding: hide 
editor_options: 
  chunk_output_type: console
---

```{r load-libraries, library, message=F, results='hide', warning=FALSE}
library(tidytext)
library(tidyverse)
library(childesr)
library(glue)
library(feather)
library(here)
library(entropy)
library(tidyboot)
library(janitor)
library(arrangements)
library(stringr)

knitr::opts_chunk$set(echo = TRUE, cache = T, warning = F, 
                      fig.width = 8, fig.height = 4)

theme_set(theme_classic(base_size = 16))
```

```{r read-data}
all_data <- read_feather(here("Data/all_data.feather"))
```

How is information spread among the sounds in English words? A salient feature of African American English Vernacular is the reduction of final consonants and consonant clusters. Why hasn't this created communication problems, namely identifying and disambiguating words? One possibility is that the information important for identifying a word is not uniformly distributed across all of its characters or sounds, but instead concentrated at the beginning or middle of the word. In these cases, reducing final sounds might reduce the articulatory complexity of words without much cost for the listener in processing these words and inferring their intended meaning. By the time the listener hears only the first sounds of a word, they may already have identified which word they are hearing. 

Through a series of exploratory analyses we tackle this problem in three ways:

1. Computing the surprisal of characters in English words from baserates of character production (unigram) and then conditioning on the prior charaters in each word (bigram and trigram). This analyses roughly captures the difficulty that a listener might have in predicting each character.
2. Repeating this analysis at the level of phonemes rather than characters, arguably a better proxy for the speaker's and listener's representations of spoken English. 
3. Computing hold-out entropy for individual phonemes in English words. This is roughly equivalent to asking "if you knew every other sound in a word, how much trouble would you have predicting what sounds appears in position $X$". 
4. Comparing our English by-letter analysis to results for French and German which have different expected information profiles. We expect that French words will have less information at the end of words compared to English, while German words will have more compared to English. 

Across all of our analyses, we find that the letters and sounds at the end of words in English have less information than the first letters and sounds in a word. Letters at the end of words in French and German also have little information compared to letters at the beginning of words in those languages, like in English.

# English Letters


```{r plot-hist-helper}
plot_hists <- function(select_language = "English", select_rep = "letter") {

    all_data %>%
    filter(language == select_language, rep == select_rep) %>%
    distinct(word_length, n, rep) %>%
    ggplot(aes(x = word_length, y = n)) +
    geom_col(fill = "white", color = "black") +
    xlab("Word length") + 
    ylab("Frequency")
}
    # xlab(paste0(str_to_sentence(select_rep), " Length")) +

```

We use the Providence corpus from CHILDES as data for English. In the plots below, each plot represents the information in each letter, divided by words of different lengths. Word lengths are given by number of letters. See below a histogram of the distribution of word lengths in the Providence corpus. We observe that most words ($84%$) have $5$ letters or fewer, while $99%$ of words in the corpus have fewer than $10$ letters. There are a tiny number of outliers, mainly onomatopoeia, which have $10$ or more letters. 

```{r get cumsums for letter and phone frequencies, include = F, eval = F}
get_freq_sums <- function(select_language = "English", select_rep = "letter") {
  
  all_data %>% 
    filter(language == select_language, rep == select_rep) %>% 
    distinct(word_length, n, rep) %>% 
    mutate(share = n/sum(n)) %>% 
    mutate(cumsum = cumsum(share)) %>% 
    mutate(language = select_language)
}
```

```{r plot-english-letter-hist}
plot_hists()
```

We see a distinctive shape for information at the level of individual letters in English. The first letter has relatively high information, the second letter has less information and the third letter has high information, and the final letter has less information (if the word has more than $3$ letters). 

```{r plot-curve-helper}
plot_curves <- function(select_language = "English", select_rep = "letter", 
                        select_gram = "Unigram") {
  all_data %>%
    filter(language == select_language, rep == select_rep,
           gram == select_gram) %>%
    filter(word_length %in% 2:6) %>%
    ggplot(aes(x = order, y = mean_s)) +
    facet_grid(. ~ word_length, 
               labeller = as_labeller(function(x) paste0("Length ", x))) +
    geom_point() +
    geom_line() +
    xlab(paste0(str_to_sentence(select_rep), " Position")) +
    ylab("Average information") + 
    ggtitle(paste0("Information in ", select_language, " ", 
                   select_rep, "s by ",select_rep, " position - ", select_gram))
}
```

```{r prov-letter-unigrams}
plot_curves()
```

Now we ask: if you know the previous letter, how much information is in the next letter in English? There is relatively less information in each letter knowing the previous letter, but we see the same general information trajectory across letters. 

```{r prov-letter-bigrams}
plot_curves(select_gram = "Bigram")
```

What about if you know the last two letters and are trying to predict the next letter? We see decreasing information within words as you move left-to-right. 

```{r prov-letter-trigrams}
plot_curves(select_gram = "Trigram")
```

# English Phonemes

Briefly, what we're doing is splitting each word in the Providence corpus from CHILDES up into its constituent phones--the sounds that make up the word. We split up the words in the corpus by their length in number of phones--for example, the word "food" has three phones: one for the "f", one for the long "oo", and one for the "d". We would take all words like this that have three phones. For each phone position  within the words of that length, we calculate the average information in that position. 

As seen in the histogram below, almost all words in the Providence corpus ($92%$) have four or fewer phones. $99.5%$ have seven or fewer phones.

```{r plot-english-phone-hist}
plot_hists(select_rep = "phone")
```

Unlike with letters, there isn't as much of a consistent shape to the information distribution for phones in the unigram condition. If anything, the first phone has relatively little information while the second phone has a lot of information. 

```{r prov-phone-unigrams}
plot_curves(select_rep = "phone")
```

With bigrams and trigrams for English phones, the next phone is very predictable if you know the phone or two that came before it. Especially if you know the preceding two phones, then each phone past the first two is predictable and carries relatively little information in English. 

```{r prov-phone-bigrams}
plot_curves(select_rep = "phone", select_gram = "Bigram")
```

```{r prov-phone-trigrams}
plot_curves(select_rep = "phone", select_gram = "Trigram")
```

# Hold-out entropy

For this next analysis, we took all words with $2$, $3$, $4$, $5$ or $6$ phones. For each word length, we held out the phones in one phone position. For example, in all words with 4 phones, we held out the first phone. We then computed the entropy, or amount of variation, in the remaining words. The higher the entropy when holding out phones in a given position, the more information is contained in that phone on average. 

```{r leave-out-entropy, include = F}
hold_out_entropy <- function(df) {
  spread_df <- df %>%
    spread(phone_order, phone) %>%
    clean_names()
  
  phone_length <- max(df$phone_order)
  
  lens <- paste0("x", 1:phone_length)
  
  boot_entropy <- function(group) {
    spread_df %>%
      group_by_at(group) %>%
      summarise(n = n()) %>%
      summarise(entropy = entropy(n)) %>%
      ungroup() %>%
      tidyboot_mean(entropy)
  }
  
  
  groupings <- combinations(lens, phone_length -1, 
                            layout = "list") 
  
  groupings <- map(seq(length(lens),1,-1),
                   function(x) c(groupings[[x]], lens[x]))
  
  map_df(groupings, boot_entropy) %>%
    mutate(phone_order = seq(phone_length, 1, -1))
  
}
```

```{r show-leave-out-entropy}
split_lengths <- adult_phonemes %>%
  group_by(word_length) %>%
  nest() %>%
  filter(word_length > 1) %>%
  filter(word_length %in% 1:6 ) %>%
  mutate(entropy = map(data, hold_out_entropy))

split_lengths %>%
  select(-data) %>%
  unnest() %>%
  ggplot(aes(x = phone_order, y = empirical_stat)) + 
  facet_wrap(~ word_length, 
               labeller = as_labeller(function(x) paste0("Length ", x)),
               scales = "free") +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  theme_classic() + 
  geom_line() + 
  xlab("Phone position") + 
  ylab("Entropy")
```

We see that word-initial phones tend to have more information in them than word-medial or word-final phones, so less is lost by dropping the final sounds in a word than in dropping the initial sounds. 

# French and German

For our final analysis, we're going to look at how the information within words in French and German compare to English. We use the French Palasis corpus and German Wagner corpus from CHILDES for our data. We chose French because for many French words the final letters are not pronounced regardless of how the letters in the word appear. For example, "ils sauraient" meaning "they would know" is pronounced without the final "ent" in "sauraient". We are interested in German because German has a much richer case system than English or French, where case markings in German are suffixes at the ends of words. We think that in German, the ends of words may be more important than in French or English. 


## French letters

First, we examine the word length frequencies by number of letters in the French Palasis corpus. In the Palasis corpus, $99%$ of words have $10$ letters or fewer. This French corpus has a higher share of words with six to ten letters than the English Providence corpus: $24%$ versus $16%$. However, almost all words in both corpora have $10$ or fewer letters. While there are relatively more medium length words in the French corpus compared to the English corpus, there are very few long words in either corpus. 

```{r plot-french-hist}
plot_hists(select_language = "French")
```

In French, the final letters in a lot of words are dropped when those words are spoken aloud. We expect that the final letters in French words will on average contain little information, and that is what we see. In particular, if you know the two letters that came before, then each letter beyond the first in French has little information. Even if you don't know which letters came before, then the final letters of French words have relatively little information in them. 

```{r plot french letter unigrams}
plot_curves(select_language = "French")
```

```{r plot french letter bigrams}
plot_curves(select_language = "French", select_gram = "Bigram")
```

```{r french-letter-trigrams}
plot_curves(select_language = "French", select_gram = "Trigram")
```

## German letters

In the German Wagner corpus, the distribution of word lengths in number of letters has a thicker tail: $99%$ of words have $13$ or fewer letters, compared to the Palasis and Providence corpora where over $99%$ of words had $10$ or fewer letters. However, $97.4%$ of German words have $10$ or fewer letters, so the corpus is similarly comprised of short and medium-length words.  

```{r plot-german-hist}
plot_hists(select_language = "German")
```

We expect that German, as a language with a case system which gets shown through affixation on nouns, will have relatively higher information in the final letters of words compared to English or French. We don't observe this, even in unigrams. The final letters of German words hold little information compared to the first letter of German words, which is even more noticeable when you have two letters of predictive context for each word. 

```{r plot german letter unigrams}
plot_curves(select_language = "German")
```

```{r plot german letter bigrams}
plot_curves(select_language = "German", select_gram = "Bigram")
```

```{r plot german letter trigrams}
plot_curves(select_language = "German", select_gram = "Trigram")
```

# General Discussion

Across all three languages we examined and both letters and phonemes in English, we found that the first two or three letters and sounds in a word contain relatively more information than any other letters or sounds in the word. In particular, when we knew the letters or sounds that came before, the middles and ends of words became predictable and held little information. Even for a language like German which marks case at the end of nouns, listeners can identify and disambiguate the words they hear by the time they have heard the first few sounds. 