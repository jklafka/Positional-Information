---
title: "Phoneme surprisal"
author: Josef Klafka and Dan Yurovsky 
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: false
    number_sections: false
    theme: lumen
    toc_float: false
    code_folding: hide 
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(tidytext)
library(tidyverse)
library(childesr)
library(glue)
library(feather)
library(here)
library(entropy)
library(tidyboot)
library(janitor)
library(arrangements)

knitr::opts_chunk$set(echo = TRUE)
```

We're going to discuss the amount of information across letters in English, then information in English phones (sounds), then the hold-out entropy in English and finally the information in letters in French and German. We are interested in French because when many French words are pronounced, the final letters are not pronounced in any way (e.g. "ils sauraient"="they would know", the final "ent" in "sauraient" is not pronounced at all). We are interested in German because German has a much richer case system than English or French, and case markings in German are suffixes (at the ends of words). So in German, the ends of words may be much more important than in French or English. 

```{r read data}
prov_letters <- read_feather(here("Data/providence_letters.feather"))
prov_phones <- read_feather(here("Data/providence_phonemes.feather"))
adult_phonemes <- read_feather(here("Data/adult_phonemes.feather"))
french_letters <- read_feather(here("Data/palasis_letters.feather"))
german_letters <- read_feather(here("Data/wagner_letters.feather"))
```

# English Letters

How much information is in each letter in English words? We use the Providence corpus from CHILDES. In the plots below, each plot represents the information in each letter in words of different lengths by number of letters--we show the length above each plot. 

We see a distinctive shape for information at the level of individual letters in English. The first letter has relatively high information, the second letter has less information and the third letter has high infomration, and the final letter (if the word has more than 3 letters) has less information. 

```{r plot prov letter unigrams}
prov_letters %>%
  filter(gram == "Unigram") %>%
  filter(word_length %in% 2:6) %>%
  ggplot(aes(x = letter_order, y = mean_s)) +
    facet_wrap(~word_length) +
    geom_point() +
    geom_line() +
    xlab("Phone Position") +
    ylab("Average Information") + 
    ggtitle("Information in English letters by letter position - unigrams")
```

Now we ask: if you know the last letter, how much information is in the next letter in English? There is relatively less information in each letter knowing the previous letter, but we see the same general information trajectory across letters. 

```{r plot prov letter bigrams}
prov_letters %>%
  filter(gram == "Bigram") %>%
  filter(word_length %in% 2:6) %>%
  ggplot(aes(x = letter_order, y = mean_s)) +
    facet_wrap(~word_length) +
    geom_point() +
    geom_line() +
    xlab("Phone Position") +
    ylab("Average Information") + 
    ggtitle("Information in English letters by letter position - bigrams")
```

What about if you know the last two letters and are trying to predict the next letter? We see decreasing information within words as you move left-to-right. 

```{r plot prov letter trigrams}
prov_letters %>%
  filter(gram == "Trigram") %>%
  filter(word_length %in% 2:6) %>%
  ggplot(aes(x = letter_order, y = mean_s)) +
    facet_wrap(~word_length) +
    geom_point() +
    geom_line() +
    xlab("Phone Position") +
    ylab("Average Information") + 
    ggtitle("Information in English letters by letter position - trigrams")
```

# English Phonemes

## Surprisal

Briefly, what we're doing is splitting each word in the Providence corpus from CHILDES up into its constituent phones, i.e. the sounds that make up the word. We then calculate how common each phone is across the entire corpus. From this frequency measure, we compute the surprisal of each phone, which is inversely proportional to the frequency of the phone. The less common a phone is, the more difficult it is to process the phone. 

We split up the words in the corpus by their length in number of phones--for example, the word "food" has three phones: one for the "f", one for the long "oo", and one for the "d". We would take all words like this that have three phones. For each phone position  within the words of that length, we calculate the average surprisal/information in that position. For example, looking at all of the words with three phones, we take all of the first phones and calculate how much information is in that position on average. In other words, how difficult is it to process the first sound in a word, compared to the other sounds in a word?  

Unlike with letters, there isn't as much of a consistent shape to the information distribution for phones considered without context. For the most part, the first phone has relatively little information while the second phone has a lot of information. 

```{r plot prov phone unigrams}
prov_phones %>%
  filter(gram == "Unigram") %>%
  filter(word_length %in% 2:6) %>%
  ggplot(aes(x = phone_order, y = mean_s)) +
    facet_wrap(~word_length) +
    geom_point() +
    geom_line() +
    xlab("Phone Position") +
    ylab("Average Information") + 
    ggtitle("Information in English phones by phone position - unigrams")
```

With bigrams and trigrams for English phones, the next phone is very predictable if you know the phone or two that came before it. Especially if you know the preceding two phones, then each phone past the first two is predictable and carries relatively little information in English. 

```{r plot prov phone bigrams}
prov_phones %>%
  filter(gram == "Bigram") %>%
  filter(word_length %in% 2:6) %>%
  ggplot(aes(x = phone_order, y = mean_s)) +
    facet_wrap(~word_length) +
    geom_point() +
    geom_line() +
    xlab("Phone Position") +
    ylab("Average Information") + 
    ggtitle("Information in English phones by phone position - bigrams")
```

```{r plot prov phone trigrams}
prov_phones %>%
  filter(gram == "Trigram") %>%
  filter(word_length %in% 2:6) %>%
  ggplot(aes(x = phone_order, y = mean_s)) +
    facet_wrap(~word_length) +
    geom_point() +
    geom_line() +
    xlab("Phone Position") +
    ylab("Average Information") + 
    ggtitle("Information in English phones by phone position - trigrams")
```

## Hold-out entropy

For this next analysis, we took all words with 2, 3, 4, 5 or 6 phones. For each word length, we held out the phones in one phone position. For example, in all words with 4 phones, we held out the first phone. We then computed the entropy, or amount of variation, in the remaining words. The higher the entropy when holding out phones in a given position, the more information is contained in that phone on average. 

```{r leave-out-entropy, include = F}
hold_out_entropy <- function(df) {
  spread_df <- df %>%
    spread(phone_order, phone) %>%
    clean_names()
  
  phone_length <- max(df$phone_order)
  
  lens <- paste0("x", 1:phone_length)
  
  boot_entropy <- function(group) {
    spread_df %>%
      group_by_at(group) %>%
      summarise(n = n()) %>%
      summarise(entropy = entropy(n)) %>%
      ungroup() %>%
      tidyboot_mean(entropy)
  }
  
  
  groupings <- combinations(lens, phone_length -1, 
                            layout = "list") 
  
  groupings <- map(seq(length(lens),1,-1),
                   function(x) c(groupings[[x]], lens[x]))
  
  map_df(groupings, boot_entropy) %>%
    mutate(phone_order = seq(phone_length, 1, -1))
  
}
```

```{r show leave-out-entropy}
split_lengths <- adult_phonemes %>%
  group_by(word_length) %>%
  nest() %>%
  filter(word_length > 1) %>%
  filter(word_length %in% 1:6 ) %>%
  mutate(entropy = map(data, hold_out_entropy))

split_lengths %>%
  select(-data) %>%
  unnest() %>%
  ggplot(aes(x = phone_order, y = empirical_stat)) + 
  facet_wrap(~ word_length, scales = "free") +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  theme_classic() + 
  geom_line() + 
  xlab("Phone position") + 
  ylab("Entropy")
```

We see that word-initial phones tend to have more information in them than word-medial or word-final phones, so less is lost by dropping the final sounds in a word than in dropping the initial sounds. 

# French letters

In French, the final letters in a lot of words are dropped when those words are spoken aloud. We expect that the final letters in French words will on average contain little information, and that is what we see. In particular, if you know the two letters that came before, then each letter beyond the first in French has little information. Even if you don't know which letters came before, then the final letters of French words have relatively little information in them. 

```{r plot french letter unigrams}
french_letters %>%
  filter(gram == "Unigram") %>%
  filter(word_length %in% 2:6) %>%
  ggplot(aes(x = letter_order, y = mean_s)) +
    facet_wrap(~word_length) +
    geom_point() +
    geom_line() +
    xlab("Phone Position") +
    ylab("Average Information") + 
    ggtitle("Information in French letters by letter position - unigrams")
```

```{r plot french letter bigrams}
french_letters %>%
  filter(gram == "Bigram") %>%
  filter(word_length %in% 2:6) %>%
  ggplot(aes(x = letter_order, y = mean_s)) +
    facet_wrap(~word_length) +
    geom_point() +
    geom_line() +
    xlab("Phone Position") +
    ylab("Average Information") + 
    ggtitle("Information in French letters by letter position - bigrams")
```

```{r plot french letter trigrams}
french_letters %>%
  filter(gram == "Trigram") %>%
  filter(word_length %in% 2:6) %>%
  ggplot(aes(x = letter_order, y = mean_s)) +
    facet_wrap(~word_length) +
    geom_point() +
    geom_line() +
    xlab("Phone Position") +
    ylab("Average Information") + 
    ggtitle("Information in French letters by letter position - trigrams")
```

# German letters

We expect that German, as a language with a case system which gets shown through affixation on nouns, will have relatively higher information in the final letters of words compared to English or French. We don't observe this, even in unigrams. The final letters of German words hold little information compared to the first letter of German words, which is even more noticeable when you have two letters of predictive context for each word. 

```{r plot german letter unigrams}
german_letters %>%
  filter(gram == "Unigram") %>%
  filter(word_length %in% 2:6) %>%
  ggplot(aes(x = letter_order, y = mean_s)) +
    facet_wrap(~word_length) +
    geom_point() +
    geom_line() +
    xlab("Phone Position") +
    ylab("Average Information") + 
    ggtitle("Information in German letters by letter position - unigrams")
```

```{r plot german letter bigrams}
german_letters %>%
  filter(gram == "Bigram") %>%
  filter(word_length %in% 2:6) %>%
  ggplot(aes(x = letter_order, y = mean_s)) +
    facet_wrap(~word_length) +
    geom_point() +
    geom_line() +
    xlab("Phone Position") +
    ylab("Average Information") + 
    ggtitle("Information in German letters by letter position - bigrams")
```

```{r plot german letter trigrams}
german_letters %>%
  filter(gram == "Trigram") %>%
  filter(word_length %in% 2:6) %>%
  ggplot(aes(x = letter_order, y = mean_s)) +
    facet_wrap(~word_length) +
    geom_point() +
    geom_line() +
    xlab("Phone Position") +
    ylab("Average Information") + 
    ggtitle("Information in German letters by letter position - trigrams")
```