---
title: "Information within words in English, French and German speech"
author: Josef Klafka and Dan Yurovsky 
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: false
    number_sections: true
    theme: lumen
    toc_float: false
    code_folding: hide 
editor_options: 
  chunk_output_type: console
---

```{r load-libraries, library, message=F, results='hide', warning=FALSE}
library(tidytext)
library(tidyverse)
library(childesr)
library(glue)
library(feather)
library(here)
library(entropy)
library(tidyboot)
library(janitor)
library(arrangements)
library(stringr)

knitr::opts_chunk$set(echo = TRUE, cache = T, warning = F, 
                      fig.width = 8, fig.height = 4)

theme_set(theme_classic(base_size = 16))
```

```{r read-data}
all_data <- read_feather(here("Data/all_phonemes.feather"))
all_counts <- read_feather(here("Data/all_counts.feather"))
```

How is information spread among the sounds in English words? A salient feature of African American English Vernacular is the reduction of final consonants and consonant clusters. Why hasn't this created communication problems, namely identifying and disambiguating words? One possibility is that the information important for identifying a word is not uniformly distributed across all of its characters or sounds, but instead concentrated at the beginnings or middles of words. In these cases, reducing final sounds might reduce the articulatory complexity of words without much cost for the listener in processing these words and inferring their intended meaning. By the time the listener hears only the first sounds of a word, they may already have identified which word they are hearing. 

Through a series of exploratory analyses we tackle this problem in three ways:
1. Computing the surprisal of characters in English words from baserates of character production (unigram) and then conditioning on the prior charaters in each word (bigram and trigram). This analyses roughly captures the difficulty that a listener might have in predicting each character.
2. Repeating this analysis at the level of phonemes rather than characters, arguably a better proxy for the speaker's and listener's representations of spoken English. 
3. Computing hold-out entropy for individual phonemes in English words. This is roughly equivalent to asking "if you knew every other sound in a word, how much trouble would you have predicting what sounds appears in position $X$". 
4. Comparing our English by-letter analysis to results for French and German which have different expected information profiles. We expect that French words will have less information at the end of words compared to English, while German words will have more compared to English. 

Across all of our analyses, we find that the letters and sounds at the end of words in English have less information than the first letters and sounds in a word. Letters at the end of words in French and German also have little information compared to letters at the beginning of words in those languages, like in English.  

# English Letters

We use the Providence corpus from CHILDES as data for English. In the plots below, each plot represents the information in each letter, divided by words of different lengths. Word lengths are given by number of letters--we show the length above each plot. 

We see a distinctive shape for information at the level of individual letters in English. The first letter has relatively high information, the second letter has less information and the third letter has high infomration, and the final letter has less information (if the word has more than $3$ letters). 

```{r plot-helper}
plot_curves <- function(select_language = "English", select_rep = "letter", 
                        select_gram = "Unigram") {
  all_data %>%
    filter(language == select_language, rep == select_rep,
           gram == select_gram) %>%
    filter(word_length %in% 2:6) %>%
    ggplot(aes(x = order, y = mean_s)) +
    facet_grid(. ~ word_length, 
               labeller = as_labeller(function(x) paste0("Length ", x, " \n n = ", 
                                                         all_counts %>% 
                                                           filter(language == select_language,
                                                                  rep == select_rep, 
                                                                  length == x) %>%
                                                           pull(length)))) +
    geom_point() +
    geom_line() +
    xlab(paste0(str_to_sentence(select_rep), " Position")) +
    ylab("Average information") + 
    ggtitle(paste0("Information in ", select_language, " ", 
                   select_rep, "s by ",select_rep, " position - ", select_gram))
}
```

```{r prov-letter-unigrams}
plot_curves()
```

Now we ask: if you know the previous letter, how much information is in the next letter in English? There is relatively less information in each letter knowing the previous letter, but we see the same general information trajectory across letters. 

```{r prov-letter-bigrams}
plot_curves(select_gram = "Bigram")
```

What about if you know the last two letters and are trying to predict the next letter? We see decreasing information within words as you move left-to-right. 

```{r prov-letter-trigrams}
plot_curves(select_gram = "Trigram")
```

# English Phonemes

Briefly, what we're doing is splitting each word in the Providence corpus from CHILDES up into its constituent phones--the sounds that make up the word. We split up the words in the corpus by their length in number of phones--for example, the word "food" has three phones: one for the "f", one for the long "oo", and one for the "d". We would take all words like this that have three phones. For each phone position  within the words of that length, we calculate the average information in that position. 

Unlike with letters, there isn't as much of a consistent shape to the information distribution for phones in the unigram condition. If anything, the first phone has relatively little information while the second phone has a lot of information. 

```{r prov-phone-unigrams}
plot_curves(select_rep = "phone")
```

With bigrams and trigrams for English phones, the next phone is very predictable if you know the phone or two that came before it. Especially if you know the preceding two phones, then each phone past the first two is predictable and carries relatively little information in English. 

```{r prov-phone-bigrams}
plot_curves(select_rep = "phone", select_gram = "Bigram")
```

```{r prov-phone-trigrams}
plot_curves(select_rep = "phone", select_gram = "Trigram")
```

# Hold-out entropy

For this next analysis, we took all words with $2$, $3$, $4$, $5$ or $6$ phones. For each word length, we held out the phones in one phone position. For example, in all words with 4 phones, we held out the first phone. We then computed the entropy, or amount of variation, in the remaining words. The higher the entropy when holding out phones in a given position, the more information is contained in that phone on average. 

```{r leave-out-entropy, include = F}
hold_out_entropy <- function(df) {
  spread_df <- df %>%
    spread(phone_order, phone) %>%
    clean_names()
  
  phone_length <- max(df$phone_order)
  
  lens <- paste0("x", 1:phone_length)
  
  boot_entropy <- function(group) {
    spread_df %>%
      group_by_at(group) %>%
      summarise(n = n()) %>%
      summarise(entropy = entropy(n)) %>%
      ungroup() %>%
      tidyboot_mean(entropy)
  }
  
  
  groupings <- combinations(lens, phone_length -1, 
                            layout = "list") 
  
  groupings <- map(seq(length(lens),1,-1),
                   function(x) c(groupings[[x]], lens[x]))
  
  map_df(groupings, boot_entropy) %>%
    mutate(phone_order = seq(phone_length, 1, -1))
  
}
```

```{r show-leave-out-entropy}
split_lengths <- adult_phonemes %>%
  group_by(word_length) %>%
  nest() %>%
  filter(word_length > 1) %>%
  filter(word_length %in% 1:6 ) %>%
  mutate(entropy = map(data, hold_out_entropy))

split_lengths %>%
  select(-data) %>%
  unnest() %>%
  ggplot(aes(x = phone_order, y = empirical_stat)) + 
  facet_wrap(~ word_length, 
               labeller = as_labeller(function(x) paste0("Length ", x)),
               scales = "free") +
  geom_pointrange(aes(ymin = ci_lower, ymax = ci_upper)) + 
  theme_classic() + 
  geom_line() + 
  xlab("Phone position") + 
  ylab("Entropy")
```

We see that word-initial phones tend to have more information in them than word-medial or word-final phones, so less is lost by dropping the final sounds in a word than in dropping the initial sounds. 

# French and German

For our final analysis, we're going to look at how the information within words in French and German compare to English. We use the Palasis French corpus and Wagner German corpus from CHILDES for our data. We chose French because for many French words the final letters are not pronounced regardless of how the letters in the word appear. For example, "ils sauraient" meaning "they would know" is pronounced without the final "ent" in "sauraient". We are interested in German because German has a much richer case system than English or French, where case markings in German are suffixes at the ends of words. We think that in German, the ends of words may be more important than in French or English. 


## French letters

In French, the final letters in a lot of words are dropped when those words are spoken aloud. We expect that the final letters in French words will on average contain little information, and that is what we see. In particular, if you know the two letters that came before, then each letter beyond the first in French has little information. Even if you don't know which letters came before, then the final letters of French words have relatively little information in them. 

```{r plot french letter unigrams}
plot_curves(select_language = "French")
```

```{r plot french letter bigrams}
plot_curves(select_language = "French", select_gram = "Bigram")
```

```{r french-letter-trigrams}
plot_curves(select_language = "French", select_gram = "Trigram")
```

## German letters

We expect that German, as a language with a case system which gets shown through affixation on nouns, will have relatively higher information in the final letters of words compared to English or French. We don't observe this, even in unigrams. The final letters of German words hold little information compared to the first letter of German words, which is even more noticeable when you have two letters of predictive context for each word. 

```{r plot german letter unigrams}
plot_curves(select_language = "German")
```

```{r plot german letter bigrams}
plot_curves(select_language = "German", select_gram = "Bigram")
```

```{r plot german letter trigrams}
plot_curves(select_language = "German", select_gram = "Trigram")
```