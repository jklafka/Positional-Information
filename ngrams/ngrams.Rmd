---
title: "Eye-tracking"
author: "Joe Klafka"
date: "2/8/2019"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(tidytext)
library(tidyboot)
library(entropy)
library(magrittr)
library(feather)
library(here)
library(ggthemes)
library(childesr)

knitr::opts_chunk$set(echo = TRUE)
```


```{r plotting for Monday}
df <- read_feather(here("Data/childes.feather"))

label_data <- data_frame(length = 9, language = "Spanish", gram_order = 6,
                         speaker_role = c("Adult", "Child"), empirical_entropy = c(9.25,10.25))

df %>% 
  filter(language %in% c("English", "Mandarin", "Spanish"), gram == "trigram", length %in% c(5, 7, 9)) %>%
  ggplot(aes(x = gram_order, y = empirical_entropy, color = speaker_role, label = speaker_role)) + 
  facet_grid(language ~ length, scales = "free") + 
  xlab("Gram position") + 
  ylab("Positional entropy") + 
  geom_pointrange(size = .25, aes(ymin = ci_lower, ymax = ci_upper)) + 
  geom_line(size = .25) + 
  theme_classic(base_size = 10) + 
  scale_color_ptol() + 
  theme(legend.position = "nones") + 
  geom_text(data = label_data)

```

```{r corpora}
prov_utterances <- get_utterances(corpus = "Providence") %>% 
  filter(speaker_role == "Target_Child") %>%
  mutate(words = stem)

okayama_utterances <- get_utterances(corpus = "Okayama")  %>% 
  filter(speaker_role != "Target_Child") %>%
  rename(words = gloss)

zhou_utterances <- get_utterances(corpus = "ZhouDinner") %>%
  filter(speaker_role != "Target_Child") %>%
  rename(words = stem)

shiro_utterances <- get_utterances(corpus = "Shiro") %>% 
  filter(speaker_role == "Target_Child") %>%
  rename(words = gloss)

wagner_utterances <- get_utterances(corpus = "Wagner") %>% 
  filter(speaker_role == "Target_Child") %>%
  rename(words = gloss)

switchboard <- read_feather(here("../switchboard/switchboard.feather")) %>%
  rename(words = value)
```

```{r relative ngram entropy}
extract_wiki_tokens <- function(min_length, max_length, utterances, N) {

  tokens <- utterances %>%
    mutate(length = str_count(words, pattern = "[ +]+") + 1) %>%
    filter(length >= min_length) %>%
    filter(length <= max_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, words, token = stringr::str_split, pattern = "[ +]+") %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n())


  lags <- expand.grid(lag_n = 1:(N-1),
                      utterance_id = unique(tokens$utterance_id))

  tokens %>%
    left_join(lags, by = "utterance_id") %>%
    arrange(utterance_id, lag_n, word_order) %>%
    mutate(lag = lag(word, n = first(lag_n))) %>%
    drop_na() %>%
    mutate(gram = str_c(lag, word, sep = " ")) %>%
    rename(gram_order = word_order) %>%
    group_by(length, gram_order, gram) %>%
    summarise(n = n())

}

get_quantiles <- function(df, num_sections) {

  quantile(df$gram_order, probs = seq(0, 1, 1/num_sections)) %>%
    round() %>%
    as_data_frame() %>%
    rename(gram_order = value)
}

relative_slopes <- function(df, pos_list) {

  get_slope <- function(x) {

    selected_pos <- pos_list %>%
      group_by(length) %>%
      slice(x:(x+1)) %>%
      group_by(length) %>%
      summarise(min = min(gram_order),
                max = max(gram_order)) %>%
      split(.$length) %>%
      map(~seq(.x$min, .x$max, 1) %>% as_data_frame) %>%
      bind_rows(.id = "length") %>%
      rename(gram_order = value) %>%
      mutate(length = as.numeric(length))

    df_tokens %>%
      inner_join(selected_pos, by = c("length", "gram_order")) %>%
      group_by(length, gram_order) %>%
      summarise(entropy = entropy(n, unit = "log2")) %>%
      lm(entropy ~ length + I(length^2) + gram_order, data  = .) %>%
      tidy() %>%
      filter(term == "gram_order")
  }

  divides <- pos_list %>%
    group_by(length) %>%
    summarise(n = n()) %>%
    summarise(n = mean(n)) %>%
    pull()

  map(1:((divides)-1), get_slope) %>%
    bind_rows(.id = "cut")

}

### End Functions

NUM_SECTIONS = 5

LANGUAGE <- "Danish"
NGRAMS <- 2

df_tokens <- get_utterances(corpus = "Plunkett") %>% 
  filter(speaker_role != "Target_Child") %>%
  rename(words = gloss) %>%
  extract_wiki_tokens(min_length = NGRAMS+5, max_length = 10, N = NGRAMS)
  
pos_list <- df_tokens %>%
  group_by(length) %>%
  distinct(gram_order) %>%
  split(.$length) %>%
  map(~get_quantiles(.x, NUM_SECTIONS)) %>%
  bind_rows(.id = "length") %>%
  mutate(length = as.numeric(length))

slopes <- relative_slopes(df_tokens, pos_list)


```

```{r absolute ngram entropy}
extract_wiki_tokens <- function(min_length, max_length, utterances, N) {

  tokens <- utterances %>%
    mutate(length = str_count(words, pattern = "[ +]+") + 1) %>%
    filter(length >= min_length) %>%
    filter(length <= max_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(gram, words, token = stringr::str_split, pattern = "[ +]+") %>%
    group_by(utterance_id) %>%
    mutate(gram_order = 1:n())


  lags <- expand.grid(lag_n = 1:(N-1),
                      utterance_id = unique(tokens$utterance_id))

  tokens %>%
    left_join(lags, by = "utterance_id") %>%
    arrange(utterance_id, lag_n, gram_order) %>%
    mutate(lag = lag(gram, n = first(lag_n))) %>%
    drop_na() %>%
    mutate(gram = str_c(lag, gram, sep = " ")) %>%
    rename(gram_order = gram_order) %>%
    group_by(length, gram_order, gram) %>%
    summarise(n = n())

}

start_slope <- function(df) {

 df %>%
    filter(gram_order %in% c(2,3)) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, gram_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(gram_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `3` - `2`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

second_slope <- function(df) {

 df %>%
    ungroup() %>%
    mutate(gram_order %in% c(3, 4)) %>%
    filter(gram_order > 0) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, gram_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(gram_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `4` - `3`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

#note this measurement only makes sense if utterance length is at least 6
mid_slope <- function(df) {

 df %>%
    ungroup() %>%
    mutate(gram_order = if_else(gram_order == length-2, 3,
                                if_else(gram_order == 3, 2,
                                        0))) %>%
    filter(gram_order > 0) %>%
    group_by(length, gram_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, gram_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(gram_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `3` - `2`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

penult_slope <- function(df) {

 df %>%
    ungroup() %>%
    mutate(gram_order = if_else(gram_order == length-1, 3,
                                if_else(gram_order == length -2, 2,
                                        0))) %>%
    filter(gram_order > 0) %>%
    group_by(length, gram_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, gram_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(gram_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `3` - `2`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}

end_slope <- function(df) {

 df %>%
    ungroup() %>%
    mutate(gram_order = if_else(gram_order == length, 3,
                                if_else(gram_order == length -1 , 2,
                                        0))) %>%
    filter(gram_order > 0) %>%
    group_by(length, gram_order) %>%
    sample_frac(1, replace = T) %>%
    group_by(length, gram_order) %>%
    summarise(entropy = entropy(n, unit = "log2")) %>%
    spread(gram_order, entropy) %>%
    ungroup() %>%
    mutate(diff = `3` - `2`) %>%
    summarise(diff = mean(diff)) %>%
    pull(diff)

}


get_wiki_slopes <- function(tokens, REPS) {

  start_slopes <- replicate(REPS, start_slope(tokens)) %>%
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975),
              ci_lower = quantile(value, .025))

  second_slopes <- replicate(REPS, second_slope(tokens)) %>%
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975),
              ci_lower = quantile(value, .025))

  mid_slopes <- replicate(REPS, mid_slope(tokens)) %>%
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975),
              ci_lower = quantile(value, .025))

  penult_slopes <- replicate(REPS, penult_slope(tokens)) %>%
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975),
              ci_lower = quantile(value, .025))

  end_slopes <- replicate(REPS, end_slope(tokens)) %>%
    as_data_frame() %>%
    summarise(mean = mean(value), ci_upper = quantile(value, .975),
              ci_lower = quantile(value, .025))

  rbind(start_slopes, second_slopes, mid_slopes, penult_slopes, end_slopes)
}

### END FUNCTIONS

LANGUAGE <- 
NGRAMS <- 
df <-

REPS <- 100

tokens <- extract_wiki_tokens(NGRAMS + 5, 50, df, NGRAMS)
slopes <- get_wiki_slopes(tokens, REPS)
```

```{r basic ngram entropy}
# takes in a tbl of utterances the column with the actual utterances is 
# named words, along with an integer target_length and a ngram length N.
ngram_entropy <- function(utterances, target_length, N) {

  
  tokens <- utterances %>%
    mutate(length = str_count(words, pattern = "[ +]+") + 1) %>%
    filter(length == target_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, words, token = stringr::str_split, pattern = " +") %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n())
  
    
  lags <- expand.grid(lag_n = 1:(N-1), 
                      utterance_id = unique(tokens$utterance_id))
  
  
  grams <- tokens %>%
    left_join(lags, by = "utterance_id") %>%
    group_by(utterance_id, lag_n) %>%
    arrange(utterance_id, lag_n, word_order) %>%
    mutate(lag = lag(word, n = first(lag_n))) %>%
    group_by(utterance_id, word_order, word) %>%
    summarise(gram = paste(lag, collapse = " ")) %>%
    summarise(gram = paste(gram, word, collapse = " ")) %>%
    filter(word_order >= N) %>%
    rename(gram_order = word_order) %>%
    group_by(gram_order, gram)

 grams %>%
  summarise(n = n()) %>%
  tidyboot(summary_function = function(x) x %>% 
        summarise(entropy = entropy(n, unit = "log2")),
       statistics_functions = function(x) x %>%
       summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
  mutate(length = target_length)
}
```

```{r switchboard}
# unigrams to check against previous results
switchboard_unigrams <- map(2:10, ~ngram_entropy(switchboard, .x, 1)) %>% 
  bind_rows()

switchboard_unigrams %>% ggplot(aes(x = gram_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F)

# bigrams
switchboard_bigrams <- map(3:10, ~ngram_entropy(switchboard, .x, 2)) %>% 
  bind_rows()

switchboard_bigrams %>% ggplot(aes(x = gram_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F)

#trigrams
switchboard_trigrams <- map(4:10, ~ngram_entropy(switchboard, .x, 3)) %>% 
  bind_rows()

switchboard_trigrams %>% ggplot(aes(x = gram_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F)
```





















