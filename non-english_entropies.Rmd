---
title: Unigram positional entropy, conditional entropy and mutual information in non-English CHILDES corpora
author: Josef Klafka
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: false
    number_sections: false
    theme: lumen
    toc_float: false
    code_folding: show 
---

```{r setup, include = FALSE}
# load packages
library(knitr)
library(tidyverse)
library(directlabels)
library(childesr) #data
library(SnowballC) #stemmer
library(tidytext)
library(entropy)
library(tidyboot)
library(dplyr)
library(tokenizers)
library(tau)
knitr::opts_chunk$set(echo = TRUE)

opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
               error = FALSE, cache = FALSE, tidy = FALSE)

theme_set(theme_classic(base_size = 16))
```

Get utterances
```{r get_utterances}
prov_utterances <- get_utterances(corpus = "Providence")

brown_utterances <- get_utterances(corpus = "Brown")
```

Unigram Entropy word length correction
```{r word_len_corrections}
get_counts <- function(role, sen_length, utterances) {
  
  if(role == "Target_Child") {
    sub_utterances <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1) # uses stems of words, not original words
  } else {
     sub_utterances <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1)
  }

  sub_utterances %>%
    filter(length == sen_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, gloss) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    filter(word_order <= length)
}

get_mean_lengths <- function(role, sen_length, utterances) {
  
  counts <- get_counts(role, sen_length, utterances)
  
  lens <- counts %>% 
    group_by(word_order, word) %>% 
    mutate(word_len = str_length(word)) 
  
  lens$word_len %>% 
    aggregate(list(lens$word_order), mean)
}

get_entropies <- function(role, sen_length, utterances) {
  
  if(role == "Target_Child") {
    sub_utterances <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1) # uses stems of words, not original words
  } else {
     sub_utterances <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1)
  }

  tokens <- sub_utterances %>%
    filter(length == sen_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, gloss) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n()) %>%
    group_by(word_order, word) %>%
    filter(word_order <= length)
   
  tokens %>%
    summarise(n = n()) %>%
    tidyboot(summary_function = function(x) x %>% 
               summarise(entropy = entropy(n, unit = "log2")),
             statistics_functions = function(x) x %>%
             summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
    mutate(role = role, length = sen_length)
}

get_cor_entropies <- function(role, utterances) {
  
  mean_lens <- map(2:10, ~get_mean_lengths(role, .x, utterances)) %>% 
    bind_rows()
  
  entropies <- map(2:10, ~get_entropies(role, .x, utterances)) %>%
    bind_rows()
  
  entropies %>% 
    mutate(mean_len = mean_lens[, "x"]) %>% 
    mutate(cor_entropy = empirical_entropy / mean_len)
}

##
mean_lens <- map(2:10, ~get_mean_lengths("not child", .x, prov_utterances)) %>% bind_rows()

cor_entropies <- entropies %>% mutate(mean_len = mean_lens[, "x"]) %>% mutate(cor_entropy = empirical_entropy / mean_len)

entropies <- map(2:10, ~get_entropies("not child", .x, prov_utterances)) %>%
  bind_rows()
##

cor_entropies <- get_cor_entropies("not child", okayama_utterances)

ggplot(cor_entropies, aes(x = word_order, y = cor_entropy)) +
     facet_wrap(~ length) + 
     geom_smooth(se = F)
```

Non-English Unigram frequencies
```{r Spanish unigram frequencies}
shiro_utterances <- get_utterances(language = "SPA", corpus="Shiro") 
#Venezuelan children narrative (113 children; half 1st graders half 4th graders; half high SES half low SES)


get_noneng_entropies <- function(role, sen_length, utterances, lang) {
  
  if(role == "Target_Child") {
    sub_utterances <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(stems = wordStem(gloss, language = lang)) %>% 
      mutate(length = str_count(stems, " ") + 1) 
  } else {
     sub_utterances <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(stems = wordStem(gloss, language = lang)) %>% 
      mutate(length = str_count(stems, " ") + 1)
  }

  tokens <- sub_utterances %>%
    filter(length == sen_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stems) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n())
   
  tokens %>%
    group_by(word_order, word) %>%
    summarise(n = n()) %>%
    tidyboot(summary_function = function(x) x %>% 
               summarise(entropy = entropy.empirical(n, unit = "log2")),
             statistics_functions = function(x) x %>%
             summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
    mutate(role = role, length = sen_length)
}

entropies <- map(2:10, ~get_noneng_entropies("not child", .x, shiro_utterances, "spanish")) %>%
  bind_rows() %>% filter(word_order <= length) #some utterances have ""

ggplot(entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F) +
  ggtitle("Adult Unigram Entropy vs. Word Position (Spanish Shiro Corpus with stemming)")


entropies_child <- map(2:10, ~ get_noneng_entropies("Target_Child", .x, shiro_utterances, "spanish")) %>%
  bind_rows() %>% filter(word_order <= length)

ggplot(entropies_child, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F) +
  ggtitle("Child Unigram Entropy vs. Word Position (Spanish Shiro Corpus with stemming)")

entropies_all <- bind_rows(entropies, entropies_child)

ggplot(entropies_all, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper, color = role)) +
  facet_wrap(~ length) + 
  geom_pointrange(position = position_dodge(.25)) +
  geom_smooth(se = F) +
  ggtitle("Adult and Child Unigram Entropy vs. Word Position (Spanish Shiro Corpus with stemming)")
```


```{r German Unigram Frequencies}
wagner_utterances <- get_utterances(corpus = "Wagner") #German corpus of children during daily routines (age range from 1,5 to 14,10)

entropies <- map(2:10, ~get_noneng_entropies("not child", .x, wagner_utterances, "german")) %>%
  bind_rows() %>% filter(word_order <= length) #some utterances have ""

ggplot(entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F) +
  ggtitle("Adult Unigram Entropy vs. Word Position (German Wagner Corpus with stemming)")


entropies_child <- map(2:10, ~ get_noneng_entropies("Target_Child", .x, wagner_utterances, "german")) %>%
  bind_rows() %>% filter(word_order <= length)

ggplot(entropies_child, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F) +
  ggtitle("Child Unigram Entropy vs. Word Position (German Wagner Corpus with stemming)")

entropies_all <- bind_rows(entropies, entropies_child)

ggplot(entropies_all, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper, color = role)) +
  facet_wrap(~ length) + 
  geom_pointrange(position = position_dodge(.25)) +
  geom_smooth(se = F) +
  ggtitle("Adult and Child Unigram Entropy vs. Word Position (German Wagner Corpus with stemming)")
```


```{r Japanese Unigram Entropy}
okayama_utterances <- get_utterances(corpus = "Okayama") #mother-child conversations in the Osaka area

get_japanese_entropies <- function(role, sen_length, utterances) {
  
  if(role == "Target_Child") {
    sub_utterances <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1) 
  } else {
     sub_utterances <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      mutate(length = str_count(gloss, " ") + 1)
  }
 
  tokens <- sub_utterances %>%
    filter(length == sen_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, gloss) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n())
   
  tokens %>%
    group_by(word_order, word) %>%
    summarise(n = n()) %>%
    tidyboot(summary_function = function(x) x %>% 
               summarise(entropy = entropy.empirical(n, unit = "log2")),
             statistics_functions = function(x) x %>%
             summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
    mutate(role = role, length = sen_length)
}

entropies <- map(2:10, ~get_japanese_entropies("not child", .x, okayama_utterances)) %>%
  bind_rows() %>% filter(word_order <= length) #some utterances have "" counted for word_order

ggplot(entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F) +
  ggtitle("Adult Unigram Entropy vs. Word Position (Japanese Okayama Corpus)")

entropies_child <- map(2:10, ~ get_japanese_entropies("Target_Child", .x, okayama_utterances)) %>%
  bind_rows() %>% filter(word_order <= length)

ggplot(entropies_child, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F) +
  ggtitle("Child Unigram Entropy vs. Word Position (Japanese Okayama Corpus)")

entropies_all <- bind_rows(entropies, entropies_child)

ggplot(entropies_all, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper, color = role)) +
  facet_wrap(~ length) + 
  geom_pointrange(position = position_dodge(.25)) +
  geom_smooth(se = F) +
  ggtitle("Adult and Child Unigram Entropy vs. Word Position (Japanese Okayama Corpus)")
```


```{r Chinese Unigram Entropy}
zhou_dinner <- get_utterances(corpus = "zhoudinner")

get_chinese_entropies <- function(role, sen_length, utterances) {
  
  if(role == "Target_Child") {
    sub_utterances <- utterances %>%
      filter(speaker_role == "Target_Child") %>%
      filter(gloss != "") %>% 
      mutate(length = str_count(stem, " ") + 1) 
  } else {
     sub_utterances <- utterances %>%
      filter(speaker_role != "Target_Child") %>%
      filter(speaker_role != "") %>% 
      mutate(length = str_count(stem, " ") + 1)
  }

  tokens <- sub_utterances %>%
    filter(length == sen_length) %>%
    mutate(utterance_id = 1:n()) %>%
    unnest_tokens(word, stem) %>%
    group_by(utterance_id) %>%
    mutate(word_order = 1:n())
   
  tokens %>%
    group_by(word_order, word) %>%
    summarise(n = n()) %>%
    tidyboot(summary_function = function(x) x %>% 
               summarise(entropy = entropy.empirical(n, unit = "log2")),
             statistics_functions = function(x) x %>%
             summarise_at(vars(entropy), funs(ci_upper, ci_lower))) %>%
    mutate(role = role, length = sen_length)
}

entropies <- map(2:10, ~get_chinese_entropies("not child", .x, zhou_dinner)) %>%
  bind_rows() %>% filter(word_order <= length) #some utterances have "" counted for word_order

ggplot(entropies, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F) +
  ggtitle("Adult Unigram Entropy vs. Word Position (Mandarin Zhou Dinner Corpus)")

entropies_child <- map(2:10, ~ get_chinese_entropies("Target_Child", .x, zhou_dinner)) %>%
  bind_rows() %>% filter(word_order <= length)

ggplot(entropies_child, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper)) +
  facet_wrap(~ length) + 
  geom_pointrange() +
  geom_smooth(se = F) +
  ggtitle("Child Unigram Entropy vs. Word Position (Mandarin Zhou Dinner Corpus)")

entropies_all <- bind_rows(entropies, entropies_child)

ggplot(entropies_all, aes(x = word_order, y = empirical_entropy,
                      ymin = ci_lower, ymax = ci_upper, color = role)) +
  facet_wrap(~ length) + 
  geom_pointrange(position = position_dodge(.25)) +
  geom_smooth(se = F) +
  ggtitle("Adult and Child Unigram Entropy vs. Word Position (Mandarin Zhou Dinner Corpus)")
```